{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5-c_3HqF0ykA",
   "metadata": {
    "id": "5-c_3HqF0ykA"
   },
   "source": [
    "# Given someone with suicidal desire, detect active rescue \n",
    "\n",
    "Author: Daniel Low (Harvard University)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "location = 'local'\n",
    "random_seed = 123\n",
    "# Classification config\n",
    "toy = False\n",
    "dvs = ['dv'] #columns which will be DVs\n",
    "\n",
    "cv = 5\n",
    "classes = {\n",
    "\t'dv':['suicidal_desire', 'active_rescue'] # 0 and 1\n",
    "\t\t   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x-b5q5cd9LWe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-b5q5cd9LWe",
    "outputId": "a4de6c41-520f-4ba2-8112-975b3be85ca9"
   },
   "outputs": [],
   "source": [
    "!python --version # tested with Python 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wb2NjiV32ves",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wb2NjiV32ves",
    "outputId": "1da004a8-fd11-4446-a052-253deb049ece"
   },
   "outputs": [],
   "source": [
    "# !pip install -q deplacy==2.0.5\n",
    "# !pip install -q flair==0.13.0\n",
    "# !pip install -q --upgrade urllib3==2.0.7\n",
    "# !pip install -q sentence-transformers==2.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y6nL6hsy_ljN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6nL6hsy_ljN",
    "outputId": "9ec170b2-a8f8-40e8-8bc7-c378598dbd57"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Authors: Daniel M. Low\n",
    "License: See license in github repository\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from importlib import reload\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.options.display.width = 0\n",
    "\n",
    "# local scripts\n",
    "import srl_constructs\n",
    "sys.path.append('./../concept-tracker') # wherever cts.py is\n",
    "from concept_tracker.utils import pipelines, hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "\n",
    "\n",
    "if location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/datasets/'\n",
    "  output_dir = 'home/dlow/zero_shot/data/output/'\n",
    "elif location =='local':\n",
    "  input_dir = './data/output/ctl/'\n",
    "  output_dir = './data/output/active_rescue/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05664f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = srl_constructs.constructs_in_order\n",
    "\n",
    "remove_constructs = [\n",
    "\t\t\t\t'Bullying', # 'tell me to kill myself' matches with 'kill myself'\n",
    "\t\t\t\t\t 'Social withdrawal',  #'want to be alone' matches with many loneliness comments\n",
    "\t\t\t\t\t 'Suicide exposure' #'suicide aftermath' matches with 'suicide' etc\n",
    "\t\t\t\t\t 'Agitated',# matches 'frustrated', 'stress'\n",
    "\t\t\t\t\t 'Emotional pain & psychache', \n",
    "\t\t\t\t\t 'Grief & bereavement', #'commited suicide' with 'suicide'\n",
    "\t\t\t\t\t 'Perfectionism',\n",
    "\t\t\t\t\t 'Discrimination',#\"treated with less respect\" matches with common phrase by therapists \"No one deserves to be treated that way\"\n",
    "\t\t\t\t\t ]\n",
    "\n",
    "\n",
    "# add_constructs = ['texter_word_count',\t'counselor_word_count', '1p_sg', '3rd_sg_pl', 'past', 'present','future', 'simple', 'progressive']\n",
    "# add_constructs = ['amount_of_messages_texter',\t'amount_of_messages_counselor']# 'amount_of_messages']#, '1p_sg', '3rd_sg_pl', 'past', 'present','future', 'simple', 'progressive']\n",
    "\n",
    "add_constructs = ['word_count', 'word_count_texter', 'word_count_counselor', 'word_count_texter_counselor_ratio', 'amount_of_messages_texter', 'amount_of_messages_counselor', 'amount_of_messages_texter_counselor_ratio']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_names = [x for x in feature_names if x not in remove_constructs]\n",
    "feature_names+= add_constructs\n",
    "\n",
    "feature_vectors = [f'cts_{len(feature_names)}_srl_prototypes']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = pd.read_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv')\n",
    "test_subset = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_clauses-all.csv')\n",
    "test_subset_25 = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_clauses-46.csv')\n",
    "test_subset_50 = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_clauses-71.csv')\n",
    "test_subset_75 = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_clauses-106.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab831186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1750*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac78f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_subset['dv'].value_counts())\n",
    "display(test_subset['dv'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a396dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_subset.copy()\n",
    "# amount_of_messages = 1000\n",
    "\n",
    "# messages = [n.split('\\n')[:amount_of_messages] for n in df[f'message_with_interaction_clean'].values]\n",
    "# messages_texter_all = []\n",
    "# messages_counselor_all = []\n",
    "# word_count_texter = []\n",
    "# word_count_counselor = []\n",
    "# word_count_both = []\n",
    "# for convo in messages:\n",
    "# \tmessages_texter = [n.replace('texter: ','') for n in convo if 'texter:' in n]\n",
    "# \tmessages_texter_all.append(len(messages_texter))\n",
    "# \tmessages_texter_join = ' '.join(messages_texter)\n",
    "# \tword_count_texter.append(len(messages_texter_join.split()))\n",
    "\n",
    "# \tmessages_counselor = [n.replace('counselor: ','') for n in convo if 'counselor:' in n]\n",
    "# \tmessages_counselor_all.append(len(messages_counselor))\n",
    "# \tmessages_counselor_join = ' '.join(messages_counselor)\n",
    "# \tword_count_counselor.append(len(messages_counselor_join.split()))\n",
    "\t\n",
    "# \tconvo_join = ' '.join(convo).replace('texter: ', '').replace('counselor: ', '')\n",
    "# \tword_count_both.append(len(convo_join.split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df[f'word_count'] = word_count_both\n",
    "# df[f'word_count_texter'] = word_count_texter\n",
    "# df[f'word_count_counselor'] = word_count_counselor\n",
    "# df[f'word_count_texter_counselor_ratio'] = df[f'word_count_texter'] / df[f'word_count_counselor']\n",
    "# df[f'amount_of_messages_texter'] = messages_texter_all\n",
    "# df[f'amount_of_messages_counselor'] = messages_counselor_all\n",
    "# df[f'amount_of_messages_texter_counselor_ratio'] = df[f'amount_of_messages_texter'] / df[f'amount_of_messages_counselor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef5b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_25, messages_50, messages_75 = 28,43,64\n",
    "\n",
    "\n",
    "\n",
    "def word_count_by_person(df, amount_of_messages):\n",
    "\tmessages = [n.split('\\n')[:amount_of_messages] for n in df[f'message_with_interaction_clean'].values]\n",
    "\tmessages_texter_all = []\n",
    "\tmessages_counselor_all = []\n",
    "\tword_count_texter = []\n",
    "\tword_count_counselor = []\n",
    "\tword_count_both = []\n",
    "\tfor convo in messages:\n",
    "\t\tmessages_texter = [n.replace('texter: ','') for n in convo if 'texter:' in n]\n",
    "\t\tmessages_texter_all.append(len(messages_texter))\n",
    "\t\tmessages_texter_join = ' '.join(messages_texter)\n",
    "\t\tword_count_texter.append(len(messages_texter_join.split()))\n",
    "\n",
    "\t\tmessages_counselor = [n.replace('counselor: ','') for n in convo if 'counselor:' in n]\n",
    "\t\tmessages_counselor_all.append(len(messages_counselor))\n",
    "\t\tmessages_counselor_join = ' '.join(messages_counselor)\n",
    "\t\tword_count_counselor.append(len(messages_counselor_join.split()))\n",
    "\t\t\n",
    "\t\tconvo_join = ' '.join(convo).replace('texter: ', '').replace('counselor: ', '')\n",
    "\t\tword_count_both.append(len(convo_join.split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdf[f'word_count'] = word_count_both\n",
    "\tdf[f'word_count_texter'] = word_count_texter\n",
    "\tdf[f'word_count_counselor'] = word_count_counselor\n",
    "\tdf[f'word_count_texter_counselor_ratio'] = df[f'word_count_texter'] / df[f'word_count_counselor']\n",
    "\tdf[f'amount_of_messages_texter'] = messages_texter_all\n",
    "\tdf[f'amount_of_messages_counselor'] = messages_counselor_all\n",
    "\tdf[f'amount_of_messages_texter_counselor_ratio'] = df[f'amount_of_messages_texter'] / df[f'amount_of_messages_counselor']\n",
    "\n",
    "\n",
    "\n",
    "\treturn df \n",
    "\n",
    "\n",
    "\n",
    "train_subset = word_count_by_person(train_subset, 1000)\n",
    "test_subset_25 = word_count_by_person(test_subset_25, messages_25)\n",
    "test_subset_50 = word_count_by_person(test_subset_50, messages_50)\n",
    "test_subset_75 = word_count_by_person(test_subset_75, messages_75)\n",
    "test_subset = word_count_by_person(test_subset, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47137f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_25['conversation_id'].values == test_subset['conversation_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset['texter_word_count'] = train_subset['word_count'].values\n",
    "# train_subset['texter_counselor_word_count'] = train_subset['word_count_with_interaction'].values\n",
    "\n",
    "\n",
    "# test_subset['texter_word_count'] = test_subset['word_count'].values\n",
    "# test_subset['texter_counselor_word_count'] = test_subset['word_count_with_interaction'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # train_subset['counselor_word_count'] = train_subset['word_count_with_interaction'] - train_subset['word_count']\n",
    "\n",
    "# # test_subset['texter_word_count'] = test_subset['word_count'].values\n",
    "# # test_subset['counselor_word_count'] = test_subset['word_count_with_interaction'] - train_subset['word_count']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate columns\n",
    "for col in feature_names:\n",
    "\ttry: train_subset = train_subset.drop(col+'_y',axis=1)\n",
    "\texcept: pass\n",
    "\n",
    "train_subset.columns = [n.replace('_x','') for n in train_subset.columns]\n",
    "\n",
    "\n",
    "for col in feature_names:\n",
    "\ttry: test_subset_50 = test_subset_50.drop(col+'_y',axis=1)\n",
    "\texcept: pass\n",
    "\n",
    "test_subset_50.columns = [n.replace('_x','') for n in test_subset_50.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af83ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X_train = train_subset[feature_names]\n",
    "y_train = train_subset['dv'].values\n",
    "X_test = test_subset[feature_names]\n",
    "X_test_25 = test_subset_25[feature_names]\n",
    "X_test_50 = test_subset_50[feature_names]\n",
    "X_test_75 = test_subset_75[feature_names]\n",
    "\n",
    "y_test = test_subset['dv'].values\n",
    "print('train', Counter(y_train))\n",
    "print('test',Counter(y_test))\n",
    "\n",
    "test_subset.index = test_subset['conversation_id'].values\n",
    "classes_i = classes['dv']\n",
    "# balance test set\n",
    "test_subset_1 = test_subset[test_subset['dv']==classes_i[1]]\n",
    "test_subset_0 = test_subset[test_subset['dv']==classes_i[0]].sample(n=test_subset_1.shape[0],random_state=random_seed)\n",
    "test_subset_balanced = pd.concat([test_subset_0, test_subset_1]).sample(frac=1).reset_index(drop=True)\n",
    "X_test_balanced = test_subset_balanced[feature_names]\n",
    "# TODO for 25,50,75 if you want \n",
    "X_test_balanced.index = test_subset_balanced['conversation_id'].values\n",
    "y_test_balanced = test_subset_balanced['dv'].values\n",
    "print('test',Counter(y_test_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe438b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset[train_subset['dv']=='active_rescue']['Active suicidal ideation & suicidal planning'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d22792",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset[test_subset['dv']=='active_rescue']['Active suicidal ideation & suicidal planning'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8221218",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_25[test_subset_25['dv']=='active_rescue']['Active suicidal ideation & suicidal planning'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J-TI2LYsROiR",
   "metadata": {
    "id": "J-TI2LYsROiR"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8824f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gridsearch == 'minority':\n",
    "\t\t\t\t# \t# Obtain all hyperparameter combinations\n",
    "\t\t\t\t# \tparameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\t\t\t# \tparameter_set_combinations = get_combinations(parameters)\n",
    "\t\t\t\t# \tscores = {}\n",
    "\t\t\t\t# \tfor i, set in enumerate(parameter_set_combinations):\n",
    "\t\t\t\t# \t\tpipeline.set_params(**set)\n",
    "\t\t\t\t# \t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\t# \t\ty_pred = pipeline.predict(X_val) # validation set \n",
    "\t\t\t\t# \t\trmse_per_value = []\n",
    "\t\t\t\t# \t\trmse = metrics.mean_squared_error(y_val, y_pred, squared=False ) # validation set \n",
    "\t\t\t\t# \t\tfor value in np.unique(y_val):\n",
    "\t\t\t\t# \t\t\ty_pred_test_i = [[pred,test] for pred,test in zip(y_pred,y_val) if test == value] # validation set \n",
    "\t\t\t\t# \t\t\ty_pred_i = [n[0] for n in y_pred_test_i]\n",
    "\t\t\t\t# \t\t\ty_test_i = [n[1] for n in y_pred_test_i]\n",
    "\t\t\t\t# \t\t\trmse_i = metrics.mean_squared_error(y_test_i, y_pred_i, squared=False )\n",
    "\t\t\t\t# \t\t\trmse_per_value.append(rmse_i )\n",
    "\t\t\t\t# \t\tscores[i] = [rmse]+rmse_per_value+[str(set)]\n",
    "\t\t\t\t# \tscores = pd.DataFrame(scores).T\n",
    "\t\t\t\t# \tscores.columns = ['RMSE', 'RMSE_2', 'RMSE_3', 'RMSE_4', 'Parameters']\n",
    "\t\t\t\t# \tscores = scores.sort_values('RMSE_4')\n",
    "\t\t\t\t# \tbest_params = eval(scores['Parameters'].values[0])\n",
    "\t\t\t\t# \tpipeline.set_params(**best_params)\n",
    "\t\t\t\t# \tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\t# \ty_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concept_tracker.utils import metrics_report\n",
    "reload(metrics_report)\n",
    "import warnings\n",
    "\n",
    "toy = False\n",
    "balance_test_set = False\n",
    "random_seed = 123\n",
    "\n",
    "\n",
    "sample_sizes = ['all']#[100, 500, 'all'] \n",
    "model_names = ['LGBMClassifier']# ['LogisticRegression']#,'LGBMClassifier'] #'XGBClassifier',\n",
    "task = 'classification'\n",
    "hyperparameter_tuning = 'bayesian'\n",
    "\n",
    "if task == 'classification':\n",
    "\tscoring = 'f1'\n",
    "\tmetrics_to_report = 'all'\n",
    "\t\n",
    "elif task == 'regression':\n",
    "\tscoring = 'neg_mean_squared_error'\n",
    "\tmetrics_to_report = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(metrics_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(metrics_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if toy:\n",
    "\tsample_sizes = [50]\n",
    "\tfeature_vectors = feature_vectors[:2]\n",
    "\n",
    "ts_i = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "y_proba_1_all_models = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "\tif toy:\n",
    "\t\toutput_dir_i = output_dir + f'results_{ts_i}_toy/'\n",
    "\telse:\n",
    "\t\toutput_dir_i = output_dir + f'results_{ts_i}_{n}/'\n",
    "\tos.makedirs(output_dir_i, exist_ok=True)\n",
    "\n",
    "\tresults = []\n",
    "\t# results_content_validity = []\n",
    "\t\n",
    "\t\n",
    "\n",
    "\tfor feature_vector in feature_vectors:\n",
    "\t\tfor dv in dvs:\n",
    "\t\t\tfor amount_of_clauses in ['25%', '50%', '75%', '100%']:\n",
    "\n",
    "\n",
    "\t\t\t\tif balance_test_set:\n",
    "\t\t\t\t\ty_test_i = y_test_balanced.copy()\n",
    "\t\t\t\t\tX_test_i = X_test_balanced.copy()\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ty_test_i = y_test.copy()\n",
    "\t\t\t\t\tif amount_of_clauses == '100%':\n",
    "\t\t\t\t\t\tX_test_i = X_test.copy()\n",
    "\t\t\t\t\telif amount_of_clauses == '75%':\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tX_test_i = X_test_75.copy()\n",
    "\t\t\t\t\telif amount_of_clauses == '50%':\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tX_test_i = X_test_50.copy()\n",
    "\t\t\t\t\telif amount_of_clauses == '25%':\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tX_test_i = X_test_25.copy()\n",
    "\t\t\t\t\tX_test_i = X_test_i.replace([np.inf, -np.inf], np.nan)\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\tclasses_i = classes[dv]\n",
    "\t\t\t\ty_train_i = [int(n.replace(classes_i[1], '1').replace(classes_i[0], '0')) for n in y_train]\n",
    "\t\t\t\ty_test_i = [int(n.replace(classes_i[1], '1').replace(classes_i[0], '0')) for n in y_test_i]\n",
    "\n",
    "\t\t\t\t# if task == 'classification':\n",
    "\t\t\t\t# \tencoder = LabelEncoder()\n",
    "\t\t\t\t# \t# Fit and transform the labels to integers\n",
    "\t\t\t\t# \ty_train = encoder.fit_transform(y_train)\n",
    "\t\t\t\t# \ty_test = encoder.transform(y_test)\n",
    "\n",
    "\t\t\t\t# X_test_3_dv = X_test_3[X_test_3['y_test']==dv][feature_names]\n",
    "\t\t\t\t# y_test_3_dv = [1]*len(X_test_3_dv)\n",
    "\n",
    "\t\t\t\tif n !='all':\n",
    "\t\t\t\t\tX_train_i = X_train.copy()\n",
    "\t\t\t\t\tX_train_i['y'] = y_train_i\n",
    "\t\t\t\t\tX_train_i = X_train_i.sample(n = n)\n",
    "\t\t\t\t\ty_train_i = X_train_i['y'].values\n",
    "\t\t\t\t\tX_train_i = X_train_i.drop('y', axis=1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tX_train_i = X_train.copy()\n",
    "\t\t\t\tif feature_vector != 'tfidf':\n",
    "\t\t\t\t\tif 'y' in X_test_i.columns or 'dv' in X_test_i.columns or classes_i[1] in X_test_i.columns:\n",
    "\t\t\t\t\t\twarnings.warn(f'y var is in X_test, skipping feature vector {feature_vector}')\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\tfor model_name in model_names: \n",
    "\n",
    "\t\t\t\t\tpipeline = pipelines.get_pipelines(feature_vector, model_name = model_name, random_state=random_seed)\n",
    "\t\t\t\t\tprint(pipeline)\n",
    "\t\t\t\t\n",
    "\t\t\t\t\tif hyperparameter_tuning == None or hyperparameter_tuning == False:\n",
    "\t\t\t\t\t\tpipeline.fit(X_train_i,y_train_i)\n",
    "\t\t\t\t\t\tbest_params = 'No hyperparameter tuning'\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tparameters = hyperparameters.get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\t\t\t\t\tpipeline, best_params = hyperparameters.hyparameter_tuning(pipeline, parameters, X_train_i, y_train_i, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring = scoring,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmethod = 'bayesian', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcv=cv, return_train_score=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tn_iter=32, random_state=random_seed)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Performance\n",
    "\t\t\t\t\tdv_clean = dv.replace(' ','_').capitalize()\n",
    "\t\t\t\t\tif task == 'classification':\n",
    "\t\t\t\t\t\ty_proba = pipeline.predict_proba(X_test_i)       # Get predicted probabilities\n",
    "\t\t\t\t\t\ty_proba_1 = y_proba[:,1]\n",
    "\t\t\t\t\t\ty_proba_1_all_models.append(y_proba_1)\n",
    "\t\t\t\t\t\ty_pred = (y_proba_1>=0.5)*1                   # standard threshold\n",
    "\t\t\t\t\t\toutput_filename = f'{feature_vector}_{model_name}_{classes_i[1]}_{n}_clauses-{amount_of_clauses}'\n",
    "\t\t\t\t\t\tcustom_cr, sklearn_cr, cm_df_meaning, cm_df, cm_df_norm, y_pred_df = metrics_report.save_classification_performance(y_test_i, y_pred, y_proba_1, output_dir_i, output_filename=output_filename,feature_vector=feature_vector, model_name=model_name,best_params = best_params, classes = classes_i,amount_of_clauses=amount_of_clauses, save_output=True)\n",
    "\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# \t# results_i_content_3 = custom_classification_report(y_test_3_dv, y_pred_content_validity_3, y_pred_content_validity_3, output_dir_i,gridsearch=gridsearch,\n",
    "\t\t\t\t\t# \t# \t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=f'{feature_vector}_{model_name}_{dv}_content-validity-3',round_to = 2, ts = ts_i)\n",
    "\t\t\t\t\t# elif task == 'regression':\n",
    "\t\t\t\t\t# \tif gridsearch:\n",
    "\t\t\t\t\t# \t\ty_pred = best_model.predict(X_test)\n",
    "\t\t\t\t\t# \telse:\n",
    "\t\t\t\t\t# \t\ty_pred = pipeline.predict(X_test)\n",
    "\n",
    "\t\t\t\t\t# \tresults_i =regression_report(y_test,y_pred,y_train=y_train,\n",
    "\t\t\t\t\t# \t\t\t\t\t\t\tmetrics_to_report = metrics_to_report,\n",
    "\t\t\t\t\t# \t\t\t\t\t\t\t\tgridsearch=gridsearch,\n",
    "\t\t\t\t\t# \t\t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=model_name, plot = True, save_fig_path = path,n = n, round_to = 2)\n",
    "\t\t\t\t\t# # results_i.to_csv(output_dir_i + f'results_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}_{dv}.csv')\n",
    "\t\t\t\t\tdisplay(custom_cr)\n",
    "\t\t\t\t\tresults.append(custom_cr)\n",
    "\t\t\t\t\t# results_content_validity.append(results_i_content_13)\n",
    "\t\t\t\t\t# results_content_validity.append(results_i_content_3)\n",
    "\t\t\t\t\t# Feature importance\n",
    "\t\t\t\t\t# if feature_vector == 'tfidf':\n",
    "\t\t\t\t\t# \tif model_name in ['XGBRegressor']:\n",
    "\t\t\t\t\t# \t\twarnings.warn('Need to add code to parse XGBoost feature importance dict')\n",
    "\t\t\t\t\t# \telse:\n",
    "\t\t\t\t\t# \t\tfeature_importances = tfidf_feature_importances(pipeline, top_k = 50, savefig_path = output_dir_i + f'feature_importance_{feature_vector}_{model_name}_{n}_{ts_i}_{dv}')\n",
    "\t\t\t\t\t# else:\n",
    "\t\t\t\t\tfeature_names = X_train_i.columns\n",
    "\t\t\t\t\t# TODO add correlation with DV to know direction\n",
    "\t\t\t\t\tfeature_importance = metrics_report.generate_feature_importance_df(pipeline, model_name,feature_names,  model_name_in_pipeline='model')\n",
    "\t\t\t\t\tif str(feature_importance) != 'None':       # I only implemented a few methods for a few models\n",
    "\t\t\t\t\t\tfeature_importance.to_csv(output_dir_i + f'feature_importance_{output_filename}.csv', index = False)        \n",
    "\t\t\t\t\t\t# display(feature_importance.iloc[:50])\n",
    "\t\t\t\t\t\n",
    "\tresults_df = pd.concat(results)\n",
    "\tresults_df = results_df.reset_index(drop=True)\n",
    "\toutput_filename = f'{feature_vector}_{classes_i[1]}_{n}'\n",
    "\n",
    "\tresults_df.to_csv(output_dir_i + f'results_{output_filename}.csv', index=False)\n",
    "\n",
    "\t# results_df_content_validity = pd.concat(results_content_validity)\n",
    "\t# results_df_content_validity = results_df_content_validity.reset_index(drop=True)\n",
    "\t# results_df_content_validity.to_csv(output_dir_i + f'results_content_validity_{n}_{ts_i}.csv', index=False)\n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\t# NaN analysis\n",
    "\tif type(X_train_i) == pd.core.frame.DataFrame:\n",
    "\t\tdf = X_train_i.copy()\n",
    "\t\t# Find the column and index of NaN values\n",
    "\t\tnan_indices = df.index[df.isnull().any(axis=1)].tolist()\n",
    "\t\tnan_columns = df.columns[df.isnull().any()].tolist()\n",
    "\t\t# print(\"Indices of NaN values:\", nan_indices)\n",
    "\t\tprint(\"Columns with NaN values:\", nan_columns)\n",
    "\t\tprint(df.size)\n",
    "\t\tnans = df.isna().sum().sum()\n",
    "\t\tprint('% of nans:', np.round(nans/df.size,3))\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d68add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "infinite_rows = np.isinf(X_test_i).any(axis=1)\n",
    "print(\"Rows with infinite values:\")\n",
    "print(X_test_i[infinite_rows].index)\n",
    "\n",
    "\n",
    "# X = X.replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d49d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8436b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_filename = f'results_cts_51_srl_prototypes_active_rescue_all.csv'\n",
    "results_dir = f'results_24-03-22T18-05-48_all_LGBMClassifier/'\n",
    "\n",
    "results_df = pd.read_csv('./data/output/active_rescue/' + results_dir + results_filename)\n",
    "results_df_binary = results_df[(results_df['Class']==1) & (results_df['Amount of clauses']=='binary')]\n",
    "results_df_binary.rename(columns = {'Average':f'% of messages'}, inplace = True)\n",
    "results_df_binary.rename(columns = {'Amount of clauses':'Average'}, inplace = True)\n",
    "\n",
    "# plot sensitivity, specificity, precision, ROC AUC, PR AUC\n",
    "results_df_binary\n",
    "# use lines with dots \n",
    "\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Sensitivity\", x=f\"% of messages\", label ='Sensitivity')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Specificity\", x=f\"% of messages\", label = 'Specificity')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Precision\", x=f\"% of messages\",\tlabel = 'Precision')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"ROC AUC\", x=f\"% of messages\",\tlabel = 'ROC AUC')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"PR AUC\", x=f\"% of messages\",\tlabel = 'PR AUC', color = 'pink')\n",
    "# horizontal line\n",
    "\n",
    "plt.axhline(y=0.22, xmin = 0.04,xmax=0.96, linestyle='--', label = 'Baseline for PR AUC\\nall rescues (P/N)',  color = 'pink')\n",
    "plt.ylabel('Prediction of active rescue')\n",
    "# place legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "def  nrui_score(net_benefit, net_benefit_treat_all, threshold):\n",
    "\t#  net reduction in unnecessary interventions \n",
    "\tnrui = 100* (net_benefit - net_benefit_treat_all) / (threshold / (1 - threshold))\n",
    "\treturn nrui\n",
    "\n",
    "\n",
    "# Assuming the true positive, false positive rates and pt are provided or computed beforehand\n",
    "# This function calculates the net benefit\n",
    "def calculate_net_benefit(tp, fp, N, pt):\n",
    "\tnet_benefit = (tp/N) - (fp / N) * (pt / (1 - pt))\n",
    "\treturn net_benefit\n",
    "\n",
    "\n",
    "\n",
    "for i, amount_of_clauses, model_name in zip(range(4), ['25%', '50%', '75%', '100%'], ['LGBMClassifier', 'LGBMClassifier', 'LGBMClassifier', 'LGBMClassifier']):\n",
    "\ty_proba_1 = y_proba_1_all_models[i]\n",
    "\n",
    "\n",
    "\t# compute precision and recall for each threshold\n",
    "\tthresholds = np.arange(0.005, 0.2, 0.005)\n",
    "\tnet_benefits = []\n",
    "\tnet_benefits_all_1 = []\n",
    "\tnet_benefits_all_0 = []\n",
    "\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4,4)}) #w\n",
    "\tsns.set_style(\"white\")\n",
    "\tnruis = []\n",
    "\tnruis_all_0 = []\n",
    "\n",
    "\tfor threshold in thresholds:\n",
    "\t\tN = len(y_proba_1)\n",
    "\t\ttn, fp, fn, tp = metrics.confusion_matrix(y_test_i, y_proba_1 > threshold).ravel()\n",
    "\t\tnet_benefit = calculate_net_benefit(tp, fp, N, threshold)\n",
    "\t\tnet_benefits.append(net_benefit)\n",
    "\n",
    "\t\ttn_all_1, fp_all_1, fn_all_1, tp_all_1 = metrics.confusion_matrix(y_test_i, y_proba_1 > 0).ravel()\n",
    "\t\tnet_benefit_all_1 = calculate_net_benefit(tp_all_1, fp_all_1, N, threshold)\n",
    "\t\tnet_benefits_all_1.append(net_benefit_all_1)\n",
    "\n",
    "\t\ttn_all_0, fp_all_0, fn_all_0, tp_all_0 = metrics.confusion_matrix(y_test_i, y_proba_1 > 1).ravel()\n",
    "\t\tnet_benefit_all_0 = calculate_net_benefit(tp_all_0, fp_all_0, N, threshold)\n",
    "\t\tnet_benefits_all_0.append(net_benefit_all_0)\n",
    "\n",
    "\t\tnrui = nrui_score(net_benefit, net_benefit_all_1, threshold)\n",
    "\t\tnrui_all_0 = nrui_score(net_benefit_all_0, net_benefit_all_1, threshold)\n",
    "\n",
    "\t\tnruis.append(nrui)\n",
    "\t\tnruis_all_0.append(nrui_all_0)\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4,4)})\n",
    "\tsns.set_style(\"white\")\n",
    "\tthresholds_perc = [f'{np.round(n * 100, 1)}' for n in thresholds]\n",
    "\tplt.title(amount_of_clauses)\n",
    "\tplt.plot(thresholds, net_benefits, label = model_name)\n",
    "\tplt.plot(thresholds, net_benefits_all_1, label = 'Rescue all', color = 'pink')\n",
    "\tplt.plot(thresholds, net_benefits_all_0, label = 'Rescue None')\n",
    "\t# display xticklabels, which are proportions, as probabilities (multiply by 100)\n",
    "\tskip  = 6\n",
    "\tplt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "\t# plt.title('Net benefit')\n",
    "\tplt.xlabel('Threshold probability')\n",
    "\tplt.ylabel('Net Benefit')\n",
    "\tplt.legend()\n",
    "\t# plot horizontal line at 0\n",
    "\t# plt.axhline(y=0, color='gray', linestyle='--')\n",
    "\tplt.show()\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4,4)})\n",
    "\tsns.set_style(\"white\")\n",
    "\tplt.title(amount_of_clauses)\n",
    "\tplt.plot(thresholds, nruis, label = model_name)\n",
    "\t# plt.plot(thresholds, nruis_all_0, label = 'Rescue None')\n",
    "\tplt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "\tplt.xlabel('Threshold probability')\n",
    "\t# make ylabel font smaller \n",
    "\n",
    "\tplt.ylabel('Net reduction in interventions\\nper 100 individuals') # net_benefits_all_0\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54f134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6054e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = f'results_cts_51_srl_prototypes_active_rescue_all.csv'\n",
    "results_dir = f'results_24-03-22T17-32-20_all/'\n",
    "\n",
    "results_df = pd.read_csv('./data/output/active_rescue/' + results_dir + results_filename)\n",
    "results_df_binary = results_df[(results_df['Class']==1) & (results_df['Amount of clauses']=='binary')]\n",
    "results_df_binary.rename(columns = {'Average':f'% of messages'}, inplace = True)\n",
    "results_df_binary.rename(columns = {'Amount of clauses':'Average'}, inplace = True)\n",
    "\n",
    "# plot sensitivity, specificity, precision, ROC AUC, PR AUC\n",
    "results_df_binary\n",
    "# use lines with dots \n",
    "\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Sensitivity\", x=f\"% of messages\", label ='Sensitivity')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Specificity\", x=f\"% of messages\", label = 'Specificity')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"Precision\", x=f\"% of messages\",\tlabel = 'Precision')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"ROC AUC\", x=f\"% of messages\",\tlabel = 'ROC AUC')\n",
    "sns.lineplot(data=\tresults_df_binary, y=\"PR AUC\", x=f\"% of messages\",\tlabel = 'PR AUC', color = 'pink')\n",
    "# horizontal line\n",
    "\n",
    "plt.axhline(y=0.22, xmin = 0.04,xmax=0.96, linestyle='--', label = 'Baseline for PR AUC\\nall rescues (P/N)',  color = 'pink')\n",
    "plt.ylabel('Prediction of active rescue')\n",
    "# place legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "def  nrui_score(net_benefit, net_benefit_treat_all, threshold):\n",
    "\t#  net reduction in unnecessary interventions \n",
    "\tnrui = 100* (net_benefit - net_benefit_treat_all) / (threshold / (1 - threshold))\n",
    "\treturn nrui\n",
    "\n",
    "\n",
    "# Assuming the true positive, false positive rates and pt are provided or computed beforehand\n",
    "# This function calculates the net benefit\n",
    "def calculate_net_benefit(tp, fp, N, pt):\n",
    "\tnet_benefit = (tp/N) - (fp / N) * (pt / (1 - pt))\n",
    "\treturn net_benefit\n",
    "\n",
    "\n",
    "\n",
    "for i, amount_of_clauses, model_name in zip(range(4), ['25%', '50%', '75%', '100%'], ['LogisticRegression', 'LogisticRegression', 'LogisticRegression', 'LogisticRegression']):\n",
    "\ty_proba_1 = y_proba_1_all_models[i]\n",
    "\n",
    "\n",
    "\t# compute precision and recall for each threshold\n",
    "\tthresholds = np.arange(0.005, 0.2, 0.005)\n",
    "\tnet_benefits = []\n",
    "\tnet_benefits_all_1 = []\n",
    "\tnet_benefits_all_0 = []\n",
    "\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4,4)}) #w\n",
    "\tsns.set_style(\"white\")\n",
    "\tnruis = []\n",
    "\tnruis_all_0 = []\n",
    "\n",
    "\tfor threshold in thresholds:\n",
    "\t\tN = len(y_proba_1)\n",
    "\t\ttn, fp, fn, tp = metrics.confusion_matrix(y_test_i, y_proba_1 > threshold).ravel()\n",
    "\t\tnet_benefit = calculate_net_benefit(tp, fp, N, threshold)\n",
    "\t\tnet_benefits.append(net_benefit)\n",
    "\n",
    "\t\ttn_all_1, fp_all_1, fn_all_1, tp_all_1 = metrics.confusion_matrix(y_test_i, y_proba_1 > 0).ravel()\n",
    "\t\tnet_benefit_all_1 = calculate_net_benefit(tp_all_1, fp_all_1, N, threshold)\n",
    "\t\tnet_benefits_all_1.append(net_benefit_all_1)\n",
    "\n",
    "\t\ttn_all_0, fp_all_0, fn_all_0, tp_all_0 = metrics.confusion_matrix(y_test_i, y_proba_1 > 1).ravel()\n",
    "\t\tnet_benefit_all_0 = calculate_net_benefit(tp_all_0, fp_all_0, N, threshold)\n",
    "\t\tnet_benefits_all_0.append(net_benefit_all_0)\n",
    "\n",
    "\t\tnrui = nrui_score(net_benefit, net_benefit_all_1, threshold)\n",
    "\t\tnrui_all_0 = nrui_score(net_benefit_all_0, net_benefit_all_1, threshold)\n",
    "\n",
    "\t\tnruis.append(nrui)\n",
    "\t\tnruis_all_0.append(nrui_all_0)\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4,4)})\n",
    "\tsns.set_style(\"white\")\n",
    "\tthresholds_perc = [f'{np.round(n * 100, 1)}' for n in thresholds]\n",
    "\tplt.title(amount_of_clauses)\n",
    "\tplt.plot(thresholds, net_benefits, label = model_name)\n",
    "\tplt.plot(thresholds, net_benefits_all_1, label = 'Rescue all', color = 'pink')\n",
    "\tplt.plot(thresholds, net_benefits_all_0, label = 'Rescue None')\n",
    "\t# display xticklabels, which are proportions, as probabilities (multiply by 100)\n",
    "\tskip  = 6\n",
    "\tplt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "\t# plt.title('Net benefit')\n",
    "\tplt.xlabel('Threshold probability')\n",
    "\tplt.ylabel('Net Benefit')\n",
    "\tplt.legend()\n",
    "\t# plot horizontal line at 0\n",
    "\t# plt.axhline(y=0, color='gray', linestyle='--')\n",
    "\tplt.show()\n",
    "\n",
    "\tplt.clf()\n",
    "\tsns.set(rc={'figure.figsize':(4.5,4)})\n",
    "\tsns.set_style(\"white\")\n",
    "\tplt.title(amount_of_clauses)\n",
    "\tplt.plot(thresholds, nruis, label = model_name)\n",
    "\t# plt.plot(thresholds, nruis_all_0, label = 'Rescue None')\n",
    "\tplt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "\tplt.xlabel('Threshold probability')\n",
    "\t# make ylabel font smaller \n",
    "\n",
    "\tplt.ylabel('Net reduction in interventions\\nper 100 individuals') # net_benefits_all_0\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4534c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_opt = (y_proba_1>=0.847)*1                   # standard threshold\n",
    "y_pred_opt\n",
    "\n",
    "custom_cr, sklearn_cr, cm_df_meaning, cm_df, cm_df_norm, y_pred_df = metrics_report.save_classification_performance(y_test_i, y_pred_opt, y_proba_1, output_dir_i, output_filename=output_filename,feature_vector=feature_vector, model_name=model_name,best_params = best_params, classes = classes_i, save_output=False)\n",
    "print(y_test_i, y_pred)\n",
    "display(custom_cr)\n",
    "display(sklearn_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2*0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382563ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52651ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc072211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def  nrui_score(net_benefit, net_benefit_treat_all, threshold):\n",
    "\t#  net reduction in unnecessary interventions \n",
    "\tnrui = 100* (net_benefit - net_benefit_treat_all) / (threshold / (1 - threshold))\n",
    "\treturn nrui\n",
    "\n",
    "\n",
    "# Assuming the true positive, false positive rates and pt are provided or computed beforehand\n",
    "# This function calculates the net benefit\n",
    "def calculate_net_benefit(tp, fp, N, pt):\n",
    "\tnet_benefit = (tp/N) - (fp / N) * (pt / (1 - pt))\n",
    "\treturn net_benefit\n",
    "\n",
    "# compute precision and recall for each threshold\n",
    "thresholds = np.arange(0.005, 0.2, 0.005)\n",
    "net_benefits = []\n",
    "net_benefits_all_1 = []\n",
    "net_benefits_all_0 = []\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "sns.set(rc={'figure.figsize':(4,4)}) #w\n",
    "sns.set_style(\"white\")\n",
    "nruis = []\n",
    "nruis_all_0 = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "\tN = len(y_proba_1)\n",
    "\ttn, fp, fn, tp = metrics.confusion_matrix(y_test_i, y_proba_1 > threshold).ravel()\n",
    "\tnet_benefit = calculate_net_benefit(tp, fp, N, threshold)\n",
    "\tnet_benefits.append(net_benefit)\n",
    "\n",
    "\ttn_all_1, fp_all_1, fn_all_1, tp_all_1 = metrics.confusion_matrix(y_test_i, y_proba_1 > 0).ravel()\n",
    "\tnet_benefit_all_1 = calculate_net_benefit(tp_all_1, fp_all_1, N, threshold)\n",
    "\tnet_benefits_all_1.append(net_benefit_all_1)\n",
    "\n",
    "\ttn_all_0, fp_all_0, fn_all_0, tp_all_0 = metrics.confusion_matrix(y_test_i, y_proba_1 > 1).ravel()\n",
    "\tnet_benefit_all_0 = calculate_net_benefit(tp_all_0, fp_all_0, N, threshold)\n",
    "\tnet_benefits_all_0.append(net_benefit_all_0)\n",
    "\n",
    "\tnrui = nrui_score(net_benefit, net_benefit_all_1, threshold)\n",
    "\tnrui_all_0 = nrui_score(net_benefit_all_0, net_benefit_all_1, threshold)\n",
    "\n",
    "\tnruis.append(nrui)\n",
    "\tnruis_all_0.append(nrui_all_0)\n",
    "\n",
    "plt.clf()\n",
    "sns.set(rc={'figure.figsize':(4,4)})\n",
    "sns.set_style(\"white\")\n",
    "thresholds_perc = [f'{np.round(n * 100, 1)}' for n in thresholds]\n",
    "plt.plot(thresholds, net_benefits, label = 'LGBMClassifier')\n",
    "plt.plot(thresholds, net_benefits_all_1, label = 'Rescue all')\n",
    "plt.plot(thresholds, net_benefits_all_0, label = 'Rescue None')\n",
    "# display xticklabels, which are proportions, as probabilities (multiply by 100)\n",
    "skip  = 6\n",
    "plt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "# plt.title('Net benefit')\n",
    "plt.xlabel('Threshold probability')\n",
    "plt.ylabel('Net Benefit')\n",
    "plt.legend()\n",
    "# plot horizontal line at 0\n",
    "# plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "sns.set(rc={'figure.figsize':(4,4)})\n",
    "sns.set_style(\"white\")\n",
    "plt.plot(thresholds, nruis, label = 'LGBMClassifier')\n",
    "# plt.plot(thresholds, nruis_all_0, label = 'Rescue None')\n",
    "plt.xticks(thresholds[::skip], thresholds_perc[::skip])\n",
    "plt.xlabel('Threshold probability')\n",
    "# make ylabel font smaller \n",
    "\n",
    "plt.ylabel('Net reduction in interventions\\nper 100 individuals', fontsize = 10) # net_benefits_all_0\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  nrui_score(net_benefit, net_benefit_treat_all, threshold):\n",
    "\t#  net reduction in unnecessary interventions \n",
    "\tnrui = 100* (net_benefit - net_benefit_treat_all) / (threshold / (1 - threshold))\n",
    "\treturn nrui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65729ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61be3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ef37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(Counter(y_true)).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(dict(Counter(y_true)).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lofo-importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afe257",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymc3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb576e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)[:, None]\n",
    "y = 2.5 * X.squeeze() + np.random.normal(scale=1.0, size=100)\n",
    "\n",
    "# Bayesian linear regression model\n",
    "with pm.Model() as model:\n",
    "    # Priors for unknown model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=10)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta * X\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n",
    "\n",
    "    # Posterior distribution and sampling\n",
    "    trace = pm.sample(1000, return_inferencedata=False)\n",
    "\n",
    "# Posterior predictive checks to obtain predictions and uncertainties\n",
    "with model:\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace)\n",
    "\n",
    "# Extract predicted values and uncertainties\n",
    "predictions = posterior_pred['Y_obs'].mean(axis=0)\n",
    "uncertainties = posterior_pred['Y_obs'].std(axis=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, label='Observed data')\n",
    "plt.plot(X, predictions, label='Predicted mean')\n",
    "plt.fill_between(X.squeeze(), predictions - uncertainties, predictions + uncertainties, alpha=0.2, label='Uncertainty interval')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(predictions)\n",
    "print(uncertainties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lofo import LOFOImportance, Dataset, plot_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251ba25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b949435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_i['active_rescue'] = y_train_i    \n",
    "# dataset = Dataset(df=X_train_i, target=\"active_rescue\", features=[col for col in X_train_i.columns if col != \"active_rescue\"], \n",
    "# \t\t\t\t  auto_group_threshold=0.8)\n",
    "\n",
    "# lofo_imp = LOFOImportance(dataset, cv=3, scoring=scoring, model=pipeline)\n",
    "# lofo_imp.get_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_lofo_importance(target):\n",
    "#     cv = KFold(n_splits=7, shuffle=True, random_state=17)\n",
    "\n",
    "#     dataset = Dataset(df=df[df[target].notnull()], target=target, features=loading_features,\n",
    "#                       feature_groups={\"fnc\": df[df[target].notnull()][fnc_features].values\n",
    "#                       })\n",
    "\n",
    "#     model = Ridge(alpha=0.01)\n",
    "#     lofo_imp = LOFOImportance(X, cv=cv, scoring=\"f1\", model=pipeline)\n",
    "\n",
    "#     return lofo_imp.get_importance()\n",
    "\n",
    "# plot_importance(get_lofo_importance(target=\"domain1_var1\"), figsize=(8, 8), kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8DQVD-CRKPs",
   "metadata": {
    "id": "r8DQVD-CRKPs"
   },
   "source": [
    "# Explainability: construct tokens vs. doc tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/input/ctl/embeddings/'+'X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_cosine_similarities.pickle', 'rb') as handle:\n",
    "\tcosine_scores_per_doc = dill.load(handle)\n",
    "\t# keys are '1508885_Passive suicidal ideation'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "srl = dill.load(open(\"./../lexicon/data/input/lexicons/suicide_risk_lexicon_validated_prototypical_tokens_24-03-06T00-47-30.pickle\", \"rb\"))\n",
    "constructs_to_measure = srl_constructs.constructs_in_order\n",
    "\n",
    "construct_tokens_d = {}\n",
    "for construct in srl.constructs.keys():\n",
    "\ttokens = srl.constructs[construct]['tokens']                      \n",
    "\tconstruct_tokens_d[construct] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88914bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cosine_df(doc_id, construct, docs_clauses, construct_tokens_d, cosine_scores_per_doc):\n",
    "  doc_clauses_i = docs_clauses[doc_id]\n",
    "  construct_tokens_i = construct_tokens_d[construct]\n",
    "  df = pd.DataFrame(cosine_scores_per_doc[f'{doc_id}_{construct}'], index = construct_tokens_i, columns = doc_clauses_i)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca91e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clauses = dict(zip(test_subset['conversation_id'].values, test_subset['message_with_interaction_clean_clauses'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70da2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cosine_scores_per_doc.keys())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clauses_i = docs_clauses[convo_id]\n",
    "doc_clauses_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61395e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_tokens_i = construct_tokens_d[construct]\n",
    "construct_tokens_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a156a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores_per_doc[f'{convo_id}_{construct}'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(doc_clauses_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cosine_similarities, index = construct_tokens_i, columns = eval(doc_clauses_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e595926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return index, column of highest value\n",
    "df.idxmax(axis=1), df.idxmax(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_message(convo_id, construct, docs_clauses, construct_tokens_d, cosine_scores_per_doc):\n",
    "\tcosine_similarities = cosine_scores_per_doc[str(convo_id)+'_'+construct].copy()\n",
    "\tdoc_clauses_i = eval(docs_clauses[convo_id])\n",
    "\tconstruct_tokens_i = construct_tokens_d[construct]\n",
    "\tdf = pd.DataFrame(cosine_similarities, index = construct_tokens_i, columns = doc_clauses_i)\n",
    "\tmax_value = df.max().max()\n",
    "\t# Find the column and index of the maximum value\n",
    "\tcolumn = df.max().idxmax()\n",
    "\tindex = df[column].idxmax()\n",
    "\t# Return [value, column, index]\n",
    "\tmax_col_index = [max_value, column, index]\n",
    "\treturn df, max_col_index\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f447ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_examples = []\n",
    "\n",
    "\n",
    "for construct in srl_constructs.constructs_in_order:\n",
    "  high_scoring_docs_convo_ids = X_test_i[construct].nlargest(20).index.tolist()\n",
    "  for convo_id in high_scoring_docs_convo_ids:\n",
    "\tdf, max_col_index = highest_message(convo_id, construct, docs_clauses, construct_tokens_d, cosine_scores_per_doc)\n",
    "\tconstruct_examples.append([construct] + max_col_index)\n",
    "\t\n",
    "construct_examples_df = pd.DataFrame(construct_examples, columns = ['Construct', 'Value', 'Message', 'Construct token'])\n",
    "# construct_examples_df.round(2).\n",
    "construct_examples_df['temp_message_token'] = construct_examples_df['Message']+' '+construct_examples_df['Construct token']\n",
    "construct_examples_df['temp_message_token'] = construct_examples_df['temp_message_token'].str.lower()\n",
    "construct_examples_df = construct_examples_df.drop_duplicates(subset=['temp_message_token'])\n",
    "construct_examples_df = construct_examples_df.drop(columns = ['temp_message_token'])\n",
    "construct_examples_df = construct_examples_df[construct_examples_df['Value']>0.55]\n",
    "construct_examples_df = construct_examples_df.round(2)\n",
    "construct_examples_df\n",
    "\n",
    "\t\n",
    "construct_examples_df.to_csv('./data/output/active_rescue/construct_examples.csv', index = False)\n",
    "  \n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TNp2r9vHOWbc",
   "metadata": {
    "id": "TNp2r9vHOWbc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DIPn83clB-Ky",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "DIPn83clB-Ky",
    "outputId": "2858b4fb-ed3d-425f-a67d-ebab10ca9082"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "construct = 'gratitude'\n",
    "doc_id = 65\n",
    "\n",
    "construct = 'anger'\n",
    "doc_id = 22\n",
    "\n",
    "construct = 'annoyance'\n",
    "doc_id = 64\n",
    "'''\n",
    "\n",
    "construct = 'gratitude'\n",
    "doc_id = 65\n",
    "\n",
    "\n",
    "\n",
    "df = return_cosine_df(doc_id, construct, docs_clauses, construct_tokens_d,X_test_cosine_scores_per_doc)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TdLvzMvD1NOM",
   "metadata": {
    "id": "TdLvzMvD1NOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc716b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
