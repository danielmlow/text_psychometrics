{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074a219-4bab-455b-b1ea-19f3c7867945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef2ef8-de02-447b-9167-b2a86062fd50",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89352df-84c7-45cb-9d67-68e3c52dd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Authors: Daniel M. Low\n",
    "License: See license in github repository\n",
    "'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.options.display.width = 0\n",
    "\n",
    "\n",
    "# os.chdir(os.path.dirname(__file__)) # Set working directory to current file\n",
    "\n",
    "on_colab = False\n",
    "\n",
    "if on_colab:\n",
    "  from google.colab import drive\n",
    "  project_name = 'project_name'\n",
    "  drive.mount('/content/drive')\n",
    "  input_dir = f'/content/drive/MyDrive/datum/{project_name}/data/input/'\n",
    "  output_dir = f'/content/drive/MyDrive/datum/{project_name}/data/output/'\n",
    "else:\n",
    "  input_dir = './data/'\n",
    "  output_dir = './data/output/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693940d7-a13b-44cc-b90d-3809d17d4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "balance = True # balance training set by downsampling\n",
    "task = 'classification'\n",
    "# target = 'immiment_risk'\n",
    "normalize_lexicon = True\n",
    "\n",
    "\n",
    "\n",
    "if task == 'classification':\n",
    "\tdv = 'suicide_ladder_classification'\n",
    "\tif target == 'suicidal_desire':\n",
    "\t\tbalance_values = ['nonsuicidal','suicidal_desire']\n",
    "\telif target == 'imminent_risk':\n",
    "\t\tbalance_values = ['suicidal_desire','imminent_risk']\n",
    "\tsmallest_value = 'imminent_risk'\n",
    "\tn = 1893\n",
    "\n",
    "elif task == 'regression':\n",
    "\n",
    "\t# config\n",
    "\tdv = 'suicide_ladder_a'\n",
    "\tbalance_values = [1,2,3]\n",
    "\tsmallest_value = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec373cd-cdd9-4e47-b4cd-95501255b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_feature_importance_df(trained_model, model_name, feature_names, xgboost_method = 'weight', model_name_in_pipeline = 'estimator', lgbm_method='split'):\n",
    "\t'''\n",
    "\tFunction to generate feature importance table for methods that use .coef_ from sklearn\n",
    "\tas well as xgboost models.\n",
    "\tboth using sklearn pipelines that go into GridsearchCV, where we need to \n",
    "\tfirst access the best_estimator to access, for example, the coefficients.\n",
    "\t\n",
    "\ttrained_model: sklearn type model object fit to data\n",
    "\tmodel_name: str among the ones that appear below\n",
    "\txgboost_method: str, there are a few options: https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster.get_score     \n",
    "\t'''\n",
    "\t\n",
    "\t#  Feature importance using coefficients for linear models and gini \n",
    "\tif model_name in ['SGDRegressor', 'Ridge', 'Lasso', 'LogisticRegression', 'LinearSVC']:\n",
    "\t\ttry:\n",
    "\t\t\tcoefs = list(trained_model.named_steps['model'].coef_)\n",
    "\t\texcept:\n",
    "\t\t\tcoefs = list(trained_model.best_estimator_.named_steps[model_name_in_pipeline].coef_)                     # Obtain coefficients from GridSearch\n",
    "\t\ttry:\n",
    "\t\t\tcoefs= pd.DataFrame(coefs,index = ['Coef.'], columns = feature_names).T # make DF\n",
    "\t\texcept:\n",
    "\t\t\tcoefs= pd.DataFrame(coefs,index=feature_names, columns = ['Coef.']) # make DF\n",
    "\t\tcoefs['Abs. Coef.'] = coefs['Coef.'].abs()  # add column with absolute values to sort by, both positive and negative values are important. \n",
    "\t\tcoefs= coefs.sort_values('Abs. Coef.', ascending=False).reset_index() # sort by abs value and reset index to add a feature name column\n",
    "\t\tcoefs= coefs.drop(['Abs. Coef.'], axis=1)   # drop abs value, it's job is done\n",
    "\t\tcoefs.index +=1                             # Importance for publication, start index with 1 , as in 1st, 2nd, 3rd\n",
    "\t\tcoefs= coefs.reset_index()                  # turn index into column\n",
    "\t\tcoefs.columns= ['Importance', 'Feature', 'Coef.'] # Clean column names\n",
    "\t\tfeature_importance = coefs.copy()\n",
    "\t\treturn feature_importance\n",
    "\t\t\n",
    "\telif model_name in ['LGBMRegressor', 'LGBMClassifier']:    \n",
    "\t\ttry:\n",
    "\t\t\timportance_split = trained_model.named_steps[model_name_in_pipeline].booster_.feature_importance(importance_type='split')\n",
    "\t\t\timportance_gain = trained_model.named_steps[model_name_in_pipeline].booster_.feature_importance(importance_type='gain')\n",
    "\t\t\t# feature_names = trained_model.named_steps[model_name_in_pipeline].booster_.feature_name()\n",
    "\t\texcept:\n",
    "\t\t\timportance_split = trained_model.best_estimator_.named_steps[model_name_in_pipeline].booster_.feature_importance(importance_type='split')\n",
    "\t\t\timportance_gain = trained_model.best_estimator_.named_steps[model_name_in_pipeline].booster_.feature_importance(importance_type='gain')\n",
    "\t\t\t# feature_names = trained_model.best_estimator_.named_steps[model_name_in_pipeline].booster_.feature_name()\n",
    "\t\t\n",
    "\t\tfeature_importance = pd.DataFrame({'feature': feature_names, 'split': importance_split, 'gain': importance_gain})\n",
    "\t\t\n",
    "\t\t# Sort by gain\n",
    "\t\tfeature_importance = feature_importance.sort_values('gain', ascending=False)\n",
    "\t\treturn feature_importance\n",
    "\n",
    "\t\t\n",
    "\n",
    "\telif model_name in ['XGBRegressor', 'XGBClassifier']:\n",
    "\t\t# WARNING it will not return values for features that weren't used: if feature 3 wasn't used there will not be a f3 in the results        \n",
    "\t\ttry:\n",
    "\t\t\tfeature_importance = trained_model.named_steps[model_name_in_pipeline].get_booster().get_score(importance_type=xgboost_method )\n",
    "\t\texcept:\n",
    "\t\t\tfeature_importance = trained_model.best_estimator_.named_steps[model_name_in_pipeline].get_booster().get_score(importance_type=xgboost_method )\n",
    "\t\tfeature_importance_keys = list(feature_importance .keys())\n",
    "\t\tfeature_importance_values = list(feature_importance .values())    \n",
    "\t\tfeature_importance = pd.DataFrame(feature_importance_values,index=feature_importance_keys) # make DF\n",
    "\t\tfeature_importance = feature_importance .sort_values(0, ascending=False)\n",
    "\t\tfeature_importance = feature_importance.reset_index()\n",
    "\t\n",
    "\t\tfeature_importance.index +=1\n",
    "\t\tfeature_importance = feature_importance.reset_index()\n",
    "\t\tfeature_importance\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tfeature_importance.columns = ['Importance', 'Feature', xgboost_method.capitalize()]\n",
    "\t\t\n",
    "\t\tfeature_name_mapping = {}\n",
    "\t\tfor i, feature_name_i in enumerate(feature_names):\n",
    "\t\t\tfeature_name_mapping[f'f{i}'] = feature_name_i\n",
    "\t\t\n",
    "\t\t# Or manually edit here: \n",
    "\t\t# feature_name_mapping = {'f0': 'Unnamed: 0', 'f1': 'Adult Mortality', 'f2': 'infant deaths', 'f3': 'percentage expenditure', 'f4': 'Hepatitis B', 'f5': 'Measles ', 'f6': ' BMI ', 'f7': 'under-five deaths ', 'f8': 'Polio', 'f9': 'Diphtheria ', 'f10': ' HIV/AIDS', 'f11': ' thinness  1-19 years', 'f12': ' thinness 5-9 years', 'f13': 'Developing'}\n",
    "\t\t\n",
    "\t\tfeature_importance['Feature'] = feature_importance['Feature'].map(feature_name_mapping )\n",
    "\t# Todo: add feature_importances_ for sklearn tree based models\n",
    "\t# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity\n",
    "\t\n",
    "\t\n",
    "\t\treturn feature_importance\n",
    "\telse:\n",
    "\t\twarnings.warn(f'model not specificied for feature importance: {model_name}')\n",
    "\t\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13503de4-7a56-4321-a726-bbdfee674ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "liwc_nonsemantic = ['WC','WPS',\n",
    " 'BigWords',\n",
    " 'Dic',\n",
    " 'Linguistic',\n",
    " 'function',\n",
    " 'pronoun',\n",
    " 'ppron',\n",
    " 'i',\n",
    " 'we',\n",
    " 'you',\n",
    " 'shehe',\n",
    " 'they',\n",
    " 'ipron',\n",
    " 'det',\n",
    " 'article',\n",
    " 'number',\n",
    " 'prep',\n",
    " 'auxverb',\n",
    " 'adverb',\n",
    " 'conj',\n",
    " 'negate',\n",
    " 'verb',\n",
    " 'adj',\n",
    " 'quantity',\n",
    " 'AllPunc',\n",
    " 'Period',\n",
    " 'Comma',\n",
    " 'QMark',\n",
    " 'Exclam',\n",
    " 'Apostro',\n",
    " 'OtherP'\n",
    "]\n",
    "\n",
    "liwc_semantic = ['Analytic',\n",
    " 'Clout',\n",
    " 'Authentic',\n",
    " 'Tone', \n",
    " 'Drives',\n",
    " 'affiliation',\n",
    " 'achieve',\n",
    " 'power',\n",
    " 'Cognition',\n",
    " 'allnone',\n",
    " 'cogproc',\n",
    " 'insight',\n",
    " 'cause',\n",
    " 'discrep',\n",
    " 'tentat',\n",
    " 'certitude',\n",
    " 'differ',\n",
    " 'memory',\n",
    " 'Affect',\n",
    " 'tone_pos',\n",
    " 'tone_neg',\n",
    " 'emotion',\n",
    " 'emo_pos',\n",
    " 'emo_neg',\n",
    " 'emo_anx',\n",
    " 'emo_anger',\n",
    " 'emo_sad',\n",
    " 'swear',\n",
    " 'Social',\n",
    " 'socbehav',\n",
    " 'prosocial',\n",
    " 'polite',\n",
    " 'conflict',\n",
    " 'moral',\n",
    " 'comm',\n",
    " 'socrefs',\n",
    " 'family',\n",
    " 'friend',\n",
    " 'female',\n",
    " 'male',\n",
    " 'Culture',\n",
    " 'politic',\n",
    " 'ethnicity',\n",
    " 'tech',\n",
    " 'Lifestyle',\n",
    " 'leisure',\n",
    " 'home',\n",
    " 'work',\n",
    " 'money',\n",
    " 'relig',\n",
    " 'Physical',\n",
    " 'health',\n",
    " 'illness',\n",
    " 'wellness',\n",
    " 'mental',\n",
    " 'substances',\n",
    " 'sexual',\n",
    " 'food',\n",
    " 'death',\n",
    " 'need',\n",
    " 'want',\n",
    " 'acquire',\n",
    " 'lack',\n",
    " 'fulfill',\n",
    " 'fatigue',\n",
    " 'reward',\n",
    " 'risk',\n",
    " 'curiosity',\n",
    " 'allure',\n",
    " 'Perception',\n",
    " 'attention',\n",
    " 'motion',\n",
    " 'space',\n",
    " 'visual',\n",
    " 'auditory',\n",
    " 'feeling',\n",
    " 'time',\n",
    " 'focuspast',\n",
    " 'focuspresent',\n",
    " 'focusfuture',\n",
    " 'Conversation',\n",
    " 'netspeak',\n",
    " 'assent',\n",
    " 'nonflu',\n",
    " 'filler']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb2aee-780c-4a33-8370-9c99ce213e2b",
   "metadata": {},
   "source": [
    "# Skip loading data and extracting featues and load below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e21c14-ccae-4f6f-bfc7-740d3fd207e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Or load data and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eed5a4-e1f7-4df9-a34c-0029eb451e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_names = ['train10_train_30perc' ,'train10_val_15perc','train10_test_15perc']\n",
    "\n",
    "dataset_dir = '/Users/danielmlow/data/ctl/input/datasets/'\n",
    "\n",
    "sub_dir = 'train10_subset_30'\n",
    "\n",
    "# Text \n",
    "train = pd.read_parquet(dataset_dir + f'{sub_dir}/{set_names[0]}_messages_texter.gzip', engine='pyarrow')\n",
    "val = pd.read_parquet(dataset_dir + f'{sub_dir}/{set_names[1]}_messages_texter.gzip', engine='pyarrow')\n",
    "test = pd.read_parquet(dataset_dir + f'{sub_dir}/{set_names[2]}_messages_texter.gzip', engine='pyarrow')\n",
    "\n",
    "# Metadata (i.e., target variables)\n",
    "train_metadata = pd.read_csv(dataset_dir + f'{sub_dir}/{set_names[0]}_metadata.csv')\n",
    "val_metadata = pd.read_csv(dataset_dir+ f'{sub_dir}/{set_names[1]}_metadata.csv')\n",
    "test_metadata = pd.read_csv(dataset_dir + f'{sub_dir}/{set_names[2]}_metadata.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc5710-2066-49e5-bb4b-b3ea7de1c876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07604aa8-246d-47f8-b494-c31abeb41923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suicide risk lexicon. should be able to import it\n",
    "import sys\n",
    "sys.path.append( './../../concept-tracker/')\n",
    "from concept_tracker import lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc1e8a-4e0e-4e16-9b52-c7c671c95543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import random \n",
    "\n",
    "\n",
    "run_this = True\n",
    "\n",
    "dfs = {'train':{'name':set_names[0], 'messages':train, 'metadata':train_metadata},\n",
    "        'val':{'name':set_names[1], 'messages':val, 'metadata':val_metadata},\n",
    "       'test':{'name':set_names[2], 'messages':test, 'metadata':test_metadata},\n",
    "               }\n",
    "\n",
    "\n",
    "if run_this:\n",
    "    with open(f'./data/input/ctl/{sub_dir}_dfs.pkl', 'wb') as f:\n",
    "        pickle.dump(dfs, f) \n",
    "\n",
    "# Save dfs to extract features in lexicon.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380831bd",
   "metadata": {},
   "source": [
    "# Create two datasets for classification: non-suicidal vs. non-imminent suicidal, suicidal vs. imminment risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e72a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "display(train_metadata['suicide_ladder_c'].value_counts())\n",
    "display(test_metadata['suicide_ladder_c'].value_counts())\n",
    "\n",
    "\n",
    "for split in dfs.keys():\n",
    "\tdf_metadata = dfs[split]['metadata']\n",
    "\tnew_values = []\n",
    "\tfor n in df_metadata['suicide_ladder_c'].values:\n",
    "\t\tif n==1:\n",
    "\t\t\tnew_values.append('nonsuicidal')\n",
    "\t\telif n == 2:\n",
    "\t\t\tnew_values.append('suicidal_desire')\n",
    "\t\telif n >= 4:\n",
    "\t\t\tnew_values.append('imminent_risk')\n",
    "\t\telse:\n",
    "\t\t\tnew_values.append('suicidal_intent_capability')\n",
    "\n",
    "\tdf_metadata['suicide_ladder_classification'] = new_values\n",
    "\tdfs[split]['metadata'] = df_metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\n",
    "\tdisplay(df_metadata['suicide_ladder_classification'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c391e59-6134-4373-860a-d9197eb42a67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Balanced training set (downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187c8da-40b6-43fd-9b08-7544ab152799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if task == 'classification':\n",
    "\tdv = 'suicide_ladder_classification'\n",
    "\tif target == 'suicidal_desire':\n",
    "\t\tbalance_values = ['nonsuicidal','suicidal_desire']\n",
    "\telif target == 'imminent_risk':\n",
    "\t\tbalance_values = ['suicidal_desire','imminent_risk']\n",
    "\tsmallest_value = 'imminent_risk'\n",
    "\tn = 1893\n",
    "\n",
    "elif task == 'regression':\n",
    "\n",
    "\t# config\n",
    "\tdv = 'suicide_ladder_a'\n",
    "\tbalance_values = [1,2,3]\n",
    "\tsmallest_value = 3\n",
    "\tn = train_metadata[dv].value_counts()[smallest_value]  \n",
    "\n",
    "display(train_metadata[dv].value_counts())\n",
    "\n",
    "# n = n- 10 #(-10 just in case there are issues like NaNs)\n",
    "print(n)\n",
    "\n",
    "\n",
    "dv_counts = []\n",
    "dv_counts_perc = []\n",
    "total_sample_size = 0\n",
    "for split in dfs.keys():\n",
    "\tprint(split)\n",
    "\tprint('====================')\n",
    "\t\n",
    "\tmessages = dfs[split]['messages']\n",
    "\tmetadata = dfs[split]['metadata']\n",
    "\ttotal_sample_size+=metadata.shape[0]\n",
    "\tprint(messages.shape[0])\n",
    "\tprint(messages['conversation_id'].unique().shape[0])\n",
    "\t\n",
    "\t# display()\n",
    "\n",
    "\n",
    "\tif split == 'train' and balance:\n",
    "\t\t# balance training set\n",
    "\t\t\n",
    "\t\tmetadata_balanced = []\n",
    "\t\tfor i in balance_values:\n",
    "\t\t\t\n",
    "\t\t\tmetadata_balanced_i  = metadata[metadata[dv]==i].sample(n=n, random_state = 42) # perceived risk == i\n",
    "\n",
    "\t\t\t# ids_subset = random.sample(ids, 1371) # subsample those\n",
    "\t\t\t# messages_dv_i = messages_dv[messages_dv['perceived_risk'].isin(ids_subset)]\n",
    "\t\t\tmetadata_balanced.append(metadata_balanced_i)\n",
    "\t\t\n",
    "\t\tmetadata_balanced = pd.concat(metadata_balanced)\n",
    "\t\tmetadata_balanced = metadata_balanced.sample(frac=1) #reshuffle\n",
    "\t\tmetadata_balanced = metadata_balanced.sort_values(by=['conversation_id'])\n",
    "\t\tmetadata_balanced.reset_index(inplace= True, drop=True)\n",
    "\t\t\n",
    "\t\tdv_counts.append(metadata_balanced[dv].value_counts())\n",
    "\t\tdv_counts_perc.append(metadata_balanced[dv].value_counts(normalize = True))\n",
    "\t\tmessages_dv = messages.merge(metadata_balanced, on = 'conversation_id', how='inner')\n",
    "\t\tbalanced_convo_ids = messages_dv['conversation_id'].values\n",
    "\t\t# liwc = dfs['train']['liwc22']\n",
    "\t\t# liwc_balanced = liwc[liwc['conversation_id'].isin(balanced_convo_ids)]\n",
    "\t\t# dfs['train']['liwc22_balanced'] = liwc_balanced.copy()\n",
    "\t\t\n",
    "\t\t# metadata_dv = metadata[['conversation_id', 'perceived_risk']]\n",
    "\t\t# print(metadata_dv.shape,messages.shape)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\tif split != 'train':\n",
    "\t\tmessages_dv = messages.merge(metadata, on = 'conversation_id', how='inner')\n",
    "\t\n",
    "\tmessages_dv = messages_dv[~messages_dv[dv].isna()]\n",
    "\tprint(messages_dv.shape[0])\n",
    "\tmessages_dv = messages_dv.sort_values(by=['conversation_id', 'message_timestamp_utc'])\n",
    "\n",
    "\t\n",
    "\tif task == 'classification':\n",
    "\t\tmessages_dv = messages_dv[messages_dv[dv].isin(balance_values)]\n",
    "\t\tmessages_dv = messages_dv.reset_index(drop=True)\n",
    "\t\tconvo_ids = messages_dv['conversation_id'].values\n",
    "\t\tmessages_dv = messages_dv.reset_index(drop=True)\n",
    "\n",
    "\t\n",
    "\tdfs[split]['metadata_messages'] = messages_dv\n",
    "\n",
    "\tconvo_ids = []\n",
    "\tX = []\n",
    "\ty = []\n",
    "\t# concat messages in messages_dv\n",
    "\tfor i in messages_dv['conversation_id'].unique():\n",
    "\t\tmessages_dv_i = messages_dv[messages_dv['conversation_id']==i]\n",
    "\t\tmessages_convo_i = [n.strip(' ') if n.endswith(('.', ',', ']', ')', '!','?', '>')) else n.strip(' ')+'.' for n in messages_dv_i['message'].tolist() ]\n",
    "\t\tX_i = ' '.join(messages_convo_i) # messages of 1 convo\n",
    "\t\t\n",
    "\t\t# X_i = X_i.replace('\n",
    "\t\tX.append(X_i)\n",
    "\t\ty_i= messages_dv_i[dv].unique()\n",
    "\t\tif len(y_i)!=1:\n",
    "\t\t\tprint('multiple values, fix:', y_i)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\ty_i = y_i[0]\n",
    "\t\ty.append(y_i)\n",
    "\t\tconvo_ids.append(i)\n",
    "\n",
    "\n",
    "\tdfs[split]['X'] = X\n",
    "\tdfs[split]['y'] = y\n",
    "\n",
    "\tdf_text = pd.DataFrame({'conversation_id': convo_ids,\n",
    "\t\t\t\t\t\t\t'text':X,\n",
    "\t\t\t\t\t\t\t'y':y\n",
    "\t\t\t\t\t\t})\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t# print('text before', df_text.shape)\n",
    "\t# df_text = df_text [~((df_text['y'].isna()) | (df_text['y']==''))]\n",
    "\t# print('text after', df_text.shape)\n",
    "\n",
    "\t\n",
    "\n",
    "\tdfs[split]['df_text'] = df_text.copy()\n",
    "\tname = dfs[split]['name']\n",
    "\n",
    "\n",
    "\t\n",
    "\tif split == 'train':\n",
    "\t\tdf_text.to_csv(f'./data/input/ctl/{name}_text_y_balanced_{task}.csv')\n",
    "\t\tdf_text = pd.read_csv(f'./data/input/ctl/{name}_text_y_balanced_{task}.csv', index_col = 0)\n",
    "\t\tdf_text = df_text [~((df_text['y'].isna()) | (df_text['y']==''))]\n",
    "\t\t\n",
    "\n",
    "\n",
    "\t\tdf_text.to_csv(f'./data/input/ctl/{name}_text_y_balanced_{task}.csv')\n",
    "\telse:\n",
    "\t\tdisplay(df_text['y'][65:80])\n",
    "\t\tdf_text.to_csv(f'./data/input/ctl/{name}_text_y_{task}.csv')\n",
    "\t\tdf_text = pd.read_csv(f'./data/input/ctl/{name}_text_y_{task}.csv', index_col = 0)\n",
    "\t\tdf_text = df_text [~((df_text['y'].isna()) | (df_text['y']==''))]\n",
    "\t\tdf_text.to_csv(f'./data/input/ctl/{name}_text_y_{task}.csv')\n",
    "\n",
    "\tdv_counts.append(df_text['y'].value_counts())\n",
    "\tdv_counts_perc.append(df_text['y'].value_counts(normalize=True))\n",
    "\n",
    "\t\n",
    "\t\t\t\t\t\t\n",
    "\t\n",
    "\n",
    "\t# balance liwc dataset\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t# print(metadata['conversation_id'].unique().shape[0])\n",
    "\t\n",
    "print('total_sample_size', total_sample_size)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edf82e-ff4b-4012-b1d3-3d2ce5fe5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for split in dfs.keys():\n",
    "\tprint(Counter(dfs[split]['y']))\n",
    "\t[np.round(n/len(dfs[split]['y']),2) for n in dict(Counter(dfs[split]['y'])).values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db8d51-7d44-427f-b900-35f225bfdf32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset_name = 'train10_subset_30'\n",
    "\n",
    "# for split in dfs.keys():\n",
    "# \tdf_text = dfs[split]['df_text'][['conversation_id', 'text', 'y']]\n",
    "# \tdf_text.to_csv(f'./data/input/ctl/{dataset_name}_{split}_text_y.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92eb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'classification':\n",
    "\n",
    "\tdv_counts = pd.concat(dv_counts,axis=1        )\n",
    "\tdv_counts_perc = pd.concat(dv_counts_perc, axis=1)\n",
    "\tdv_counts = dv_counts.iloc[:,1:]\n",
    "\tdv_counts_perc = dv_counts_perc.iloc[:,1:]\n",
    "\tdv_counts.columns = dfs.keys()\n",
    "\tdv_counts_perc.columns = dfs.keys()\n",
    "\n",
    "\t# print('downsampled sample size', dv_counts.sum().sum())\n",
    "\n",
    "\t# dv_distr = dv_counts.astype(str).add(\" (\").add(dv_counts_perc.round(2).astype(str)).add(\"%)\")\n",
    "\tdv_distr = dv_counts_perc.round(2).astype(str).add(\" (\").add(dv_counts.astype(str)).add(\")\").sort_index()\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\tdv_distr = dv_distr.reindex(balance_values)\n",
    "\tdv_distr.index = [n.capitalize().replace('_', ' ') for n in balance_values]\n",
    "\tdv_distr.index.name = 'Suicide risk'\n",
    "\n",
    "\tdv_distr.columns = ['Training', 'Validation', \"Test\"]\n",
    "\n",
    "\tdv_distr.to_csv(f'./data/output/tables/distribution_dv_{dv}_{task}_{target}.csv', index= True)\n",
    "\tdv_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2f2a3-d9fe-4546-a5ce-520d143349a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'regression':\n",
    "\n",
    "\tdv_counts = pd.concat(dv_counts,axis=1        )\n",
    "\tdv_counts_perc = pd.concat(dv_counts_perc, axis=1)\n",
    "\tdv_counts = dv_counts.iloc[:,1:]\n",
    "\tdv_counts_perc = dv_counts_perc.iloc[:,1:]\n",
    "\tdv_counts.columns = dfs.keys()\n",
    "\tdv_counts_perc.columns = dfs.keys()\n",
    "\n",
    "\tprint('downsampled sample size', dv_counts.sum().sum())\n",
    "\n",
    "\tdv_distr = dv_counts.astype(str).add(\" (\").add(dv_counts_perc.round(2).astype(str)).add(\"%)\")\n",
    "\tdv_distr = dv_counts_perc.round(2).astype(str).add(\" (\").add(dv_counts.astype(str)).add(\")\").sort_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdv_distr.index = ['Low', 'Medium', 'High']\n",
    "\tdv_distr.index.name = 'Suicide risk'\n",
    "\tdv_distr.columns = ['Training', 'Validation', \"Test\"]\n",
    "\n",
    "\tdv_distr.to_csv(f'./data/output/tables/distribution_dv_{dv}_{task}.csv', index= True)\n",
    "\tdv_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867b56c-f460-453e-bd8c-2587dd168217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove non IV columns from LIWC22\n",
    "# for split in dfs.keys():\n",
    "#     dfs[split]['liwc22'] = dfs[split]['liwc22'].drop(['Segment', 'conversation_id', 'message', 'Emoji'], axis=1)\n",
    "#     if balance and split=='train':\n",
    "#         dfs[split]['liwc22_balanced'] = dfs[split]['liwc22'].drop(['Segment', 'conversation_id', 'message', 'Emoji'], axis=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653888f-cb09-40aa-96fa-1152bb08b1b4",
   "metadata": {},
   "source": [
    "# Extract liwc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c43f23-f0fe-4612-b271-0d3b3b801f70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## automated liwc: this didn't work, I used the desktop app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5936b1-99fc-4181-83d2-81c6bfa36e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Ryan L. Boyd\n",
    "# 2022-03-17\n",
    "\n",
    "\n",
    "run_this = False # We'll do it manually below\n",
    "\tif run_this:\n",
    "\n",
    "\n",
    "\t\t# This is an example script that demonstrates how to make a call to the LIWC-22 command line interface (CLI)\n",
    "\t\t# from Python. Briefly described, what we want to do is launch the CLI application as a subprocess, then wait\n",
    "\t\t# for that subprocess to finish.\n",
    "\n",
    "\t\t# This is a very crude example script, so please feel free to improve/innovate on this example :)          \"\"\"\n",
    "\n",
    "\n",
    "\t# Make sure that you have the LIWC-22.exe GUI running — it is required for the CLI to function correctly :)\n",
    "\t# Make sure that you have the LIWC-22.exe GUI running — it is required for the CLI to function correctly :)\n",
    "\t# Make sure that you have the LIWC-22.exe GUI running — it is required for the CLI to function correctly :)\n",
    "\t# Make sure that you have the LIWC-22.exe GUI running — it is required for the CLI to function correctly :)\n",
    "\n",
    "\n",
    "\timport subprocess\n",
    "\n",
    "\n",
    "\t#  ______    _     _                      _ _   _       _________   _________   ______ _ _\n",
    "\t# |  ____|  | |   | |                    (_| | | |     |__   __\\ \\ / |__   __| |  ____(_| |\n",
    "\t# | |__ ___ | | __| | ___ _ __  __      ___| |_| |__      | |   \\ V /   | |    | |__   _| | ___ ___\n",
    "\t# |  __/ _ \\| |/ _` |/ _ | '__| \\ \\ /\\ / | | __| '_ \\     | |    > <    | |    |  __| | | |/ _ / __|\n",
    "\t# | | | (_) | | (_| |  __| |     \\ V  V /| | |_| | | |    | |   / . \\   | |    | |    | | |  __\\__ \\\n",
    "\t# |_|  \\___/|_|\\__,_|\\___|_|      \\_/\\_/ |_|\\__|_| |_|    |_|  /_/ \\_\\  |_|    |_|    |_|_|\\___|___/\n",
    "\n",
    "\tinputFolderTXT = \"C:/Users/Ryan/Datasets/TED - English Only - TXT Files/\"\n",
    "\toutputLocation = \"C:/Users/Ryan/Datasets/TED Talk TXT Files - Analyzed.csv\"\n",
    "\n",
    "\t# This command will read texts from a folder, analyze them using the standard \"Word Count\" LIWC analysis,\n",
    "\t# then save our output to a specified location.\n",
    "\tcmd_to_execute = [\"LIWC-22-cli\",\n",
    "\t\t\t\t\t\"--mode\", \"wc\",\n",
    "\t\t\t\t\t\"--input\", inputFolderTXT,\n",
    "\t\t\t\t\t\"--output\", outputLocation]\n",
    "\n",
    "\n",
    "\n",
    "\t# Let's go ahead and run this analysis:\n",
    "\tsubprocess.call(cmd_to_execute)\n",
    "\n",
    "\t# We will see the following in the terminal as it begins working:\n",
    "\t#\n",
    "\t#    Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
    "\t#    Processing:\n",
    "\t#     - [folder] C:\\Users\\Ryan\\Datasets\\TED - English Only - TXT Files\n",
    "\t#    [===================                     ] 47.75%; Number of Texts Analyzed: 1304; Total Words Analyzed: 2.62M\n",
    "\n",
    "\n",
    "\t# A thing of beauty, to be sure. What if we want to process our texts using an older LIWC dictionary,\n",
    "\t# or an external dictionary file? This can be done easily as well.\n",
    "\n",
    "\n",
    "\n",
    "\t# We can specify whether we want to use the LIWC2001, LIWC2007, LIWC2015,\n",
    "\t# or LIWC22 dictionary with the --dictionary argument.\n",
    "\tliwcDict = \"LIWC2015\"\n",
    "\n",
    "\t# Alternatively, you can specify the absolute path to an external dictionary\n",
    "\t# file that you would like to use, and LIWC will load this dictionary for processing.\n",
    "\t#liwcDict = \"C:/Users/Ryan/Dictionaries/Personal Values Dictionary.dicx\"\n",
    "\n",
    "\n",
    "\t# Let's update our output location as well so that we don't overwrite our previous file.\n",
    "\toutputLocation = \"C:/Users/Ryan/Datasets/TED Talk TXT Files - Analyzed (LIWC2015).csv\"\n",
    "\n",
    "\tcmd_to_execute = [\"LIWC-22-cli\",\n",
    "\t\t\t\t\t\"--mode\", \"wc\",\n",
    "\t\t\t\t\t\"--dictionary\", liwcDict,\n",
    "\t\t\t\t\t\"--input\", inputFolderTXT,\n",
    "\t\t\t\t\t\"--output\", outputLocation]\n",
    "\n",
    "\tsubprocess.call(cmd_to_execute)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t#   _____  _______      __  ______ _ _\n",
    "\t#  / ____|/ ____\\ \\    / / |  ____(_| |\n",
    "\t# | |    | (___  \\ \\  / /  | |__   _| | ___\n",
    "\t# | |     \\___ \\  \\ \\/ /   |  __| | | |/ _ \\\n",
    "\t# | |____ ____) |  \\  /    | |    | | |  __/\n",
    "\t#  \\_____|_____/    \\/     |_|    |_|_|\\___|\n",
    "\n",
    "\n",
    "\n",
    "\t# Beautiful. Now, let's do the same thing, but analyzing a CSV file full of the same texts.\n",
    "\tinputFileCSV = 'C:/Users/Ryan/Datasets/TED Talk - English Transcripts.csv'\n",
    "\toutputLocation = 'C:/Users/Ryan/Datasets/TED Talk CSV File - Analyzed.csv'\n",
    "\n",
    "\n",
    "\t# We're going to use a variation on the command above. Since this is a CSV file, we want to include the indices of\n",
    "\t#     1) the columns that include the text identifiers (although this is not required, it makes our data easier to merge later)\n",
    "\t#     2) the columns that include the actual text that we want to analyze\n",
    "\t#\n",
    "\t# In my CSV file, the first column has the text identifiers, and the second column contains the text.\n",
    "\t# For more complex datasets, please use the --help argument with LIWC-22 to learn more about how to process your text.\n",
    "\tcmd_to_execute = [\"LIWC-22-cli\",\n",
    "\t\t\t\t\t\"--mode\", \"wc\",\n",
    "\t\t\t\t\t\"--input\", inputFileCSV,\n",
    "\t\t\t\t\t\"--row-id-indices\", \"1\",\n",
    "\t\t\t\t\t\"--column-indices\", \"2\",\n",
    "\t\t\t\t\t\"--output\", outputLocation]\n",
    "\n",
    "\t# Let's go ahead and run this analysis:\n",
    "\tsubprocess.call(cmd_to_execute)\n",
    "\n",
    "\n",
    "\t# We will see the following in the terminal as LIWC does its magic:\n",
    "\t#    Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
    "\t#    Processing:\n",
    "\t#     - [file] C:\\Users\\Ryan\\Datasets\\TED Talk - English Transcripts.csv\n",
    "\t#    [========================================] 100.00%; Number of Rows Analyzed: 2737; Total Words Analyzed: 5.40M\n",
    "\t#    Done. Please examine results in C:\\Users\\Ryan\\Datasets\\TED Talk CSV File - Analyzed.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t#                       _                  _____ _        _\n",
    "\t#     /\\               | |                / ____| |      (_)\n",
    "\t#    /  \\   _ __   __ _| |_   _ _______  | (___ | |_ _ __ _ _ __   __ _\n",
    "\t#   / /\\ \\ | '_ \\ / _` | | | | |_  / _ \\  \\___ \\| __| '__| | '_ \\ / _` |\n",
    "\t#  / ____ \\| | | | (_| | | |_| |/ |  __/  ____) | |_| |  | | | | | (_| |\n",
    "\t# /_/    \\_|_| |_|\\__,_|_|\\__, /___\\___| |_____/ \\__|_|  |_|_| |_|\\__, |\n",
    "\t#                          __/ |                                   __/ |\n",
    "\t#                         |___/                                   |___/\n",
    "\n",
    "\t# What if we want to simply pass a string to the CLI for analysis? This is possible. As described on the\n",
    "\t# Help section of the liwc.app website, this is generally not recommended as it will not be very performant.\n",
    "\t#\n",
    "\t# Also, of serious importance! Most command lines/terminals have a limit on the length of any string that it\n",
    "\t# will parse. This means that you likely cannot analyze very long texts (e.g., like a long paper, speech,\n",
    "\t# or book) by passing the text directly into the console. Instead, you will likely need to process your\n",
    "\t# data directly from the disk instead.\n",
    "\t#\n",
    "\t# However, if you insist...\n",
    "\n",
    "\t# The string that we would like to analyze.\n",
    "\tinputString = \"This is some text that I would like to analyze. After it has finished, I will say \\\"Thank you, LIWC!\\\"\"\n",
    "\n",
    "\t# For this one, let's save our result as a newline-delimited json file (.ndjson)\n",
    "\toutputLocation = 'C:/Users/Ryan/Datasets/LIWC-22 Results from String.ndjson'\n",
    "\n",
    "\n",
    "\tcmd_to_execute = [\"LIWC-22-cli\",\n",
    "\t\t\t\t\t\"--mode\", \"wc\",\n",
    "\t\t\t\t\t\"--input\", \"console\",\n",
    "\t\t\t\t\t\"--console-text\", inputString,\n",
    "\t\t\t\t\t\"--output\", outputLocation]\n",
    "\n",
    "\n",
    "\t# Let's go ahead and run this analysis:\n",
    "\tsubprocess.call(cmd_to_execute)\n",
    "\n",
    "\t# The results from this analysis:\n",
    "\t#{\"Segment\": 1,\"WC\": 20,\"Analytic\": 3.8,\"Clout\": 40.06,\"Authentic\": 28.56,\"Tone\": 99,\"WPS\": 10,\"BigWords\": 10,\n",
    "\t#\"Dic\": 100, \"Linguistic\": 80,\"function\": 70,\"pronoun\": 30,\"ppron\": 15,\"i\": 10,\"we\": 0,\"you\": 5,\"shehe\": 0,\"they\": 0,\n",
    "\t#\"ipron\": 15,\"det\": 15,\"article\": 0,\"number\": 0,\"prep\": 15,\"auxverb\": 20,\"adverb\": 0,\"conj\": 5,\"negate\": 0,\n",
    "\t#\"verb\": 35,\"adj\": 0,\"quantity\": 5,\"Drives\": 5,\"affiliation\": 0,\"achieve\": 5,\"power\": 0,\"Cognition\": 15,\n",
    "\t#\"allnone\": 0,\"cogproc\": 15,\"insight\": 5,\"cause\": 0,\"discrep\": 10,\"tentat\": 0,\"certitude\": 0,\"differ\": 0,\n",
    "\t#\"memory\": 0,\"Affect\": 15,\"tone_pos\": 15,\"tone_neg\": 0,\"emotion\": 10,\"emo_pos\": 10,\"emo_neg\": 0,\"emo_anx\": 0,\n",
    "\t#\"emo_anger\": 0,\"emo_sad\": 0,\"swear\": 0,\"Social\": 20,\"socbehav\": 15,\"prosocial\": 5,\"polite\": 5,\"conflict\": 0,\"moral\": 0,\n",
    "\t#\"comm\": 15,\"socrefs\": 5,\"family\": 0,\"friend\": 0,\"female\": 0,\"male\": 0,\"Culture\": 5,\"politic\": 0,\"ethnicity\": 0,\"\n",
    "\t#tech\": 5,\"Lifestyle\": 0,\"leisure\": 0,\"home\": 0,\"work\": 0,\"money\": 0,\"relig\": 0,\"Physical\": 0,\"health\": 0,\"illness\": 0,\n",
    "\t#\"wellness\": 0,\"mental\": 0,\"substances\": 0,\"sexual\": 0,\"food\": 0,\"death\": 0,\"need\": 0,\"want\": 0,\"acquire\": 0,\"lack\": 0,\n",
    "\t#\"fulfill\": 0,\"fatigue\": 0,\"reward\": 0,\"risk\": 0,\"curiosity\": 0,\"allure\": 0,\"Perception\": 0,\"attention\": 0,\"motion\": 0,\n",
    "\t#\"space\": 0,\"visual\": 0,\"auditory\": 0,\"feeling\": 0,\"time\": 10,\"focuspast\": 0,\"focuspresent\": 10,\"focusfuture\": 5,\n",
    "\t#\"Conversation\": 0,\"netspeak\": 0,\"assent\": 0,\"nonflu\": 0,\"filler\": 0,\n",
    "\t#\"AllPunc\": 30,\"Period\": 5,\"Comma\": 10,\"QMark\": 0,\"Exclam\": 5,\"Apostro\": 0,\"OtherP\": 10}\n",
    "\n",
    "\n",
    "\n",
    "\t# And, lastly — what if we want to get the output directly from the command line or terminal as a json string?\n",
    "\t# Why, we can do that too!\n",
    "\n",
    "\n",
    "\tinputString = \"This is some text that I would like to analyze. After it has finished,\" \\\n",
    "\t\t\t\t\" we will get results in the console. Hooray!\"\n",
    "\toutputLocation = \"console\"\n",
    "\n",
    "\tcmd_to_execute = [\"LIWC-22-cli\",\n",
    "\t\t\t\t\t\"--mode\", \"wc\",\n",
    "\t\t\t\t\t\"--input\", \"console\",\n",
    "\t\t\t\t\t\"--console-text\", inputString,\n",
    "\t\t\t\t\t\"--output\", outputLocation]\n",
    "\n",
    "\t# Let's go ahead and run this analysis. We do this somewhat differently than what we've been doing, however.\n",
    "\t# This will end up giving us a list, where each element is a line of output from the console.\n",
    "\tresults = subprocess.check_output(cmd_to_execute, shell=True).strip().splitlines()\n",
    "\n",
    "\t# In this case, the item that we want to parse from a json to a Python dictionary is in results[1], so we will\n",
    "\t# go right ahead and parse that to a dictionary now:\n",
    "\timport json\n",
    "\tresults_json = json.loads(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bc502-284f-43d4-94a3-f21526dd03c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[split]['df_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d607769-f5c2-40e5-b504-086e4b076168",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dir = './data/input/ctl/'\n",
    "\n",
    "\n",
    "liwc_train = pd.read_csv(liwc_dir+f'train10_train_30perc_text_y_balanced_{task}_liwc22.csv', index_col = 0)\n",
    "liwc_test = pd.read_csv(liwc_dir+f'train10_test_15perc_text_y_{task}_liwc22.csv', index_col = 0)\n",
    "\n",
    "for split, df_i in zip(['train', 'test'], [liwc_train, liwc_test]):\n",
    "    df_text = dfs[split]['df_text']\n",
    "    df_i = df_i[df_i['conversation_id'].isin(df_text['conversation_id'].unique())]\n",
    "    dfs[split]['liwc22_X'] = df_i.drop(['Segment', 'conversation_id', 'y', 'text', 'Emoji'], axis=1)\n",
    "    dfs[split]['liwc22_y'] = df_i['y'].values\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb1454-f6d3-4229-be3b-fec2e03f1160",
   "metadata": {},
   "source": [
    "# Extract Suicide Risk Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b744de0-f7bb-4a20-807b-d8ac3c238c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concept_tracker.utils import lemmatizer # local script\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_tracker.lexicon import lemmatize_tokens\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39721408-43d9-4c14-8cc4-7d203cccaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "sys.path.append( './../../concept-tracker/') # TODO: replace with pip install construct-tracker\n",
    "from concept_tracker import lexicon\n",
    "\n",
    "\n",
    "def load_lexicon(path):\n",
    "\tlexicon = dill.load(open(path, \"rb\"))\n",
    "\treturn lexicon\n",
    "srl = load_lexicon(\"./data/input/lexicons/suicide_risk_lexicon_calibrated_unmatched_tokens_unvalidated_24-02-15T21-55-05.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl.exact_match_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb25c75-c26e-4a3f-b6db-223d3cc53094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "\tprint('extracting', split)\n",
    "\tdf_text = dfs[split]['df_text']\n",
    "\tdocs = df_text['text'].values\n",
    "\t\n",
    "\n",
    "\t# srl = lemmatize_tokens(srl) # TODO: integrate this to class: self.lemmatize_tokens() adds tokens_lemmatized\n",
    "\n",
    "\t# Extract\n",
    "\tfeature_vectors, matches_counter_d, matches_per_doc, matches_per_construct  = lexicon.extract(docs,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsrl.constructs,normalize = True, return_matches=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tadd_lemmatized_lexicon=True, lemmatize_docs=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\texact_match_n = srl.exact_match_n,exact_match_tokens = srl.exact_match_tokens)\n",
    "\t\n",
    "\n",
    "\t\n",
    "\n",
    "\tdf_text[feature_vectors.columns] = feature_vectors.values\n",
    "\tdfs[split]['srl_unvalidated'] = df_text.drop(['conversation_id', 'y', 'text', 'word_count', 'Direct self-injury 2', 'Relationship issues 2'], axis=1).copy()\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[split]['srl_unvalidated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce787b38-d2a1-40c1-9501-e5b5b45d445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dfs['train']['srl_unvalidated'] \n",
    "# X_val = dfs['val']['srl_unvalidated']    \n",
    "X_test = dfs['test']['srl_unvalidated']\n",
    "y_train = dfs['train']['y']\n",
    "# y_val = dfs['val']['y'] \n",
    "y_test = dfs['test']['y']\n",
    "\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "# print(len(X_val), len(y_val))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246429bd-834e-4264-bee6-c6811c0d9d14",
   "metadata": {},
   "source": [
    "# Suicide Risk Lexicon (only GPT-4 Turbo tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b591056-efba-4a64-8d36-f964b826f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_gpt4 = {}\n",
    "\n",
    "for construct in srl.constructs.keys():\n",
    "    gpt4_tokens = []\n",
    "    for source in srl.constructs[construct]['tokens_metadata'].keys():\n",
    "        if 'gpt-4-1106-preview' in source:\n",
    "            tokens_i = srl.constructs[construct]['tokens_metadata'][source]['tokens']\n",
    "            gpt4_tokens.extend(tokens_i)\n",
    "            \n",
    "            \n",
    "    srl_gpt4[construct]={'tokens':list(np.unique(gpt4_tokens))}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4afb8-fdff-473f-862f-0c4eea28aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll consider the 2 version of two of these after editing either the definition or prompt_name\n",
    "srl_gpt4['Direct self-injury'] = srl_gpt4['Direct self-injury 2'].copy()\n",
    "del srl_gpt4['Direct self-injury 2']\n",
    "srl_gpt4['Relationship issues'] = srl_gpt4['Relationship issues 2'].copy()\n",
    "del srl_gpt4['Relationship issues 2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68230936",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(srl_gpt4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35abdc-366e-4ba1-b63e-6c42597d7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_tracker.utils import lemmatizer\n",
    "for c in tqdm.tqdm(list(srl_gpt4.keys())):\n",
    "\tlexicon_tokens = srl_gpt4[c]['tokens']\n",
    "\n",
    "\n",
    "\t# If you add lemmatized and nonlemmatized you'll get double count in many cases (\"plans\" in doc will be matched by \"plan\" and \"plans\" in lexicon)\n",
    "\tlexicon_tokens_lemmatized = lemmatizer.spacy_lemmatizer(lexicon_tokens, language='en') # custom function\n",
    "\tlexicon_tokens_lemmatized = [' '.join(n) for n in lexicon_tokens_lemmatized]\n",
    "\tlexicon_tokens += lexicon_tokens_lemmatized\n",
    "\tlexicon_tokens = list(np.unique(lexicon_tokens)) # unique set\n",
    "\tsrl_gpt4[c]['tokens_lemmatized']=lexicon_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c06bb5-7751-463d-b772-d3de1baadcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'test']:\n",
    "    df_text = dfs[split]['df_text']\n",
    "    docs = df_text['text'].values    \n",
    "    feature_vectors, matches_counter_d, matches_per_doc, matches_per_construct  = lexicon.extract(docs,\n",
    "                                                                                          srl_gpt4,\n",
    "                                                                                          normalize = normalize_lexicon,\n",
    "                                                                                          exact_match_n = srl.exact_match_n\n",
    "                                                                                          )\n",
    "    df_text[feature_vectors.columns] = feature_vectors.values\n",
    "    dfs[split]['SRL GPT-4 Turbo'] = df_text.drop(['conversation_id', 'y', 'text', 'word_count'], axis=1).copy()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3d6f0",
   "metadata": {},
   "source": [
    "# TextDescriptives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textdescriptives==2.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549aa243",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_columns = ['token_length_mean',\n",
    "#  'token_length_median',\n",
    " 'token_length_std',\n",
    " 'sentence_length_mean',\n",
    "#  'sentence_length_median',\n",
    " 'sentence_length_std',\n",
    "#  'syllables_per_token_mean',\n",
    "#  'syllables_per_token_median',\n",
    "#  'syllables_per_token_std',\n",
    " 'n_tokens',\n",
    "#  'n_unique_tokens',\n",
    "#  'proportion_unique_tokens',\n",
    "#  'n_characters',\n",
    " 'n_sentences',\n",
    "#  'first_order_coherence',\n",
    "#  'second_order_coherence',\n",
    " 'pos_prop_ADJ',\n",
    " 'pos_prop_ADP',\n",
    " 'pos_prop_ADV',\n",
    " 'pos_prop_AUX',\n",
    " 'pos_prop_CCONJ',\n",
    " 'pos_prop_DET',\n",
    " 'pos_prop_INTJ',\n",
    " 'pos_prop_NOUN',\n",
    " 'pos_prop_NUM',\n",
    " 'pos_prop_PART',\n",
    " 'pos_prop_PRON',\n",
    " 'pos_prop_PROPN',\n",
    " 'pos_prop_PUNCT',\n",
    " 'pos_prop_SCONJ',\n",
    " 'pos_prop_SYM',\n",
    " 'pos_prop_VERB',\n",
    " 'pos_prop_X',\n",
    "#  'flesch_reading_ease',\n",
    "#  'flesch_kincaid_grade',\n",
    "#  'smog',\n",
    " 'gunning_fog',\n",
    " 'automated_readability_index',\n",
    "#  'coleman_liau_index',\n",
    "#  'lix',\n",
    "#  'rix',\n",
    "#  'entropy',\n",
    "#  'perplexity',\n",
    "#  'per_word_perplexity',\n",
    " 'passed_quality_check',\n",
    "#  'n_stop_words',\n",
    " 'alpha_ratio',\n",
    " 'mean_word_length',\n",
    "#  'doc_length',\n",
    " 'symbol_to_word_ratio_#',\n",
    " 'proportion_ellipsis',\n",
    "#  'proportion_bullet_points',\n",
    "#  'contains_lorem ipsum',\n",
    "#  'duplicate_line_chr_fraction',\n",
    "#  'duplicate_paragraph_chr_fraction',\n",
    "#  'duplicate_ngram_chr_fraction_5',\n",
    "#  'duplicate_ngram_chr_fraction_6',\n",
    "#  'duplicate_ngram_chr_fraction_7',\n",
    "#  'duplicate_ngram_chr_fraction_8',\n",
    "#  'duplicate_ngram_chr_fraction_9',\n",
    "#  'duplicate_ngram_chr_fraction_10',\n",
    " 'top_ngram_chr_fraction_2',\n",
    "#  'top_ngram_chr_fraction_3',\n",
    "#  'top_ngram_chr_fraction_4',\n",
    "#  'oov_ratio',\n",
    " 'dependency_distance_mean',\n",
    " 'dependency_distance_std',\n",
    " 'prop_adjacent_dependency_relation_mean',\n",
    " 'prop_adjacent_dependency_relation_std']\n",
    "# df_text[['y']+metrics.columns].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textdescriptives as td\n",
    "# load your favourite spacy model (remember to install it first using e.g. `python -m spacy download en_core_web_sm`)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textdescriptives/all\")  #TODO: dont compute coherence, quality, etc.\n",
    "\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "\tprint('extracting', split)\n",
    "\tdf_text = dfs[split]['df_text'].copy()\n",
    "\t# docs = df_text['text'].values\n",
    "\t\n",
    "\tdoc = nlp.pipe(df_text['text'])\n",
    "\ttd_features = td.extract_df(doc, include_text=False, metrics =[\"descriptive_stats\",\"readability\", 'quality', 'pos_proportions', 'dependency_distance'])\n",
    "\t\n",
    "\n",
    "\ttd_features = td_features[td_columns] # only keep td_columns\n",
    "\n",
    "\tassert td_features.shape[0] == df_text.shape[0]\n",
    "\tdfs[split]['text_descriptives'] = td_features.copy()\n",
    "\n",
    "\tdf_text_td = df_text.join(td_features, how=\"left\")\n",
    "\n",
    "\tdfs[split]['srl_unvalidated_text_descriptives'] = df_text_td.drop(['conversation_id', 'y', 'text', 'word_count', 'Direct self-injury 2', 'Relationship issues 2'], axis=1).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5db52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "td_features = dfs['train']['text_descriptives']\n",
    "td_features = td_features[td_columns]\n",
    "td_features['y'] = dfs['train']['y'].values\n",
    "td_features_corr = td_features.corr(method='spearman')\n",
    "td_features_corr = td_features_corr.fillna(td_features_corr.median().median())\n",
    "\n",
    "sns.set(font_scale=0.75)\n",
    "sns.clustermap(td_features_corr)\n",
    "# n = 5\n",
    "\n",
    "# Adjust both x-tick and y-tick label sizes\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), fontsize=n)  # Set the fontsize for x-tick labels\n",
    "# ax.set_yticklabels(ax.get_yticklabels(), fontsize=n)  # Set the fontsize for y-tick labels\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cfa5a-37e1-4f67-be80-e0485350d3f8",
   "metadata": {},
   "source": [
    "# Extract embeddings\n",
    "\n",
    "- 4000 docs - 8m\n",
    "\n",
    "- 1000 docs - 1.5 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs[split]['X']) == dfs[split]['df_text'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdfaad-eb38-4580-b751-540ac5c9185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_this = True\n",
    "# 25m for train set.\n",
    "if run_this:\n",
    "\timport tensorboard\n",
    "\tfrom sentence_transformers import SentenceTransformer, util \n",
    "\tembeddings_name = 'all-MiniLM-L6-v2'\n",
    "\t# Encode the documents with their sentence embeddings \n",
    "\t# a list of pre-trained sentence transformers\n",
    "\t# https://www.sbert.net/docs/pretrained_models.html\n",
    "\t# https://huggingface.co/models?library=sentence-transformers\n",
    "\t\n",
    "\t# all-MiniLM-L6-v2 is optimized for semantic similarity of paraphrases\n",
    "\tsentence_embedding_model = SentenceTransformer(embeddings_name)       # load embedding\n",
    "\t\n",
    "\tsentence_embedding_model._first_module().max_seq_length = 500\n",
    "\t# TODO: Change max_seq_length to 500\n",
    "\t# Note: sbert will only use fewer tokens as its meant for sentences, \n",
    "\tprint(sentence_embedding_model .max_seq_length)\n",
    "\n",
    "\n",
    "\n",
    "\tfor split in ['train', 'test']:\n",
    "\t\tdfs[split]['embeddings'] = sentence_embedding_model.encode(dfs[split]['X'], convert_to_tensor=True,show_progress_bar=True)\n",
    "\t\n",
    "\t# TODO move up to where I encoded this\n",
    "\t\t\n",
    "\tfor split in ['train', 'test']:\n",
    "\t\tembeddings = dfs[split]['embeddings']\n",
    "\t\tembeddings = pd.DataFrame(embeddings, columns = [f'{embeddings_name}_{str(n).zfill(4)}' for n in range(embeddings.shape[1])])\n",
    "\t\tdfs[split][embeddings_name] = embeddings\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a1d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da1dc4d6-bbb8-4ddf-b407-6411b8b6101c",
   "metadata": {},
   "source": [
    "# Load everything above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883ec0a-0b68-48f8-894f-36c45fa71e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "run_this = False #True saves, False loads\n",
    "if run_this:\n",
    "    with open(f'./data/input/ctl/ctl_dfs_features_{task}.pkl', 'wb') as f:\n",
    "        pickle.dump(dfs, f) \n",
    "else:\n",
    "\n",
    "    with open(f'./data/input/ctl/ctl_dfs_features_{task}.pkl', 'rb') as f:\n",
    "    \tdfs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd20843-4250-48c2-8458-35ca47dae020",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427097d-aef0-498f-9f8f-87770df277dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lightgbm import LGBMClassifier # TODO: add\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm==4.3.0\n",
    "from lightgbm import LGBMRegressor\n",
    "import string\n",
    "from sklearn.linear_model import Lasso\n",
    "# import contractions # TODO: add\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "sys.path.insert(1,'./../../concept-tracker')\n",
    "from concept_tracker.utils import metrics_report # local script\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "# from imblearn.pipeline import Pipeline as imb_Pipeline\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2fe81-a7ec-48ae-a873-8da188fed281",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './data/output/'\n",
    "output_dir_i = output_dir+'ml_performance/'\n",
    "os.makedirs(output_dir_i,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b39925-d5a2-4212-9e07-069a45a91991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108da81-b3e0-47ca-b72c-54f10ab44d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "ridge_alphas_toy = [0.1, 10]\n",
    "def get_params(feature_vector,model_name = 'Ridge', toy=False):\n",
    "\tif model_name in ['LogisticRegression']:\n",
    "\t\tif feature_vector == 'tfidf':\n",
    "\t\t\tif toy:\n",
    "\t\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t   'vectorizer__max_features': [256, 512],\n",
    "\t\t\t\t}\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'vectorizer__max_features': [512,2048,None],\n",
    "\t\t\t\t\t'model__C': ridge_alphas,\n",
    "\t\t\t\t}\n",
    "\t\n",
    "\t\telse:\n",
    "\t\t\tif toy:\n",
    "\t\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'model__C': ridge_alphas_toy,\n",
    "\t\t\t\t}\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'model__C': ridge_alphas,\n",
    "\t\t\t\t}\n",
    "\t\n",
    "\telif model_name in ['Ridge', 'Lasso']:\n",
    "\t\tif feature_vector == 'tfidf':\n",
    "\t\t\tif toy:\n",
    "\t\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t   'vectorizer__max_features': [256, 512],\n",
    "\t\t\t\t}\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'vectorizer__max_features': [512,2048,None],\n",
    "\t\t\t\t\t'model__alpha': ridge_alphas,\n",
    "\t\t\t\t}\n",
    "\t\n",
    "\t\telse:\n",
    "\t\t\tif toy:\n",
    "\t\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'model__alpha': ridge_alphas_toy,\n",
    "\t\t\t\t}\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'model__alpha': ridge_alphas,\n",
    "\t\t\t\t}\n",
    "\t\n",
    "\n",
    "\telif model_name in [ 'LGBMRegressor', 'LGBMClassifier']:\n",
    "\t\tif toy:\n",
    "\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\tparam_grid = {\n",
    "\t\t\t   # 'vectorizer__max_features': [256,2048],\n",
    "\t\t\t\t# 'model__colsample_bytree': [0.5, 1],\n",
    "\t\t\t\t'model__max_depth': [10,20], #-1 is the default and means No max depth\n",
    "\t\t\n",
    "\t\t\t}\n",
    "\t\telse:\n",
    "\t\t\tif feature_vector =='tfidf':\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'vectorizer__max_features': [256,2048,None],\n",
    "\t\t\t\t\t'model__num_leaves': [30,45,60],\n",
    "\t\t\t\t\t'model__colsample_bytree': [0.1, 0.5, 1],\n",
    "\t\t\t\t\t'model__max_depth': [0,5,15], #0 is the default and means No max depth\n",
    "\t\t\t\t\t'model__min_child_weight': [0.01, 0.001, 0.0001],\n",
    "\t\t\t\t\t'model__min_child_samples': [10, 20,40], #alias: min_data_in_leaf\n",
    "\t\t\t\t   'vectorizer__max_features': [256, 512],\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\n",
    "\t\t\tparam_grid = {\n",
    "\t\t\t\t'model__num_leaves': [30,45,60],\n",
    "\t\t\t\t'model__colsample_bytree': [0.1, 0.5, 1],\n",
    "\t\t\t\t'model__max_depth': [0,5,15], #0 is the default and means No max depth\n",
    "\t\t\t\t'model__min_child_weight': [0.01, 0.001, 0.0001],\n",
    "\t\t\t\t'model__min_child_samples': [10, 20,40], #alias: min_data_in_leaf\n",
    "\t\t\n",
    "\t\t\t}\n",
    "\n",
    "\t\n",
    "\telif model_name in [ 'XGBRegressor', 'XGBClassifier']:\n",
    "\t\tif toy:\n",
    "\t\t\twarnings.warn('WARNING, running toy version')\n",
    "\t\t\tparam_grid = {\n",
    "\t\t\t\t'model__max_depth': [10,20], #-1 is the default and means No max depth\n",
    "\t\t\n",
    "\t\t\t}\n",
    "\t\telse:\n",
    "\t\t\tif feature_vector =='tfidf':\n",
    "\t\t\t\tparam_grid = {\n",
    "\t\t\t\t\t'vectorizer__max_features': [256,2048,None],\n",
    "\t\t\t\t\t'model__colsample_bytree': [0.1, 0.5, 1],\n",
    "\t\t\t\t\t'model__max_depth': [5,15, None], #None is the default and means No max depth\n",
    "\t\t\t\t\t'model__min_child_weight': [0.01, 0.001, 0.0001],\n",
    "\t\t\t\t\n",
    "\t\t\t\t   \n",
    "\t\t\t\t\t}\n",
    "\t\t\t\n",
    "\t\t\tparam_grid = {\n",
    "\t\t\t\t'model__colsample_bytree': [0.1, 0.5, 1],\n",
    "\t\t\t\t'model__max_depth': [5,15, None], #None is the default and means No max depth\n",
    "\t\t\t\t'model__min_child_weight': [0.01, 0.001, 0.0001],\n",
    "\t\t\n",
    "\t\t\t}\n",
    "\n",
    "\treturn param_grid\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "def get_pipelines(feature_vector, model_name = 'Ridge'):\n",
    "\t\n",
    "\t# model = getattr(__main__, model_name)()\n",
    "\tmodel = globals()[model_name]()\n",
    "\t# if model_name == 'Ridge':\n",
    "\t#     model = Ridge()\n",
    "\t# elif model_name == 'XGBRegressor':\n",
    "\t#     model = XGBRegressor()\n",
    "\tmodel.set_params(random_state = 123)\n",
    "\t\n",
    "\t\n",
    "\tif feature_vector =='tfidf':\n",
    "\t\tpipeline = Pipeline([\n",
    "\t\t\t ('vectorizer', vectorizer),\n",
    "\t\t\t ('model', model), \n",
    "\t\t\t])\n",
    "\telse:\n",
    "\t\tpipeline = Pipeline([\n",
    "\t\t\t('imputer', SimpleImputer(strategy='median')),\n",
    "\t\t\t('standardizer', StandardScaler()),\n",
    "\t\t\t ('model', model), \n",
    "\t\t\t])\n",
    "\treturn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc03624-0f60-4f3b-952c-557b3814e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b32ec-3385-471b-82a4-7be75a891d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tfidf_feature_importances(pipe, top_k = 100, savefig_path = '', model_name_in_pipeline = 'model', xgboost_method = 'weight' ):\n",
    "    # # Using sklearn pipeline:\n",
    "    feature_names = pipe.named_steps[\"vectorizer\"].get_feature_names_out()\n",
    "    \n",
    "    try: coefs = pipe.named_steps[\"model\"].coef_.flatten() # Get the coefficients of each feature\n",
    "    except: \n",
    "        try: coefs = list(pipe.named_steps[model_name_in_pipeline].get_booster().get_score(importance_type=xgboost_method )) # pipeline directly\n",
    "        except:\n",
    "            # gridsearchcv(pipeline)\n",
    "            coefs = pipe.best_estimator_.named_steps[model_name_in_pipeline].get_booster().get_score(importance_type=xgboost_method )\n",
    "    \n",
    "    # Without sklearn pipeline\n",
    "    # feature_names = vectorizer.get_feature_names_out()\n",
    "    # print(len(feature_names ))\n",
    "    # coefs = pipeline.coef_.flatten() # Get the coefficients of each feature\n",
    "    \n",
    "    # Visualize feature importances\n",
    "    # Sort features by absolute value\n",
    "    df = pd.DataFrame(zip(feature_names, coefs), columns=[\"feature\", \"value\"])\n",
    "    df[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\n",
    "    df[\"colors\"] = df[\"value\"].apply(lambda x: \"orange\" if x > 0 else \"dodgerblue\")\n",
    "    df = df.sort_values(\"abs_value\", ascending=False) # sort by absolute coefficient value\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3.5, 6))\n",
    "    plt.style.use('default')  # Example of applying the 'ggplot' style\n",
    "    ax = sns.barplot(x=\"value\",\n",
    "                y=\"feature\",\n",
    "                data=df.head(top_k),\n",
    "                hue=\"colors\")\n",
    "    ax.legend_.remove()\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=8)\n",
    "    ax.set_title(f\"Top {top_k} Features\", fontsize=14)\n",
    "    ax.set_xlabel(\"Coef\", fontsize=12) # coeficient from linear model\n",
    "    ax.set_ylabel(\"Feature Name\", fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savefig_path+'.png', dpi=300)\n",
    "    plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_report = 1\n",
    "from concept_tracker.utils import metrics_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ad320-a4a8-454d-8b2f-6bef506d4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concept_tracker.utils.metrics_report import cm, classification_report#, regression_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf10b0f-c9e2-439e-bc28-38e1bb7cc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf \n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "\n",
    "# Now, integrate this with TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(tokenizer=nltk_lemmatize, stop_words='english')\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(string)\n",
    "    return words\n",
    "\n",
    "def tokenizer_remove_punctuation(text):\n",
    "    return re.split(\"\\\\s+\",text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                 min_df=3, ngram_range=(1,2), \n",
    "                 stop_words=None, #'english',# these include 'just': stopwords.words('english')+[\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'], strip_accents='unicode',\n",
    "                 sublinear_tf=True,\n",
    "                 # tokenizer=nltk_lemmatize,\n",
    "                token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                    use_idf=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753675ce-241c-4634-847d-6a6b809ab818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# def load_lexicon(path):\n",
    "# \tlexicon = dill.load(open(path, \"rb\"))\n",
    "# \treturn lexicon\n",
    "# lexicon = load_lexicon(\"./../data/lexicons/suicide_risk_lexicon_gpt-4-1106-preview_dml_24-01-24T18-38-38.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e71b53-1f71-4b3d-a739-ef26d54b0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# [np.round(n/len(y_train),2) for n in dict(Counter(y_train)).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1bab8-1956-4876-a03b-816aa03b1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srl_constructs import constructs_in_order\n",
    "\n",
    "\n",
    "def get_splits(feature_vector):\n",
    "\tif feature_vector in ['tfidf']:\n",
    "\t\tX_train = dfs['train']['X'] # text\n",
    "\t\t# X_val = dfs['val']['X']\n",
    "\t\tX_test = dfs['test']['X']\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\t# y_val = dfs['val']['y']\n",
    "\t\ty_test = dfs['test']['y']\n",
    "\t\t\n",
    "\telif feature_vector in ['liwc22']:        \n",
    "\t\t\n",
    "\t\tX_train = dfs['train']['liwc22_X'] \n",
    "\t\t# X_val = dfs['val']['liwc22_X']    \n",
    "\t\tX_test = dfs['test']['liwc22_X']\n",
    "\t\ty_train = dfs['train']['liwc22_y']\n",
    "\t\t# y_val = dfs['val']['liwc22_y']\n",
    "\t\ty_test = dfs['test']['liwc22_y']\n",
    "\n",
    "\telif feature_vector in ['srl_unvalidated']:        \n",
    "\t\t\n",
    "\t\tX_train = dfs['train']['srl_unvalidated'] \n",
    "\t\t# X_val = dfs['val']['srl_unvalidated']    \n",
    "\t\tX_test = dfs['test']['srl_unvalidated']\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\t# y_val = dfs['val']['y'] \n",
    "\t\ty_test = dfs['test']['y']\n",
    "\n",
    "\telif feature_vector in ['SRL GPT-4 Turbo']:\n",
    "\t\tX_train = dfs['train']['SRL GPT-4 Turbo'][constructs_in_order] \n",
    "\t\t# X_val = dfs['val']['SRL GPT-4 Turbo'][constructs_in_order]    \n",
    "\t\tX_test = dfs['test']['SRL GPT-4 Turbo'][constructs_in_order]\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\t# y_val = dfs['val']['y'] \n",
    "\t\ty_test = dfs['test']['y']\n",
    "\t\t\n",
    "\n",
    "\telif feature_vector in ['text_descriptives']:        \n",
    "\t\t\n",
    "\t\tX_train = dfs['train']['text_descriptives'] \n",
    "\t\tX_test = dfs['test']['text_descriptives']\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\ty_test = dfs['test']['y']\n",
    "\t\t\n",
    "\telif feature_vector in ['srl_unvalidated_text_descriptives']:        \n",
    "\t\t\n",
    "\t\tX_train = dfs['train']['srl_unvalidated_text_descriptives'] \n",
    "\t\tX_test = dfs['test']['srl_unvalidated_text_descriptives']\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\ty_test = dfs['test']['y']\n",
    "\t\n",
    "\n",
    "\t\n",
    "\telif feature_vector in ['all-MiniLM-L6-v2']:\n",
    "\t\tX_train = dfs['train']['all-MiniLM-L6-v2'] \n",
    "\t\t# X_val = dfs['val']['all-MiniLM-L6-v2']    \n",
    "\t\tX_test = dfs['test']['all-MiniLM-L6-v2']\n",
    "\t\ty_train = dfs['train']['y']\n",
    "\t\t# y_val = dfs['val']['y']\n",
    "\t\ty_test = dfs['test']['y']\n",
    "\t\t\n",
    "\t\n",
    "\treturn X_train, y_train,X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46414c-52d3-41a9-916e-d02d7d9180f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "parameters =   {'model__colsample_bytree': [1, 0.5, 0.1],\n",
    "                'model__max_depth': [-1,10,20], #-1 is the default and means No max depth\n",
    "                'model__min_child_weight': [0.01, 0.001, 0.0001],\n",
    "                'model__min_child_samples': [10, 20,40], #alias: min_data_in_leaf\n",
    "               }\n",
    "        \n",
    "\n",
    "combinations = list(product(*parameters.values()))\n",
    "        \n",
    "def get_combinations(parameters):\n",
    "    \n",
    "    parameter_set_combinations = []\n",
    "    for combination in combinations:\n",
    "        parameter_set_i = {}\n",
    "        \n",
    "        for i, k in enumerate(parameters.keys()):\n",
    "            parameter_set_i[k] = combination[i]\n",
    "        parameter_set_combinations.append(parameter_set_i)\n",
    "    return parameter_set_combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e83b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2270b02-dd07-41f3-bd76-bf37fbd51688",
   "metadata": {},
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087fff9-70b4-4298-808f-8ba97f79a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toy = False\n",
    "\n",
    "# config\n",
    "\n",
    "feature_vectors = ['all-MiniLM-L6-v2', 'srl_unvalidated','SRL GPT-4 Turbo', 'liwc22', 'liwc22_semantic'] # srl_unvalidated_text_descriptives','text_descriptives' ]\n",
    "sample_sizes = ['all', 150] \n",
    "\n",
    "\n",
    "if task == 'classification':\n",
    "\tscoring = 'f1'\n",
    "\tmetrics_to_report = 'all'\n",
    "\tmodel_names = ['LGBMRegressor', 'LogisticRegression']\n",
    "\t\n",
    "elif task == 'regression':\n",
    "\tscoring = 'neg_mean_squared_error'\n",
    "\t# metrics_to_report = ['Model','n', 'RMSE','RMSE per value','MAE','MAE per value',  'rho', 'gridsearch', 'Best parameters']\n",
    "\tmodel_names = ['LGBMRegressor', 'Ridge']\n",
    "\tmetrics_to_report = 'all'\n",
    "\n",
    "gridsearch = True#, 'minority'\n",
    "balance = True\n",
    "output_dir = './data/output/ml_performance/'\n",
    "os.makedirs(output_dir , exist_ok=True)\n",
    "\n",
    "\n",
    "# 64,51,54 vs .4, .25, 56 (with much more training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b712a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skopt import BayesSearchCV # had to replace np.int for in in transformers.py\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "reload(metrics_report)\n",
    "\n",
    "\n",
    "from concept_tracker.utils.metrics_report import cm, classification_report, regression_report\n",
    "regression_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def regression_report(y_test,y_pred,y_train=None,gridsearch=None, best_params=None,feature_vector=None,model_name=None,metrics_to_report = 'all', plot = True, save_fig_path = None, n = 'all', round_to = 2, figsize=(4,8), ordinal_ticks = True):\n",
    "\t'''\n",
    "\tmetrics = {'all', ['MAE','RMSE','rho', 'Best parameters']\n",
    "\t}\n",
    "\t'''\n",
    "\t\n",
    "\t# Metrics\n",
    "\t# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\trmse = metrics.mean_squared_error(y_test, y_pred, squared=False )\n",
    "\tmae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "\tr2 = metrics.r2_score(y_test, y_pred)    \n",
    "\tr, p = pearsonr(y_test, y_pred)    \n",
    "\trho, p = spearmanr(y_test, y_pred)    \n",
    "\t\n",
    "\tresults_dict = {\n",
    "\t\t'Features':feature_vector,\n",
    "\t\t'Estimator':model_name,\n",
    "\t\t'n':n,\n",
    "\t\t'y_train_min': np.min(y_train),\n",
    "\t\t'y_train_max': np.max(y_train),        \n",
    "\t\t'RMSE':np.round(rmse,round_to ),\n",
    "\t\t'MAE':np.round(mae,round_to ),\n",
    "\t\t'R^2':np.round(r2,round_to ),    \n",
    "\t\t'r':np.round(r,round_to ),    \n",
    "\t\t'rho':np.round(rho,round_to ),    \n",
    "\t\t'gridsearch':gridsearch,\n",
    "\t\t'Best parameters': str(best_params),\n",
    "\t\t}    \n",
    "\tresults = pd.DataFrame(results_dict, index=[model_name]).round(3)\n",
    "\t# results_all.append(results)\n",
    "\t\n",
    "\tif metrics_to_report == 'all' or ('RMSE per value' in metrics_to_report and 'MAE per value' in metrics_to_report):\n",
    "\t\ty_pred_test = {}\n",
    "\t\ty_pred_test['RMSE per value'] = []\n",
    "\t\ty_pred_test['MAE per value'] = []\n",
    "\t\tfor value in np.unique(y_test):\n",
    "\t\t\ty_pred_test_i = [[pred,test] for pred,test in zip(y_pred,y_test) if test == value]\n",
    "\t\t\ty_pred_test[value] = np.array(y_pred_test_i)\n",
    "\t\t\ty_pred_i = [n[0] for n in y_pred_test_i]\n",
    "\t\t\ty_test_i = [n[1] for n in y_pred_test_i]\n",
    "\t\t\trmse_i = metrics.mean_squared_error(y_test_i, y_pred_i, squared=False )\n",
    "\t\t\tmae_i = metrics.mean_absolute_error(y_test_i, y_pred_i)\n",
    "\t\t\ty_pred_test['RMSE per value'].append(np.round(rmse_i,round_to ))\n",
    "\t\t\ty_pred_test['MAE per value'].append(np.round(mae_i,round_to ))\n",
    "\t\t# print(y_pred_test['RMSE per value'])\n",
    "\t\tresults_dict.update({\n",
    "\t\t'RMSE per value':f\"{y_pred_test['RMSE per value']}\",\n",
    "\t\t'MAE per value':f\"{y_pred_test['MAE per value']}\"\n",
    "\t\t})\n",
    "\t\tmacro_avg_rmse = np.round(np.mean(y_pred_test['RMSE per value']), round_to)\n",
    "\t\tmacro_avg_mae = np.round(np.mean(y_pred_test['MAE per value']), round_to)\n",
    "\n",
    "\t\tresults_dict.update({\n",
    "\t\t'Macro avg. RMSE':f\"{macro_avg_rmse}\",\n",
    "\t\t'Macro avg. MAE':f\"{macro_avg_mae}\",\n",
    "\t\t})\n",
    "\n",
    "\t\t# metrics_to_report_2 = metrics_to_report.copy()\n",
    "\t\t# metrics_to_report_2.remove('RMSE') #redudant\n",
    "\t\t# metrics_to_report_2.remove('MAE') #redudant\n",
    "\t\tresults = pd.DataFrame(results_dict, index=[model_name]) # replace with updated metrics\n",
    "\t\t# results = results[metrics_to_report_2]\n",
    "\n",
    "\t\n",
    "\t# Plot result for a regression task: true value vs predicted values\n",
    "\t# ============================================================\n",
    "\tplt.clf()\n",
    "\tplt.figure(figsize=figsize)  # Width=10 inches, Height=6 inches\n",
    "\n",
    "\tplt.style.use('default')  # Example of applying the 'ggplot' style\n",
    "\tplt.scatter(y_test, y_pred, alpha = 0.05)\n",
    "\t# plt.title(f\"{feature_vector.capitalize().replace('_',' ')}\")\n",
    "\tplt.xlabel('True values')\n",
    "\tplt.ylabel('Predicted values')\n",
    "\t\n",
    "\t\n",
    "\tticks = list(np.unique(y_test))\n",
    "\tif ordinal_ticks and len(ticks)<12:\n",
    "\t\tplt.xticks(ticks=ticks,labels = [str(int(n)) for n in ticks]) \n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\tif save_fig_path:\n",
    "\t\tplt.savefig(save_fig_path+'.png', dpi=300)    \n",
    "\t# plt.show()\n",
    "\treturn results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if feature_vector:\n",
    "\t# \tmodel_name_for_df = f\"{feature_vector} {model_name}\"\n",
    "\t# else:\n",
    "\t# \tmodel_name_for_df = f\"{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e795357-1ca5-4c5d-af1f-bec26655e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# TODO: see where to save feature_vector (tfidf, liwc22) and where to save model_name (Ridge, LightGBM)\n",
    "\n",
    "\n",
    "\n",
    "ts_i = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "if toy:\n",
    "\tsample_sizes = [150]\n",
    "\tfeature_vectors = feature_vectors[:2]\n",
    "\n",
    "for n in sample_sizes:\n",
    "\tresults = []\n",
    "\t# for gridsearch in [True]:\n",
    "\n",
    "\t# for feature_vector in ['srl_unvalidated', 'all-MiniLM-L6-v2']:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "\tfor feature_vector in feature_vectors:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "\n",
    "\t\tif toy:\n",
    "\t\t\toutput_dir_i = output_dir + f'results_{ts_i}_toy/'\n",
    "\t\telse:\n",
    "\t\t\toutput_dir_i = output_dir + f'results_{ts_i}_{n}_{task}_{balance_values[-1]}/'\n",
    "\t\t\t\n",
    "\t\tos.makedirs(output_dir_i, exist_ok=True)\n",
    "\t\t\n",
    "\t\tif feature_vector == 'liwc22_semantic':\n",
    "\t\t\tX_train, y_train, X_test, y_test = get_splits('liwc22')\n",
    "\t\t\tX_train = X_train[liwc_semantic]\n",
    "\t\t\t# X_val = X_val[liwc_semantic]\n",
    "\t\t\tX_test = X_test[liwc_semantic]\n",
    "\t\n",
    "\t\telse:\n",
    "\t\t\tX_train, y_train,X_test, y_test = get_splits(feature_vector)\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\n",
    "\t\tif toy:\n",
    "\t\t\tX_train['y'] = y_train\n",
    "\t\t\tX_train = X_train.sample(n = 100)\n",
    "\t\t\ty_train = X_train['y'].values\n",
    "\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\n",
    "\t\telif n!='all':\n",
    "\t\t\tX_train['y'] = y_train\n",
    "\t\t\tX_train = X_train.sample(n = n, random_state=123)\n",
    "\t\t\ty_train = X_train['y'].values\n",
    "\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\n",
    "\n",
    "\t\tif task == 'classification':\n",
    "\t\t\tencoder = LabelEncoder()\n",
    "\n",
    "\t\t\t# Fit and transform the labels to integers\n",
    "\t\t\ty_train = encoder.fit_transform(y_train)\n",
    "\t\t\ty_test = encoder.transform(y_test)\n",
    "\n",
    "\t\t\n",
    "\t\tfor model_name in model_names: \n",
    "\t\n",
    "\t\t\tpipeline = get_pipelines(feature_vector, model_name = model_name)\n",
    "\t\t\tprint(pipeline)\n",
    "\t\t\n",
    "\t\t\tif gridsearch == 'minority':\n",
    "\t\t\t\t# Obtain all hyperparameter combinations\n",
    "\t\t\t\tparameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\t\t\tparameter_set_combinations = get_combinations(parameters)\n",
    "\t\t\t\tscores = {}\n",
    "\t\t\t\tfor i, set in enumerate(parameter_set_combinations):\n",
    "\t\t\t\t\tpipeline.set_params(**set)\n",
    "\t\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\t\ty_pred = pipeline.predict(X_val) # validation set \n",
    "\t\t\t\t\trmse_per_value = []\n",
    "\t\t\t\t\trmse = metrics.mean_squared_error(y_val, y_pred, squared=False ) # validation set \n",
    "\t\t\t\t\tfor value in np.unique(y_val):\n",
    "\t\t\t\t\t\ty_pred_test_i = [[pred,test] for pred,test in zip(y_pred,y_val) if test == value] # validation set \n",
    "\t\t\t\t\t\ty_pred_i = [n[0] for n in y_pred_test_i]\n",
    "\t\t\t\t\t\ty_test_i = [n[1] for n in y_pred_test_i]\n",
    "\t\t\t\t\t\trmse_i = metrics.mean_squared_error(y_test_i, y_pred_i, squared=False )\n",
    "\t\t\t\t\t\trmse_per_value.append(rmse_i )\n",
    "\t\t\t\t\tscores[i] = [rmse]+rmse_per_value+[str(set)]\n",
    "\t\t\t\tscores = pd.DataFrame(scores).T\n",
    "\t\t\t\tscores.columns = ['RMSE', 'RMSE_2', 'RMSE_3', 'RMSE_4', 'Parameters']\n",
    "\t\t\t\tscores = scores.sort_values('RMSE_4')\n",
    "\t\t\t\tbest_params = eval(scores['Parameters'].values[0])\n",
    "\t\t\t\tpipeline.set_params(**best_params)\n",
    "\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\ty_pred = pipeline.predict(X_test)\n",
    "\t\t\t\t\n",
    "\t\t\telif gridsearch == True:\n",
    "\t\t\t\tparameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\n",
    "\t\t\t\tpipeline = BayesSearchCV(pipeline, parameters, cv=5, scoring=scoring, return_train_score=False,\n",
    "\t\t\t\tn_iter=32, random_state=123)    \n",
    "\t\t\t\tif feature_vector != 'tfidf':\n",
    "\t\t\t\t\tif 'y' in X_train.columns:\n",
    "\t\t\t\t\t\twarnings.warn('y var is in X_train, removing')\n",
    "\t\t\t\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\tbest_params = pipeline.best_params_\n",
    "\t\t\t\tbest_model = pipeline.best_estimator_\n",
    "\t\t\t\tif feature_vector != 'tfidf':\n",
    "\t\t\t\t\tif 'y' in X_test.columns:\n",
    "\t\t\t\t\t\twarnings.warn('y var is in X_test, removing')\n",
    "\t\t\t\t\t\tX_test = X_test.drop('y', axis=1)\n",
    "\t\t\t\ty_pred = best_model.predict(X_test)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\tbest_params = 'No hyperparameter tuning'\n",
    "\t\t\t\ty_pred = pipeline.predict(X_test)\n",
    "\t\t\t\n",
    "\t\t\t# Predictions\n",
    "\t\t\ty_pred_df = pd.DataFrame(y_pred)\n",
    "\t\t\ty_pred_df.to_csv(output_dir_i+f'y_pred_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv', index=False)\n",
    "\t\t\tpath = output_dir_i + f'scatter_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}'\n",
    "\t\t\n",
    "\t\t\t# Performance\n",
    "\t\t\tif task == 'classification':\n",
    "\t\t\t\tcm_df_meaning, cm_df, cm_df_norm = cm(y_test,y_pred, output_dir_i, model_name, ts_i, classes = balance_values, save=True)\n",
    "\t\t\t\ty_proba = pipeline.predict_proba(X_test)       # Get predicted probabilities\n",
    "\t\t\t\ty_proba_1 = y_proba[:,1]\n",
    "\t\t\t\ty_pred = y_proba_1>=0.5*1                   # define your threshold\n",
    "\t\t\t\tresults_i = classification_report(y_test, y_pred, y_proba_1, output_dir_i,gridsearch=gridsearch,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=model_name,round_to = 2, ts = ts_i)\n",
    "\t\t\telif task == 'regression':\n",
    "\n",
    "\t\t\t\tresults_i =regression_report(y_test,y_pred,y_train=y_train,\n",
    "\t\t\t\t\t\t\t\t\t\tmetrics_to_report = metrics_to_report,\n",
    "\t\t\t\t\t\t\t\t\t\t\tgridsearch=gridsearch,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=model_name, plot = True, save_fig_path = path,n = n, round_to = 2)\n",
    "\t\t\tresults_i.to_csv(output_dir_i + f'results_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv')\n",
    "\t\t\tdisplay(results_i)\n",
    "\t\t\tresults.append(results_i)\n",
    "\t\t\tresults_df = pd.concat(results)\n",
    "\t\t\tresults_df = results_df.reset_index(drop=True)\n",
    "\t\t\tresults_df.to_csv(output_dir_i + f'results_{n}_{ts_i}.csv', index=False)\n",
    "\t\t\n",
    "\t\t\t# Feature importance\n",
    "\t\t\tif feature_vector == 'tfidf':\n",
    "\t\t\t\tif model_name in ['XGBRegressor']:\n",
    "\t\t\t\t\twarnings.warn('Need to add code to parse XGBoost feature importance dict')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfeature_importances = tfidf_feature_importances(pipeline, top_k = 50, savefig_path = output_dir_i + f'feature_importance_{feature_vector}_{model_name}_{n}_{ts_i}')\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature_names = X_train.columns\n",
    "\t\t\t\tfeature_importance = generate_feature_importance_df(pipeline, model_name,feature_names,  xgboost_method='weight', model_name_in_pipeline = 'model')\n",
    "\t\t\t\tif str(feature_importance) != 'None':       # I only implemented a few methods for a few models\n",
    "\t\t\t\t\tfeature_importance.to_csv(output_dir_i + f'feature_importance_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv', index = False)        \n",
    "\t\t\t\t\t# display(feature_importance.iloc[:50])\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t\t# NaN analysis\n",
    "\t\t\tif type(X_train) == pd.core.frame.DataFrame:\n",
    "\t\t\t\tdf = X_train.copy()\n",
    "\t\t\t\t# Find the column and index of NaN values\n",
    "\t\t\t\tnan_indices = df.index[df.isnull().any(axis=1)].tolist()\n",
    "\t\t\t\tnan_columns = df.columns[df.isnull().any()].tolist()\n",
    "\t\t\t\t# print(\"Indices of NaN values:\", nan_indices)\n",
    "\t\t\t\tprint(\"Columns with NaN values:\", nan_columns)\n",
    "\t\t\t\tprint(df.size)\n",
    "\t\t\t\tnans = df.isna().sum().sum()\n",
    "\t\t\t\tprint('% of nans:', np.round(nans/df.size,3))\n",
    "\t\t\t\n",
    "\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train,X_test, y_test = get_splits('srl_unvalidated_text_descriptives')\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5895b",
   "metadata": {},
   "source": [
    "# Error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_i = '24-02-15T20-17-48'\n",
    "n = 'all'\n",
    "\n",
    "output_dir_i = output_dir + f'results_{ts_i}_{n}_{task}_{balance_values[-1]}/'\n",
    "\n",
    "results = []\n",
    "# for gridsearch in [True]:\n",
    "\n",
    "# for feature_vector in ['srl_unvalidated', 'all-MiniLM-L6-v2']:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "for feature_vector in feature_vectors:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "\tif feature_vector == 'liwc22_semantic':\n",
    "\t\tX_train, y_train,X_val, y_val, X_test, y_test = get_splits('liwc22')\n",
    "\t\tX_train = X_train[liwc_semantic]\n",
    "\t\tX_val = X_val[liwc_semantic]\n",
    "\t\tX_test = X_test[liwc_semantic]\n",
    "\n",
    "\telse:\n",
    "\t\tX_train, y_train,X_val, y_val, X_test, y_test = get_splits(feature_vector)\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\tif toy:\n",
    "\t\tX_train['y'] = y_train\n",
    "\t\tX_train = X_train.sample(n = 100)\n",
    "\t\ty_train = X_train['y'].values\n",
    "\t\tX_train = X_train.drop('y', axis=1)\n",
    "\n",
    "\telif n!='all':\n",
    "\t\tX_train['y'] = y_train\n",
    "\t\tX_train = X_train.sample(n = n, random_state=42)\n",
    "\t\ty_train = X_train['y'].values\n",
    "\t\tX_train = X_train.drop('y', axis=1)\n",
    "\n",
    "\n",
    "\tif task == 'classification':\n",
    "\t\tencoder = LabelEncoder()\n",
    "\n",
    "\t\t# Fit and transform the labels to integers\n",
    "\t\ty_train = encoder.fit_transform(y_train)\n",
    "\t\ty_test = encoder.transform(y_test)\n",
    "\n",
    "\t\n",
    "\tfor model_name in model_names: \n",
    "\t\ty_pred = pd.read_csv(output_dir_i+f'y_pred_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv')\n",
    "\t\tbreak\n",
    "\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_df = y_pred.copy()\n",
    "i = 2\n",
    "y_df['y_test'] = y_test\n",
    "y_df.columns = ['y_pred', 'y_test']\n",
    "y_df_i = y_df[y_df['y_test']==i]\n",
    "y_df_i['error'] = y_df_i['y_pred'] - y_df_i['y_test']\n",
    "y_df_i = y_df_i.sort_values(by='error')\n",
    "X_test_text = dfs['test']['df_text']\n",
    "print(X_test_text.shape, y_df.shape)\n",
    "# display(X_test_text.head(), y_df[:5])\n",
    "display(y_df_i.iloc[:10])\n",
    "display(X_test_text.loc[y_df_i.index[:10]])\n",
    "docs = X_test_text.loc[y_df_i.index[:10]]['text'].to_list()\n",
    "\n",
    "print(docs)\n",
    "# metrics.mean_absolute_error(y_test, y_pred.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "sys.path.append( './../../concept-tracker/') # TODO: replace with pip install construct-tracker\n",
    "from concept_tracker import lexicon\n",
    "\n",
    "\n",
    "def load_lexicon(path):\n",
    "\tlexicon = dill.load(open(path, \"rb\"))\n",
    "\treturn lexicon\n",
    "srl = load_lexicon(\"./data/input/lexicons/suicide_risk_lexicon_calibrated_unmatched_tokens_unvalidated_24-02-15T19-30-52.pickle\")\n",
    "\n",
    "\n",
    "feature_vectors, matches_counter_d, matches_per_doc, matches_per_construct  = lexicon.extract(docs,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsrl.constructs,normalize = normalize_lexicon, return_matches=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tadd_lemmatized_lexicon=True, lemmatize_docs=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\texact_match_n = srl.exact_match_n,exact_match_tokens = srl.exact_match_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e10ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(docs[i])\n",
    "constructs_alphabetical = constructs_in_order.copy()\n",
    "constructs_alphabetical.sort()\n",
    "pd.DataFrame(matches_per_doc[i])[constructs_alphabetical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35483d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4089567c",
   "metadata": {},
   "source": [
    "# Clean up results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfdc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_empty_row(df, index_to_insert):\n",
    "\t# Splitting the DataFrame\n",
    "\tdf_before = df.iloc[:index_to_insert, :]\n",
    "\tdf_after = df.iloc[index_to_insert:, :]\n",
    "\n",
    "\t# Creating an empty row (all values set to NaN or any desired value)\n",
    "\t# The length of the empty DataFrame should match the number of columns in the original DataFrame\n",
    "\tempty_row = pd.DataFrame({col: np.nan for col in df.columns}, index=[index_to_insert])\n",
    "\n",
    "\t# Adjusting the index for df_after to accommodate the new row\n",
    "\tdf_after.index = df_after.index + 1\n",
    "\n",
    "\t# Concatenating the DataFrames\n",
    "\tdf_updated = pd.concat([df_before, empty_row, df_after])\n",
    "\n",
    "\t# Resetting the index if desired\n",
    "\tdf_updated = df_updated.reset_index(drop=True)\n",
    "\treturn df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = ['all', 150] # TODO\n",
    "model_names = ['LGBMRegressor', 'Ridge']\n",
    "timestamp = '24-02-08T20-05-33'\n",
    "\n",
    "\n",
    "for n in sample_sizes:\n",
    "\tfor model in model_names:\t\n",
    "\t\t\n",
    "\t\tresults_dir = f'results_{timestamp}_{n}/'\n",
    "\t\tresults_df = pd.read_csv('./data/output/ml_performance/'+results_dir+f'results_{n}_{timestamp}.csv')\n",
    "\t\tresults_df = results_df[results_df['Estimator'].str.contains(model)]\n",
    "\t\tresults_df.reset_index(drop=True,inplace=True)\n",
    "\t\tresults_df = results_df.drop(['n','Estimator',  'gridsearch', 'Best parameters', 'y_train_min', 'y_train_max', 'R^2', 'r'], axis = 1)\n",
    "\t\tresults_df = insert_empty_row(results_df, 4)\n",
    "\t\tresults_df = insert_empty_row(results_df, 6)\n",
    "\t\tresults_df.to_csv(f'./data/output/tables/'+f'results_{model}_{n}.csv', index=False)\n",
    "\t\t\n",
    "\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b0643-e035-40a7-9b1a-b414fbf7f2cf",
   "metadata": {},
   "source": [
    "# Feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7e193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fa690",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 'results_24-02-08T15-58-41'\n",
    "model = 'LGBMRegressor'\n",
    "input_dir = './data/output/ml_performance/'+timestamp+'_all/'\n",
    "files = os.listdir(input_dir)\n",
    "feature_vectors = ['srl_unvalidated', 'liwc22_semantic']\n",
    "table_names = ['SRL unvalidated', 'LIWC-22 semantic']\n",
    "\n",
    "rank_col_name = 'Rank'\n",
    "files\n",
    "feature_importance = []\n",
    "for file, table_name in zip(feature_vectors, table_names):\n",
    "    file1 = [n for n in files if ('feature_importance_'+file in n and 'clean' not in n)][0]\n",
    "    \n",
    "print(file1)\n",
    "fi = pd.read_csv(input_dir+file1)\n",
    "# fi.columns = ['Feature', 'Split', 'Gain']\n",
    "# fi=fi.drop('Split', axis=1).round(1)\n",
    "# fi = fi.reset_index()\n",
    "# fi.columns = [rank_col_name, 'Feature', 'Gain']\n",
    "# fi[rank_col_name]+=1\n",
    "# fi[rank_col_name] = fi[rank_col_name].astype(str)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature, correlate feature with y\n",
    "import math\n",
    "liwc22_X = dfs['train']['liwc22_X']\n",
    "liwc22_y = dfs['train']['liwc22_y']\n",
    "liwc_rho = {}\n",
    "for feature in liwc22_X.columns:\n",
    "\tfiltered_list1, filtered_list2 = zip(*[(x, y) for x, y in zip(liwc22_y, liwc22_X[feature].values) if not math.isnan(x) and not math.isnan(y)])\n",
    "\n",
    "\t# Converting the tuples back to lists\n",
    "\tfiltered_list1 = list(filtered_list1)\n",
    "\tfiltered_list2 = list(filtered_list2)\n",
    "\tr,p = spearmanr(filtered_list1, filtered_list2)\n",
    "\t# r,p = spearmanr(liwc22_y, liwc22_X[feature])\n",
    "\t# if p <= 0.05:\n",
    "\tliwc_rho[feature] = np.round(r,2)\n",
    "\tif str(r)=='nan':\n",
    "\t\t\n",
    "\t\tprint(feature)\n",
    "\t# else:\n",
    "\t\t# liwc_rho[feature] = np.nan\n",
    "\n",
    "\n",
    "# For each feature, correlate feature with y\n",
    "srl_unv_X = dfs['train']['srl_unvalidated'].drop('y', axis=1)\n",
    "srl_unv_y = dfs['train']['srl_unvalidated']['y'].values\n",
    "srl_unv_rho = {}\n",
    "for feature in srl_unv_X.columns:\n",
    "\t# remove nans:\n",
    "\tfiltered_list1, filtered_list2 = zip(*[(x, y) for x, y in zip(srl_unv_y, srl_unv_X[feature].values) if not math.isnan(x) and not math.isnan(y)])\n",
    "\n",
    "\t# Converting the tuples back to lists\n",
    "\tfiltered_list1 = list(filtered_list1)\n",
    "\tfiltered_list2 = list(filtered_list2)\n",
    "\tr,p = spearmanr(filtered_list1, filtered_list2)\n",
    "\t# if p <= 0.05:\n",
    "\tsrl_unv_rho[feature] = np.round(r,2)\n",
    "\t# else:\n",
    "\t\t# srl_unv_rho[feature] = np.nan\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd167d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db7913-34df-437d-86f8-8a263c11e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 'results_24-02-08T15-58-41'\n",
    "model = 'LGBMRegressor'\n",
    "input_dir = './data/output/ml_performance/'+timestamp+'_all/'\n",
    "files = os.listdir(input_dir)\n",
    "feature_vectors = ['srl_unvalidated', 'liwc22_semantic']\n",
    "table_names = ['SRL unvalidated', 'LIWC-22 semantic']\n",
    "\n",
    "rank_col_name = 'Rank'\n",
    "files\n",
    "feature_importance = []\n",
    "for file, table_name in zip(feature_vectors, table_names):\n",
    "\ttimestamp_i = timestamp.replace('results_', '')\n",
    "\tfile1 = f'feature_importance_{file}_{model}_gridsearch-True_all_{timestamp_i}.csv'\n",
    "\t\n",
    "\t\n",
    "\tfi = pd.read_csv(input_dir+file1)\n",
    "\tfi.columns = ['Feature', 'Split', 'Gain']\n",
    "\tfi=fi.drop('Split', axis=1).round(1)\n",
    "\tfi = fi.reset_index()\n",
    "\tfi.columns = [rank_col_name, 'Feature', 'Gain']\n",
    "\tfi[rank_col_name]+=1\n",
    "\tfi[rank_col_name] = fi[rank_col_name].astype(str)\n",
    "\tif 'liwc22' in file:\n",
    "\t\tfi['rho'] = fi['Feature'].map(liwc_rho)\n",
    "\telse:\n",
    "\t\tfi['rho'] = fi['Feature'].map(srl_unv_rho)\n",
    "\n",
    "\tfi.to_csv(input_dir+'feature_importance_'+file+'_clean.csv', index=False)\n",
    "\tcolumns = pd.MultiIndex.from_tuples([\n",
    "\t(table_name, rank_col_name),\n",
    "\t(table_name, 'Feature'),\n",
    "\t(table_name, 'Gain'),\n",
    "\t(table_name, 'rho'),\n",
    "\t])\n",
    "\tfi.columns = columns\n",
    "\tfeature_importance.append(fi)\n",
    "\n",
    "feature_importance_df = pd.concat([feature_importance[0],feature_importance[1].drop(columns=(table_names[1], rank_col_name))],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "feature_vectors = '_'.join(feature_vectors)\n",
    "\n",
    "\n",
    "\n",
    "feature_importance_df.to_csv(input_dir+f'feature_importance_{feature_vectors}_gridsearch-True_all_{timestamp}.csv', index= 0 )\n",
    "display(feature_importance_df)\n",
    "\n",
    "feature_importance_df.iloc[:20].to_csv(input_dir+f'feature_importance_{feature_vectors}_gridsearch-True_all_{timestamp}_top20.csv', index= 0 )\n",
    "\n",
    "# top 15 and bottom 10\n",
    "df0 = feature_importance[0].copy()\n",
    "top_15 = df0.head(15)\n",
    "bottom_10 = df0.tail(10)\n",
    "empty_row = pd.DataFrame(np.nan, index=[0], columns=bottom_10.columns)\n",
    "bottom_10 = pd.concat([empty_row, bottom_10]).reset_index(drop=True)\n",
    "df0 = pd.concat([top_15, bottom_10])\n",
    "df0 = df0.reset_index(drop=True)\n",
    "\n",
    "df1 = feature_importance[1].copy()\n",
    "top_15 = df1.head(15)\n",
    "bottom_10 = df1.tail(10)\n",
    "empty_row = pd.DataFrame(np.nan, index=[0], columns=bottom_10.columns)\n",
    "bottom_10 = pd.concat([empty_row, bottom_10]).reset_index(drop=True)\n",
    "df1 = pd.concat([top_15, bottom_10])\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "feature_importance_df = pd.concat([df0,df1],axis=1)\n",
    "feature_importance_df.to_csv('./data/output/tables/'+f'feature_importance_{feature_vectors}_gridsearch-True_all_{timestamp}_top_and_bottom.csv', index= 0 )\n",
    "display(feature_importance_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66113b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10639968",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train']['liwc22_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1098838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train']['srl_unvalidated'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a98bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv(input_dir+file1)\n",
    "fi.columns = ['Feature', 'Split', 'Gain']\n",
    "fi=fi.drop('Split', axis=1).round(1)\n",
    "fi = fi.reset_index()\n",
    "fi.columns = [rank_col_name, 'Feature', 'Gain']\n",
    "fi[rank_col_name]+=1\n",
    "fi[rank_col_name] = fi[rank_col_name].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4645327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi['rho'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e3912-5348-4f0a-ad7c-627395a6504e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38a80a-4a11-459a-b950-109d9cce2283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109390c1-962e-4e0c-829c-4972718635c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af23ab6-fcd9-47da-a768-7954c397b65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1150cf-cdc6-48c3-8cbc-41bfc85512c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd6058-5907-4068-9759-719a1bc5dad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "392f442a-ff1c-4001-9ede-2059f7f65133",
   "metadata": {},
   "source": [
    "# Ordinal classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf0cc5-e756-45d5-864d-368b2167d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e9313-8097-4428-969d-aeeeca6bbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "sklearn = 1\n",
    "import sklearn \n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e09e63-a6bf-447f-b594-e0809df5bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__repr__()\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abf934-f583-4f8d-8ee6-82e5502e7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = 'srl_unvalidated'\n",
    "scoring = 'f1_macro'\n",
    "gridsearch = True\n",
    "toy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96feda-81e9-4373-a5e0-cb2d3695724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474e12e-005e-4264-9c18-9662fe94c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8667a7-b9c9-4649-a37d-f13c0f65a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8fe94-9323-41b2-909b-d9e5e0da2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cedd8-356e-4366-8017-e17f03cdff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO crossvalidation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "sys.path.append('./../../concept-tracker/')\n",
    "from concept_tracker.ordinal import OrdinalClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "ts_i = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "if toy:\n",
    "    output_dir_i = output_dir + f'results_{ts_i}_toy/'\n",
    "else:\n",
    "    output_dir_i = output_dir + f'results_{ts_i}/'\n",
    "    \n",
    "os.makedirs(output_dir_i, exist_ok=True)\n",
    "\n",
    "\n",
    "results_all = []\n",
    "\n",
    "# disease_class_progression = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train,X_val, y_val, X_test, y_test = get_splits(feature_vector)\n",
    "\n",
    "\n",
    "if feature_vector == 'tfidf':\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# Models\n",
    "\n",
    "\n",
    "model = SVC(kernel='linear', class_weight=\"balanced\", \n",
    "              probability=True \n",
    "             )\n",
    "model = LGBMClassifier()\n",
    "oc_model = OrdinalClassifier(model)\n",
    "\n",
    "\n",
    "\n",
    "# svc_bal = SVC(kernel='rbf', class_weight=\"balanced\", probability=True)\n",
    "# svc_imb = SVC(kernel='linear', class_weight='None', probability=True)\n",
    "# svc_bal = RandomForestClassifier(class_weight=\"balanced\",)\n",
    "\n",
    "# oc_imb = OrdinalClassifier(svc_imb)\n",
    "\n",
    "models = {'LGBMClassifier':oc_model, \n",
    "          'LGBMClassifier':model}\n",
    "\n",
    "oc_params = [{\"reverse_classes\": True}, {'reverse_classes': False}]\n",
    "\n",
    "# scoring = ['f1_weighted', 'precision_weighted', 'recall_weighted', 'roc_auc']\n",
    "# cv_results = cross_validate(model, X, y_t, cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "tests = []\n",
    "\n",
    "\n",
    "\n",
    "# Train your model on X_train and y_train\n",
    "# Evaluate your model on X_test and y_test\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X, y_t)):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = [y_t[i] for i in train_index], [y_t[i] for i in test_index]\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_t, shuffle=True, test_size=0.2, \n",
    "#                                                     # random_state=random_state, \n",
    "#                                                     stratify=y_t)\n",
    "\n",
    "\n",
    "\n",
    "ord_pass = 0\n",
    "for model_name, model  in models.items():\n",
    "    param ={}\n",
    "    if \"Ordinal\" in model.__repr__():\n",
    "        if ord_pass == 0:\n",
    "            param = oc_params[ord_pass]  #first time through\n",
    "            ord_pass +=1\n",
    "        elif ord_pass==1:\n",
    "            param = oc_params[ord_pass]  # second pass\n",
    "    model.set_params(**param)\n",
    "\n",
    "    # model_name = model.__repr__()\n",
    "\n",
    "    # cv = cross_validate(model, X_train, y_train,  n_jobs=-1, scoring=\"f1_macro\")\n",
    "    # cv[\"clf\"] = model_name\n",
    "    # cv['param'] = param\n",
    "    # cv['y_name'] = y_t.name\n",
    "    \n",
    "    pipeline = get_pipelines(feature_vector, model_name = model_name)\n",
    "    print(pipeline)\n",
    "\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    if gridsearch == True:\n",
    "        parameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "        \n",
    "        pipeline = BayesSearchCV(pipeline, parameters, cv=5, scoring=scoring, return_train_score=False,\n",
    "        n_iter=32)    \n",
    "        if feature_vector != 'tfidf':\n",
    "            if 'y' in X_train.columns:\n",
    "                break\n",
    "        pipeline.fit(X_train,y_train)\n",
    "        best_params = pipeline.best_params_\n",
    "        best_model = pipeline.best_estimator_\n",
    "        if feature_vector != 'tfidf':\n",
    "            if 'y' in X_test.columns:\n",
    "                break\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        pipeline.fit(X_train,y_train)\n",
    "        best_params = 'No hyperparameter tuning'\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_df = pd.DataFrame(y_pred)\n",
    "    y_pred_df.to_csv(output_dir_i+f'y_pred_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{ts_i}.csv', index=False)\n",
    "    # path = output_dir_i + f'scatter_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{ts_i}'\n",
    "\n",
    "    # Performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(model.__repr__())\n",
    "    print(\"params are: \".format(param))\n",
    "    # try:\n",
    "        #this gets into using pred_proba\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # y_proba = model.predict_proba(X_test)       # Get predicted probabilities\n",
    "    # y_proba = np.round(y_proba,2)               # round to 2 decimals\n",
    "    # y_proba                                     # probabilitiesfor y=0 and y=1\n",
    "    # y_proba_1 = y_proba[:,1]\n",
    "    # y_pred = y_proba_1>=0.5*1                   # define your threshold\n",
    "    # y_pred_b = model.predict(X_test)            # binary classification using 0.5 threshold\n",
    "\n",
    "    # # Confusion matrix\n",
    "    # cm = metrics.confusion_matrix(y_test, y_pred)                   # REMEMBER you can also obtain proportions with normalize argument: confusion_matrix(y_test, y_pred, normalize = 'all')\n",
    "    # cm\n",
    "    # cm_display = metrics.ConfusionMatrixDisplay(cm,display_labels=[0,1]).plot() # sklearn provides a way to plot it. IMPORTANT YOU KNOW WHICH AXIS IS TRUE VS. PREDICTED\n",
    "    # cm_df_meaning = pd.DataFrame([['TN', 'FP'],['FN','TP']], index=[0,1], columns=[0,1])\n",
    "    # cm_df_meaning \n",
    "\n",
    "    # # Metrics\n",
    "    print(metrics.classification_report(y_test, y_pred))                # here we need to print to view correctly\n",
    "\n",
    "    # # Custom classification report\n",
    "    precision = metrics.precision_score(y_test, y_pred, average =average)\n",
    "    recall = metrics.recall_score(y_test, y_pred, average =average)\n",
    "    specificity = metrics.recall_score(y_test, y_pred, pos_label=0, average =average)    # specificity is the recall of the negative class or control group\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average =average)\n",
    "\n",
    "    # # Here we use all probabilities, not just Y=1, in binary we use y_proba_1\n",
    "    # roc_auc = metrics.roc_auc_score(y_test, y_proba, average ='weighted', multi_class='ovr')  # IMPORTANT: other metrics take binary predictions y_pred. Here we test different thresholds, so we need probabilities (this will change the ROC AUC score)\n",
    "\n",
    "    results_dict = {\n",
    "        'Model' : f\"{feature_vector} {model_name}\",\n",
    "        'Precision':precision,\n",
    "        'Recall':recall,\n",
    "        'Specificity':specificity,    \n",
    "        'F1':f1,\n",
    "        # 'ROC AUC':roc_auc,\n",
    "        }    \n",
    "    model_name  = str(model.get_params().get('estimator'))+' reverse_classes='+str(model.get_params().get('reverse_classes'))\n",
    "    results = pd.DataFrame(results_dict, index=[model_name]).round(3)\n",
    "    display(results)\n",
    "    results_all.append(results) \n",
    "    # TODO save output_dir_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a005a-59e6-4c32-808c-dbf4f28b2787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a5eae-b508-431e-9f5a-c8482da8204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(font_scale = 0.5)\n",
    "\n",
    "sns.clustermap(X_train.corr(method='spearman'))\n",
    "plt.savefig('dendrogram_similarity_lexicon.png', dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2ef9c-e96e-4912-8096-1bee1071c937",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4629e-ea3e-4f82-836e-a51249eb2641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab4eee-974b-4974-8268-0cca3a97621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce0542-fc19-4ae1-832f-b07400e62f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb                          # If installation needed: conda install py-xgboost               \n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a874ee6-8c9c-46fb-aeff-63541c019739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipelines avoid us making mistakes with fit, fit_transform and transform\n",
    "# across train and test sets. They are also easier to read, i.e., they organize\n",
    "# your code. \n",
    "\n",
    "# Configuration\n",
    "crossvalidation_k = 5\n",
    "scoring = 'neg_mean_squared_error'\n",
    "verbose = 1                     # how much to print regarding model training\n",
    "\n",
    "\n",
    "\n",
    "pipelines = {\n",
    "    'Ridge': \n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('estimator', Ridge(random_state = 1234))\n",
    "            ]),\n",
    "    # 'SVR': Pipeline([\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('estimator', SVR(verbose=True))\n",
    "    #     ]),\n",
    "    }\n",
    "\n",
    "\n",
    "paramater_grids = {    \n",
    "    # use name of step in pipeline for which you want to tune hyperparameters\n",
    "    # use doubnle underscore to access the hyperparameter\n",
    "    'Ridge' : {\n",
    "        \"estimator__alpha\": [0.0001, 0.01, 1, 100],\n",
    "        },\n",
    "\n",
    "    'SGDRegressor' : {\n",
    "        \"estimator__penalty\": ['l1', 'l2'],\n",
    "        \"estimator__alpha\": [0.0001, 0.01, 1, 100],\n",
    "        },\n",
    "    'SVR' : {\n",
    "        \"estimator__kernel\": ['linear', 'rbf'],\n",
    "        \"estimator__C\": [0.01,0.1, 1, 10, 100],\n",
    "        },\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# How many runs will this imply?\n",
    "model_name_i = 'SVR'\n",
    "param_grid = paramater_grids.get(model_name_i)\n",
    "len(ParameterGrid(param_grid))\n",
    "\n",
    "\n",
    "# Train models, evaluate, feature importance, and save outputs\n",
    "# ============================================================\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "os.makedirs(f'results_{ts}', exist_ok=True)\n",
    "\n",
    "results_all = [] # to store results for all models\n",
    "for model_name_i in pipelines.keys():\n",
    "    pipeline_i = pipelines.get(model_name_i)\n",
    "    params_i = paramater_grids.get(model_name_i)\n",
    "    \n",
    "    if pipeline_i == None or params_i==None:\n",
    "        print('did not find model configuration:', model_name_i)\n",
    "        break\n",
    "    \n",
    "    model = GridSearchCV(pipeline_i ,\n",
    "                      param_grid=params_i,\n",
    "                      scoring=scoring ,\n",
    "                      cv=crossvalidation_k, \n",
    "                      verbose=verbose, \n",
    "                      error_score='raise'\n",
    "                      )                             # define gridsearch CV\n",
    "    # You could try RandomizedSearchCV instead which will give you faster performance, maybe only slightly worse https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "    \n",
    "    model.fit(X_train, y_train)  # train model\n",
    "    best_params= model.best_params_                   # best params\n",
    "    print('best parameters: ', best_params)\n",
    "    pd.DataFrame(model.cv_results_).to_csv(f'results_{ts}/cv_results_all_{ts}.csv')\n",
    "    \n",
    "    # Performance evaluation\n",
    "    # ====================================================================\n",
    "    y_pred = model.predict(X_test)            \n",
    "\n",
    "    # If it's a classification task, you can get:\n",
    "        # If this were classification you could compute predict_proba() \n",
    "        # confusion matrix\n",
    "        # print(metrics.classification_report(y_test, y_pred))                # here we need to print to view correctly\n",
    "        # classification metrics\n",
    "    \n",
    "    y_pred_df = pd.DataFrame(y_pred)\n",
    "    y_pred_df.to_csv(f'results_{ts}/{model_name_i}_{ts}.csv', index=False)\n",
    "    path = output_dir + f'scatter_{model_name_i}_{ts}'\n",
    "    results =regression_report(y_test,y_pred,y_train=y_train,best_params=best_params,model_name=model_name_i, plot = True, save_fig_path = path, round_to = 2)\n",
    "    results_all.append(results)\n",
    "    \n",
    "\n",
    "    # feature importance  \n",
    "    # ============================================================\n",
    "    # feature_importance = feature_importance_df(model, model_name_i,feature_names,  xgboost_method='weight')\n",
    "    # if str(feature_importance) != 'None':       # I only implemented a few methods for a few models\n",
    "    #     feature_importance.to_csv(f'results_{ts}/feature_importance_{model_name_i}_{ts}.csv')        \n",
    "    \n",
    "\n",
    "\n",
    "results_all = pd.concat(results_all)\n",
    "results_all.to_csv(f'results_{ts}/results_{ts}.csv')        \n",
    "results_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19826895-2ef0-4b13-9e8d-1e0647cfbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a4367-21b9-43ae-82a4-000ae95c8a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf414e-f1b6-47a0-846e-660c9b2eaa7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947250b-2838-4556-9114-6302f1822f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50410136-74c6-42ff-b804-d459d124fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fi_df = feature_importance_df(model, model_name_i, X_train.columns, xgboost_method = 'weight', model_name_in_pipeline = 'estimator')\n",
    "fi_df.iloc[:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f115c9-b97a-406a-8634-3c760e92fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fi_df = feature_importance_df(model, model_name_i, X_train.columns, xgboost_method = 'weight', model_name_in_pipeline = 'estimator')\n",
    "fi_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5886cb-a136-461c-83ce-3a87377c6dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20721974-f408-443c-84df-c6ff8b76660b",
   "metadata": {},
   "source": [
    "# Deep learning embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99331c-4cf2-4282-8bb4-97becaa45fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dfs['train']['X']\n",
    "X_val = dfs['val']['X']\n",
    "X_test = dfs['test']['X']\n",
    "y_train = dfs['train']['y']\n",
    "y_val = dfs['val']['y']\n",
    "y_test = dfs['test']['y']\n",
    "from collections import Counter\n",
    "Counter(y_train) # Make sure it's balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d473f61-2bda-4caf-98f0-eb3cd2b1d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -i sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60637fe7-4e8d-4849-8e1e-e50cf2ea23b9",
   "metadata": {},
   "source": [
    "### Encode \n",
    "TODO: Change max_seq_length to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25734f-0419-4735-99ec-921a59d2b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers \n",
    "from sentence_transformers import SentenceTransformer, util \n",
    "\n",
    "# Encode the documents with their sentence embeddings \n",
    "# a list of pre-trained sentence transformers\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# https://huggingface.co/models?library=sentence-transformers\n",
    "\n",
    "# Here the progress bar will show you how long it will take to embedd the documents.\n",
    "\n",
    "# all-MiniLM-L6-v2 is optimized for semantic similarity of paraphrases\n",
    "sentence_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')       # load embedding\n",
    "\n",
    "# TODO: Change max_seq_length to 500\n",
    "# Note: sbert will only use fewer tokens as its meant for sentences, \n",
    "print(sentence_embedding_model .max_seq_length)\n",
    "# you can increase to closer to the base model it was trained on BERT has 512\n",
    "# sentence_embedding_model._first_module().max_seq_length = 500\n",
    "# print(sentence_embedding_model .max_seq_length) # now it takes up to 500, but will be a bit slower to encode and might not change performance a whole lot in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e200c-b200-4792-a0fc-1bc6d1a29d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fe716-0aae-4037-b215-7a923befa7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b861c1-7b99-4bb1-9152-b4519edadf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = dfs['train']['embeddings_balanced']\n",
    "X_train = dfs['train']['embeddings_balanced']\n",
    "\n",
    "X_test = dfs['test']['embeddings_balanced']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dbd89-231c-4850-b2de-8e6bf012593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipelines avoid us making mistakes with fit, fit_transform and transform\n",
    "# across train and test sets. They are also easier to read, i.e., they organize\n",
    "# your code. \n",
    "\n",
    "# Configuration\n",
    "crossvalidation_k = 3\n",
    "scoring = 'neg_mean_squared_error'\n",
    "verbose = 1                     # how much to print regarding model training\n",
    "\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "output_dir_i = f'results_embeddings_{ts}/'\n",
    "os.makedirs(output_dir_i, exist_ok=True)\n",
    "\n",
    "pipelines = {\n",
    "    'Ridge': \n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('estimator', Ridge(random_state = 1234))\n",
    "            ]),\n",
    "    # 'SVR': Pipeline([\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('estimator', SVR(verbose=True))\n",
    "    #     ]),\n",
    "    }\n",
    "\n",
    "\n",
    "paramater_grids = {    \n",
    "    # use name of step in pipeline for which you want to tune hyperparameters\n",
    "    # use doubnle underscore to access the hyperparameter\n",
    "    'Ridge' : {\n",
    "        \"estimator__alpha\": [0.0001, 0.01, 1, 100],\n",
    "        },\n",
    "\n",
    "    'SGDRegressor' : {\n",
    "        \"estimator__penalty\": ['l1', 'l2'],\n",
    "        \"estimator__alpha\": [0.0001, 0.01, 1, 100],\n",
    "        },\n",
    "    'SVR' : {\n",
    "        \"estimator__kernel\": ['linear', 'rbf'],\n",
    "        \"estimator__C\": [0.01,0.1, 1, 10, 100],\n",
    "        },\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# How many runs will this imply?\n",
    "model_name_i = 'SVR'\n",
    "param_grid = paramater_grids.get(model_name_i)\n",
    "len(ParameterGrid(param_grid))\n",
    "\n",
    "\n",
    "# Train models, evaluate, feature importance, and save outputs\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(output_dir_i, exist_ok=True)\n",
    "\n",
    "results_all = [] # to store results for all models\n",
    "for model_name_i in pipelines.keys():\n",
    "    pipeline_i = pipelines.get(model_name_i)\n",
    "    params_i = paramater_grids.get(model_name_i)\n",
    "    \n",
    "    if pipeline_i == None or params_i==None:\n",
    "        print('did not find model configuration:', model_name_i)\n",
    "        break\n",
    "    \n",
    "    model = GridSearchCV(pipeline_i ,\n",
    "                      param_grid=params_i,\n",
    "                      scoring=scoring ,\n",
    "                      cv=crossvalidation_k, \n",
    "                      verbose=verbose, \n",
    "                      error_score='raise'\n",
    "                      )                             # define gridsearch CV\n",
    "    # You could try RandomizedSearchCV instead which will give you faster performance, maybe only slightly worse https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "    \n",
    "    model.fit(X_train, y_train)  # train model\n",
    "    best_params= model.best_params_                   # best params\n",
    "    print('best parameters: ', best_params)\n",
    "    pd.DataFrame(model.cv_results_).to_csv(f'results_embeddings_{ts}/cv_results_all_{ts}.csv')\n",
    "    \n",
    "    # Performance evaluation\n",
    "    # ====================================================================\n",
    "    y_pred = model.predict(X_test)            \n",
    "\n",
    "    y_pred_df = pd.DataFrame(y_pred)\n",
    "    y_pred_df.to_csv(f'{output_dir_i}/{model_name_i}_{ts}.csv', index=False)\n",
    "    path = output_dir_i + f'scatter_{model_name_i}_{ts}'\n",
    "    results =regression_report(y_test,y_pred,y_train=y_train,best_params=best_params,model_name=model_name, plot = True, save_fig_path = path, round_to = 2)\n",
    "    results_all.append(results)\n",
    "    \n",
    "\n",
    "    # feature importance  \n",
    "    # ============================================================\n",
    "    # feature_importance = feature_importance_df(model, model_name_i,feature_names,  xgboost_method='weight')\n",
    "    # if str(feature_importance) != 'None':       # I only implemented a few methods for a few models\n",
    "    #     feature_importance.to_csv(f'results_{ts}/feature_importance_{model_name_i}_{ts}.csv')        \n",
    "    \n",
    "\n",
    "\n",
    "results_all = pd.concat(results_all)\n",
    "results_all.to_csv({output_dir_i}+f'results_{ts}.csv')        \n",
    "results_all\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc7bf4-9aa7-499e-be21-49a664f24e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb5c8b-a4d5-4f08-bb08-69eb88898c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf8a35-3aa5-4211-a30c-200a6fa1d9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_psychometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
