{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5-c_3HqF0ykA",
   "metadata": {
    "id": "5-c_3HqF0ykA"
   },
   "source": [
    "# Construct-Text Similarity: zero-shot classification with embeddings and using your own lexicons\n",
    "\n",
    "Author: Daniel Low (Harvard University)\n",
    "\n",
    "Please ask for updated reference if you use this code (preprint will be out soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "location = 'local'\n",
    "preprocessing = True # If False, load preprocessed data\n",
    "balance_training_set = True\n",
    "classes = ['suicidal_desire', 'active_rescue']\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x-b5q5cd9LWe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-b5q5cd9LWe",
    "outputId": "a4de6c41-520f-4ba2-8112-975b3be85ca9"
   },
   "outputs": [],
   "source": [
    "!python --version # tested with Python 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wb2NjiV32ves",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wb2NjiV32ves",
    "outputId": "1da004a8-fd11-4446-a052-253deb049ece"
   },
   "outputs": [],
   "source": [
    "# !pip install -q deplacy==2.0.5\n",
    "# !pip install -q flair==0.13.0\n",
    "# !pip install -q --upgrade urllib3==2.0.7\n",
    "# !pip install -q sentence-transformers==2.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y6nL6hsy_ljN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6nL6hsy_ljN",
    "outputId": "9ec170b2-a8f8-40e8-8bc7-c378598dbd57"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Authors: Daniel M. Low\n",
    "License: See license in github repository\n",
    "'''\n",
    "\n",
    "import os\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import spacy\n",
    "import deplacy\n",
    "import importlib\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.options.display.width = 0\n",
    "\n",
    "# local scripts\n",
    "import sys\n",
    "sys.path.append('./../concept-tracker') # wherever cts.py is\n",
    "from concept_tracker import cts\n",
    "from concept_tracker.utils.tokenizer import spacy_tokenizer\n",
    "import srl_constructs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "\n",
    "\n",
    "if location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/datasets/'\n",
    "  output_dir = 'home/dlow/zero_shot/data/output/'\n",
    "elif location =='local':\n",
    "  input_dir = input_dir = './../../../data/ctl/input/datasets/'\n",
    "  output_dir = './data/output/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0ca67",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4775592",
   "metadata": {},
   "source": [
    "## Documents to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(input_dir + f'train10_train_metadata_messages_clean.gzip', engine='pyarrow')\n",
    "# X_train = pd.read_csv('./data/input/ctl/X_train_all_with_interaction_preprocessed_24-03-07T04-25-04.csv', index_col = 0) # preprocessing\n",
    "test = pd.read_parquet(input_dir + f'train10_test_metadata_messages_clean.gzip', engine='pyarrow')\n",
    "# X_test = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_preprocessed_24-03-07T04-25-04.csv', index_col = 0) # preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# this is a variable I created with CTL's suicide risk assessment ladder which helps make sure certain constructs are not present at lower levels. See train_test_split_ids.ipynb for function\n",
    "def get_true_risk_8(row):\n",
    "\tif (row['3rd_party'] ==1 or row['testing'] == 1 or row['prank'] == 1):\n",
    "\t\treturn -1\n",
    "\telif (row['active_rescue'] > 0):\n",
    "\t\treturn 8 # active rescue\n",
    "\t\n",
    "\telif (row['ir_flag'] > 0):\n",
    "\t\treturn 7 # high risk\n",
    "\t\n",
    "\telif (row['timeframe'] > 0):\n",
    "\t\treturn 6 # high risk\n",
    "\t\n",
    "\telif (row['suicidal_capability'] > 0):\n",
    "\t\treturn 5 # high risk\n",
    "\t\n",
    "\telif (row['suicidal_intent']>0):\n",
    "\t\treturn 4\n",
    "\telif row['self_harm']>0:\n",
    "\t\treturn 3\n",
    "\n",
    "\telif (row['suicidal_desire']>0 or row['suicide']>0):\n",
    "\t\treturn 2\n",
    "\telse: \n",
    "\t\treturn 1\n",
    "\n",
    "\t\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['suicide_ladder_8'] == 8]['word_count_with_interaction'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eb212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active rescues tend to have shorter conversations than imminent risk without active rescue\n",
    "train[train['suicide_ladder_8'].isin([6,7])]['word_count_with_interaction'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['suicide_ladder_8'].isin([2])]['word_count_with_interaction'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4195b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "df_i = train.copy()\t\n",
    "active_rescue = df_i[df_i['suicide_ladder_8']==8] # immiment risk which could not be de-escaleted\n",
    "suicidal = df_i[df_i['suicide_ladder_8'].isin([2])] # no imminent risk, no intent or capability; desire or other forms of suicide without any of those tags\n",
    "suicidal_desire = suicidal[suicidal['suicidal_desire']==1] # no imminent risk, no intent or capability; desire confirmed\n",
    "\n",
    "# subsample to match minority class:\n",
    "smallest_group = active_rescue.shape[0]\n",
    "suicidal_desire = suicidal_desire.sample(n= smallest_group, random_state=random_seed)\n",
    "\n",
    "# add label \n",
    "active_rescue['dv'] = ['active_rescue']*active_rescue.shape[0]\n",
    "suicidal_desire['dv'] = ['suicidal_desire']*suicidal_desire.shape[0]\n",
    "train_subset = pd.concat([active_rescue, suicidal_desire]).sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "display(train_subset['dv'].value_counts())\n",
    "\n",
    "\n",
    "# Test \n",
    "split = 'test'\n",
    "df_i = test.copy()\t\n",
    "active_rescue = df_i[df_i['suicide_ladder_8']==8] # immiment risk which could not be de-escaleted\n",
    "suicidal = df_i[df_i['suicide_ladder_8'].isin([2])] # no imminent risk, no intent or capability; desire or other forms of suicide without any of those tags\n",
    "suicidal_desire = suicidal[suicidal['suicidal_desire']==1] # no imminent risk, no intent or capability; desire confirmed\n",
    "\n",
    "# add label \n",
    "active_rescue['dv'] = ['active_rescue']*active_rescue.shape[0]\n",
    "suicidal_desire['dv'] = ['suicidal_desire']*suicidal_desire.shape[0]\n",
    "test_subset = pd.concat([active_rescue, suicidal_desire]).sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "display(test_subset['dv'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f768371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try to automate for all types \n",
    "\n",
    "# dfs = {}\n",
    "# for df_i, name in zip([train, test], ['train', 'test']):\t\n",
    "# \tdisplay(df_i['suicide_ladder_8'].value_counts() ) # this is a variable I created with CTL's suicide risk assessment ladder which helps make sure certain constructs are not present at lower levels. See train_test_split_ids.ipynb for function\n",
    "# \t# see get_true_risk_8 function in train_test_split.ipynb for interpretation\n",
    "# \tactive_rescue = df_i[df_i['suicide_ladder_8']==8] # immiment risk which could not be de-escaleted\n",
    "# \timminent_risk = df_i[df_i['suicide_ladder_8'].isin([6,7])] # has intent and capability and timeframe\n",
    "# \ttimeframe = imminent_risk[imminent_risk['timeframe']==1] # has intent and capability and timeframe\n",
    "# \tsuicidal = df_i[df_i['suicide_ladder_8'].isin([2])] # no imminent risk, no intent or capability; desire or other forms of suicide without any of those tags\n",
    "# \tsuicidal_desire = suicidal[suicidal['suicidal_desire']==1] # no imminent risk, no intent or capability; desire confirmed\n",
    "\t\n",
    "# \tif name =='train' and balance_training_set:\n",
    "# \t\tprint(1)\n",
    "# \t\tsmallest_group = active_rescue.shape[0]\n",
    "# \t\ttimeframe = timeframe.sample(n= smallest_group, random_state=random_seed)\n",
    "# \t\tsuicidal_desire = suicidal_desire.sample(n= smallest_group, random_state=random_seed)\n",
    "# \t\tdf_i_subset = pd.concat([active_rescue, timeframe, suicidal_desire]).sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "# \telse:\n",
    "# \t\tdf_i_subset = pd.concat([active_rescue, timeframe, suicidal_desire]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# \tdfs[name] = df_i_subset.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgjKFku9Dnl3",
   "metadata": {
    "id": "cgjKFku9Dnl3"
   },
   "source": [
    "## Construct Text Similarity feature extraction\n",
    "\n",
    "Constructs:\n",
    "\n",
    "1. Build constructs\n",
    "2. Encode constructs\n",
    "\n",
    "Documents:\n",
    "\n",
    "3. Tokenize documents\n",
    "4. Encode documents\n",
    "\n",
    "Similarity:\n",
    "\n",
    "5. Compute cosine similarity between constructs and docs and take maximum similarity per document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eafa7",
   "metadata": {},
   "source": [
    "### 1. Build constructs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build constructs\n",
    "'''\n",
    "construct_tokens_d = {\n",
    "\t'annoyance': ['annoyed', 'bothering me', 'annoying'],\n",
    "\t'anger': ['angry', 'rage'],\n",
    "\t'gratitude': ['grateful', 'thank you']}\n",
    "'''\n",
    "\n",
    "# I'm going to use prototypical tokens from the Suicide Risk Lexicon (the ones clinicians labelled as 3/3 on average)\n",
    "\n",
    "srl = dill.load(open(\"./../lexicon/data/input/lexicons/suicide_risk_lexicon_validated_prototypical_tokens_24-03-06T00-47-30.pickle\", \"rb\"))\n",
    "constructs_to_measure = srl_constructs.constructs_in_order\n",
    "\n",
    "# remove_constructs = [\n",
    "# \t\t\t\t'Bullying', # 'tell me to kill myself' matches with 'kill myself'\n",
    "# \t\t\t\t\t 'Social withdrawal',  #'want to be alone' matches with many loneliness comments\n",
    "# \t\t\t\t\t 'Suicide exposure' #'suicide aftermath' matches with 'suicide' etc\n",
    "# \t\t\t\t\t 'Agitated',# matches 'frustrated', 'stress'\n",
    "# \t\t\t\t\t 'Emotional pain & psychache', \n",
    "# \t\t\t\t\t 'Grief & bereavement', #'commited suicide' with 'suicide'\n",
    "# \t\t\t\t\t 'Perfectionism',\n",
    "# \t\t\t\t\t 'Discrimination',#\"treated with less respect\" matches with common phrase by therapists \"No one deserves to be treated that way\"\n",
    "# \t\t\t\t\t ]\n",
    "# constructs_to_measure = [x for x in constructs_to_measure if x not in remove_constructs]\n",
    "\n",
    "\n",
    "construct_tokens_d = {}\n",
    "for construct in constructs_to_measure:\n",
    "\ttokens = srl.constructs[construct]['tokens']                      \n",
    "\tconstruct_tokens_d[construct] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7828b2a",
   "metadata": {},
   "source": [
    "### 2. Encode constructs\n",
    "\n",
    "For faster processing, you can can randomly select up to 10-20 tokens from each construct. If a single construct has 50 tokens, it add a lot of time without much increase in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da887796",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessing:\n",
    "\n",
    "\tembeddings_dir = './../lexicon/data/input/lexicons/'\n",
    "\tprior_embeddings = dill.load(open(embeddings_dir+'embeddings_lexicon-tokens_all-MiniLM-L6-v2.pickle', \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "\ttokens_to_encode = []\n",
    "\t# single dictionary for all tokens, not split by construct\n",
    "\tconstruct_embeddings_d = {}\n",
    "\ttokens_to_encode = []\n",
    "\tfor construct in srl.constructs.keys():\n",
    "\t\ttokens = srl.constructs[construct]['tokens']                      \n",
    "\t\tfor token_i in tokens:\n",
    "\t\t\tif token_i in prior_embeddings.keys():\n",
    "\t\t\t\tembedding = prior_embeddings[token_i]\n",
    "\t\t\t\tconstruct_embeddings_d[token_i] = embedding\n",
    "\t\t\telse:\n",
    "\t\t\t\ttokens_to_encode.append(token_i)\n",
    "\n",
    "\tprint('tokens_to_encode',len(tokens_to_encode))\n",
    "\tif len(tokens_to_encode)>0:\n",
    "\t\tfrom sentence_transformers import SentenceTransformer\n",
    "\t\tembeddings_name = 'all-MiniLM-L6-v2'\n",
    "\t\tsentence_embedding_model = SentenceTransformer(embeddings_name)       # load embedding\n",
    "\t\tembeddings = sentence_embedding_model.encode(tokens_to_encode, convert_to_tensor=True,show_progress_bar=True)\t\n",
    "\t\tembeddings_d = dict(zip(tokens_to_encode, embeddings))\n",
    "\t\tconstruct_embeddings_d.update(embeddings_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xl8ZfP9kFpgN",
   "metadata": {
    "id": "xl8ZfP9kFpgN"
   },
   "source": [
    "### 3. Tokenize and clean docs into complete clauses (subject+predict, split if you find conjunction between complete clauses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already done in another script\n",
    "def clean_ctl_text(message_with_interaction):\n",
    "\tfrom concept_tracker.utils import clean\n",
    "\t# Fast: 1 sec every 10 000 messages\n",
    "\tdocs_clean = [str(n) if str(n)!='nan' else '' for n in message_with_interaction]\n",
    "\tdocs_clean = [n.replace('!.', '!').replace('?.', '?').replace('....', '...').replace('...', '... ') for n in docs_clean]\n",
    "\tmessage_with_interaction_clean = [clean.remove_multiple_spaces(doc) for doc in docs_clean]\n",
    "\treturn message_with_interaction_clean\n",
    "\n",
    "\n",
    "# This needs to be done after tokenizing\n",
    "def clean_ctl_conversation(docs):\n",
    "\n",
    "\t\tdocs_clean_clauses_clean = []\n",
    "\t\tfor doc in docs:\n",
    "\t\t\tclauses_doc_i = [] \n",
    "\t\t\tfor clause in doc:\n",
    "\t\t\t\tclauses_doc_i.extend(clause.split('\\n'))\n",
    "\t\t\tdoc_clean = [n.replace('texter : ','').replace('counselor : ','').replace('\\n','. ').strip('.,:\\n').replace(\" '\", \"'\").replace(' â€™', \"'\").replace(' ,', ',').strip(' ').replace('observer : ', '').replace(\" n't\", \"n't\").replace(\" ( 1/2 )\", \"\").replace('{ { URL } }', '').replace('[ scrubbed ]','').replace('  ', ' ') for n in clauses_doc_i]\n",
    "\t\t\tdoc_clean = [n for n in doc_clean if (n not in ['texter', 'counselor', 'observer', '', '-- UNREADABLE MESSAGE --', '( 2/2 )', '( 1/2 )']) and (len(n)>5)]\n",
    "\t\t\t\n",
    "\t\t\tdocs_clean_clauses_clean.append(doc_clean)\n",
    "\t\treturn docs_clean_clauses_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2jBlpg3UC_id",
   "metadata": {
    "id": "2jBlpg3UC_id"
   },
   "outputs": [],
   "source": [
    "if preprocessing:\n",
    "\t# 10 minutes for both train and test subsets\n",
    "\tdocs_clean = train_subset['message_with_interaction_clean'].values\n",
    "\tdocs_clean = [n.replace('\\n', '. ') for n in docs_clean] # help tokenize by clause, specially for CTL data\n",
    "\tdocs_clauses = spacy_tokenizer(docs_clean,language = 'en', model='en_core_web_sm',method = 'clause', lowercase=False,display_tree = False,remove_punct=False,clause_remove_conj = True)\n",
    "\tdocs_clauses = clean_ctl_conversation(docs_clauses)\n",
    "\ttrain_subset['message_with_interaction_clean_clauses'] = docs_clauses\n",
    "\ttrain_subset.to_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses.csv', index=False)\n",
    "\n",
    "\n",
    "\tdocs_clean = test_subset['message_with_interaction_clean'].values\n",
    "\tdocs_clean = [n.replace('\\n', '. ') for n in docs_clean] # help tokenize by clause, specially for CTL data\n",
    "\tdocs_clauses = spacy_tokenizer(docs_clean,language = 'en', model='en_core_web_sm',method = 'clause', lowercase=False,display_tree = False,remove_punct=False,clause_remove_conj = True)\n",
    "\tdocs_clauses = clean_ctl_conversation(docs_clauses)\n",
    "\ttest_subset['message_with_interaction_clean_clauses'] = docs_clauses\n",
    "\n",
    "\n",
    "\t\n",
    "\ttest_subset.to_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses.csv', index=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d233fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset= pd.read_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses.csv')\n",
    "\n",
    "test_subset = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_XDfOBo859NM",
   "metadata": {
    "id": "_XDfOBo859NM"
   },
   "source": [
    "### 4. Encode documents\n",
    "\n",
    "1 second per conversation if tokenized into clauses (one embedding per clause)\n",
    "\n",
    "1000 conversations in 15 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code, now I've sped it up with batch processing below\n",
    "\n",
    "# if preprocessing:\n",
    "# \tfrom sentence_transformers import SentenceTransformer\n",
    "# \tembeddings_name = 'all-MiniLM-L6-v2'\n",
    "# \tsentence_embedding_model = SentenceTransformer(embeddings_name)       # load embedding\n",
    "# \timport pickle\n",
    "\n",
    "# \t# already encoded\n",
    "\n",
    "# \twith open('./data/input/ctl/embeddings/'+f'embeddings_all-MiniLM-L6-v2_docs_clauses_with-interaction_24-03-07T04-25-04.pickle', 'rb') as handle:\n",
    "# \t\t\tdocs_embeddings_d = pickle.load(handle)\n",
    "# \timport tqdm \n",
    "# \tfor df_i_subset in [train_subset, test_subset]: # TODO add train_subset\n",
    "# \t\tdocs_to_encode = []\n",
    "# \t\tdocs_embeddings_d_subset = {}\n",
    "# \t\tfor conversation_id in df_i_subset['conversation_id'].tolist():\n",
    "# \t\t\tif conversation_id in docs_embeddings_d.keys():\n",
    "# \t\t\t\tembedding = docs_embeddings_d[conversation_id]\n",
    "# \t\t\t\tdocs_embeddings_d_subset[conversation_id] = embedding\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tdocs_to_encode.append(conversation_id)\n",
    "# \t\tprint(len(docs_to_encode))\n",
    "\n",
    "\t\n",
    "# \t\tif len(docs_to_encode)>0:\n",
    "# \t\t\tprint('encoding...')\n",
    "# \t\t\tfrom sentence_transformers import SentenceTransformer\n",
    "# \t\t\tembeddings_name = 'all-MiniLM-L6-v2'\n",
    "# \t\t\tsentence_embedding_model = SentenceTransformer(embeddings_name)       # load embedding\n",
    "# \t\t\tfor doc in tqdm.tqdm(docs_to_encode):\n",
    "# \t\t\t\tdoc_clauses = df_i_subset[df_i_subset['conversation_id'] == doc]['message_with_interaction_clean_clauses'].values[0]\n",
    "# \t\t\t\tdoc_clauses = eval(doc_clauses) # when reloading a DF where each cell is a list of lists\n",
    "# \t\t\t\tembeddings = sentence_embedding_model.encode(doc_clauses, convert_to_tensor=True,show_progress_bar=False)\t\n",
    "# \t\t\t\tdocs_embeddings_d[doc] = embeddings\n",
    "\n",
    "\n",
    "# \t# re save pickle\n",
    "# \timport dill\n",
    "# \twith open('./data/input/ctl/embeddings/'+'embeddings_all-MiniLM-L6-v2_docs_clauses_with-interaction_24-03-07T04-25-04.pickle', 'wb') as handle:\n",
    "# \t\tdill.dump(docs_embeddings_d, handle, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_toy = test_subset.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Encode clauses\n",
    "\n",
    "if preprocessing:\n",
    "\tfrom sentence_transformers import SentenceTransformer\n",
    "\timport pickle\n",
    "\timport tqdm\n",
    "\n",
    "\tembeddings_name = 'all-MiniLM-L6-v2'\n",
    "\tsentence_embedding_model = SentenceTransformer(embeddings_name)\n",
    "\n",
    "\t# Load existing embeddings\n",
    "\twith open('./data/input/ctl/embeddings/'+f'embeddings_all-MiniLM-L6-v2_docs_clauses_with-interaction_24-03-07T04-25-04.pickle', 'rb') as handle:\n",
    "\t    docs_embeddings_d = pickle.load(handle)\n",
    "\t\n",
    "\t# docs_embeddings_d = {} #Warning, here it is empty instead of loading\n",
    "\t\n",
    "\tfor df_i_subset in [train_subset, test_subset]:  # Add train_subset if needed\n",
    "\t\tdocs_to_encode = []\n",
    "\t\tdocs_embeddings_d_subset = {}\n",
    "\t\t\n",
    "\t\tfor conversation_id in df_i_subset['conversation_id'].tolist():\n",
    "\t\t\tif conversation_id in docs_embeddings_d:\n",
    "\t\t\t\tdocs_embeddings_d_subset[conversation_id] = docs_embeddings_d[conversation_id]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdocs_to_encode.append(conversation_id)\n",
    "\t\tprint(docs_to_encode)\n",
    "\t\tif docs_to_encode:\n",
    "\t\t\tprint('Encoding...')\n",
    "\t\t\t\n",
    "\t\t\tdoc_tokens_example = df_i_subset[df_i_subset['conversation_id'] == docs_to_encode[0]]['message_with_interaction_clean_clauses'].values[0]\n",
    "\t\t\tif isinstance(doc_tokens_example, str):\n",
    "\t\t\t\t# eval string to turn into list of lists (happens when this columns is saved and reloaded)\n",
    "\t\t\t\tclauses_to_encode = [eval(df_i_subset[df_i_subset['conversation_id'] == doc]['message_with_interaction_clean_clauses'].values[0]) \n",
    "\t\t\t\t\t\t\t\tfor doc in docs_to_encode]\n",
    "\t\t\telif isinstance(doc_tokens_example, list):\n",
    "\t\t\t\tclauses_to_encode = [df_i_subset[df_i_subset['conversation_id'] == doc]['message_with_interaction_clean_clauses'].values[0] \n",
    "\t\t\t\t\t\t\t\tfor doc in docs_to_encode]\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t# Flatten the list of lists and remember the split points\n",
    "\t\t\tflat_clauses, split_indices = [], [0]\n",
    "\t\t\tfor clauses in clauses_to_encode:\n",
    "\t\t\t\tsplit_indices.append(split_indices[-1] + len(clauses))\n",
    "\t\t\t\tflat_clauses.extend(clauses)\n",
    "\t\t\t\n",
    "\t\t\t# Process in batches\n",
    "\t\t\tbatch_size = 512  # Adjust based on your available memory and requirements\n",
    "\t\t\tall_embeddings = []\n",
    "\t\t\tfor i in tqdm.tqdm(range(0, len(flat_clauses), batch_size)):\n",
    "\t\t\t\tbatch_clauses = flat_clauses[i:i+batch_size]\n",
    "\t\t\t\tbatch_embeddings = sentence_embedding_model.encode(batch_clauses, convert_to_tensor=False, show_progress_bar=False)\n",
    "\t\t\t\tall_embeddings.extend(batch_embeddings)\n",
    "\t\t\t\n",
    "\t\t\t# Assign embeddings to respective documents\n",
    "\t\t\tfor i, doc_id in enumerate(docs_to_encode):\n",
    "\t\t\t\tstart, end = split_indices[i], split_indices[i+1]\n",
    "\t\t\t\tdocs_embeddings_d[doc_id] = np.array(all_embeddings[start:end])\n",
    "\n",
    "\t# Save updated embeddings\n",
    "\twith open('./data/input/ctl/embeddings/'+'embeddings_all-MiniLM-L6-v2_docs_clauses_with-interaction_24-03-07T04-25-04.pickle', 'wb') as handle:\n",
    "\t\tpickle.dump(docs_embeddings_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3afba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64EEfmEFyrDp",
   "metadata": {
    "id": "64EEfmEFyrDp"
   },
   "source": [
    "### 4. Compute cosine similarity between constructs and docs\n",
    "10 sec per 500 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d58973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f410fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessing:\n",
    "\n",
    "\t# subselect keys if in train_subset['conversation_id'].values\n",
    "\tdocs_embeddings_d_train = {}\n",
    "\tdocs_embeddings_d_test = {}\n",
    "\tfor k,v in list(docs_embeddings_d.items()):\n",
    "\t\tif k in train_subset['conversation_id'].values:\n",
    "\t\t\tdocs_embeddings_d_train[k] = v\n",
    "\t\tif k in test_subset['conversation_id'].values:\n",
    "\t\t\tdocs_embeddings_d_test[k] = v\n",
    "\n",
    "\t\n",
    "\t# X_train\n",
    "\t# ========================================================\n",
    "\t# we'll measure all the constructs in construct_tokens_d for the docs in docs\n",
    "\tprint('Extract features for training set')\n",
    "\tX_train, X_train_cosine_scores_per_doc = cts.measure(\n",
    "\t\t\t\tconstruct_tokens_d = construct_tokens_d,\n",
    "\t\t\t\tconstruct_embeddings_d = construct_embeddings_d,\n",
    "\t\t\t\tdocs_embeddings_d = docs_embeddings_d_train,\n",
    "\t\t\t\tmethod = 'lexicon_clause', \n",
    "\t\t\t\tsummary_stat = ['max'],\n",
    "\t\t\t\treturn_cosine_similarity=True,\n",
    "\t\t\t\tminmaxscaler = None,\n",
    "\t\t\t\tdoc_id_col_name = 'conversation_id',\n",
    "\t\t\t\tremove_stat_name_from_col_name = True\n",
    "\t\t\t)\n",
    "\n",
    "\ttrain_subset = train_subset.merge(X_train, on = 'conversation_id')\n",
    "\ttrain_subset.to_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv', index=False)\n",
    "\twith open('./data/input/ctl/embeddings/'+'X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_cosine_similarities.pickle', 'wb') as handle:\n",
    "\t\tdill.dump(X_train_cosine_scores_per_doc, handle, protocol=dill.HIGHEST_PROTOCOL)\n",
    "\n",
    "\t# X_test\n",
    "\t# ========================================================\n",
    "\tprint('Extract features for test set')\n",
    "\tX_test, X_test_cosine_scores_per_doc = cts.measure(\n",
    "\t\t\t\tconstruct_tokens_d = construct_tokens_d,\n",
    "\t\t\t\tconstruct_embeddings_d = construct_embeddings_d,\n",
    "\t\t\t\tdocs_embeddings_d = docs_embeddings_d_test,\n",
    "\t\t\t\tmethod = 'lexicon_clause', \n",
    "\t\t\t\tsummary_stat = ['max'],\n",
    "\t\t\t\treturn_cosine_similarity=True,\n",
    "\t\t\t\tminmaxscaler = None,\n",
    "\t\t\t\tdoc_id_col_name = 'conversation_id',\n",
    "\t\t\t\tremove_stat_name_from_col_name = True\n",
    "\t\t\t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "\ttest_subset = test_subset.merge(X_test, on = 'conversation_id')\n",
    "\t\n",
    "\ttest_subset.to_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv', index=False )\n",
    "\twith open('./data/input/ctl/embeddings/'+'X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes_cosine_similarities.pickle', 'wb') as handle:\n",
    "\t\tdill.dump(X_test_cosine_scores_per_doc, handle, protocol=dill.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666be4de",
   "metadata": {},
   "source": [
    "\n",
    "# Load preprocessed data and run some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22161767",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not preprocessing:\n",
    "\ttrain_subset = pd.read_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv')\n",
    "\ttest_subset = pd.read_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ceebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_tracker.feature_extraction import pronouns, verbs\n",
    "from importlib import reload\n",
    "reload(pronouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clauses = train_subset['message_with_interaction_clean_clauses'] .values\n",
    "doc_clauses = [eval(n) for n in doc_clauses]\n",
    "doc_proportions_df = verbs.extract_tenses_and_aspects(doc_clauses, list_of_clauses = True)\n",
    "\n",
    "texter_messages = [n.replace('\\n', '. ') for n in  train_subset['message_clean']]\n",
    "pronouns_df = pronouns.count_pronouns(texter_messages, normalize=True)\n",
    "assert train_subset.shape[0]== doc_proportions_df.shape[0] == pronouns_df.shape[0]\n",
    "\n",
    "train_subset = pd.concat([train_subset,doc_proportions_df, pronouns_df], axis=1)\n",
    "train_subset.to_csv('./data/input/ctl/X_train_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv', index=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clauses = test_subset['message_with_interaction_clean_clauses'] \n",
    "doc_clauses = [eval(n) for n in doc_clauses]\n",
    "doc_proportions_df = verbs.extract_tenses_and_aspects(doc_clauses, list_of_clauses = True)\n",
    "\n",
    "texter_messages = [n.replace('\\n', '. ') for n in  test_subset['message_clean']]\n",
    "pronouns_df = pronouns.count_pronouns(texter_messages, normalize=True)\n",
    "\n",
    "assert test_subset.shape[0]== doc_proportions_df.shape[0] == pronouns_df.shape[0]\n",
    "test_subset = pd.concat([test_subset,doc_proportions_df, pronouns_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01774da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset.to_csv('./data/input/ctl/X_test_all_with_interaction_desire_active_rescue_subset_tokenized_clauses_cts-prototypes.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5a30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mq6Nab8QD4EW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "mq6Nab8QD4EW",
    "outputId": "5caf71cd-af77-4337-9320-d667c5b85b67"
   },
   "outputs": [],
   "source": [
    "# The zero-shot feature is able to separate docs about the construct from docs\n",
    "# not about the construct often quite well, depending on how the construct was built\n",
    "construct = 'Active suicidal ideation & suicidal planning'\n",
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "\n",
    "train_subset['hover_text'] = train_subset['message_with_interaction_clean'].apply(lambda x: x.replace('\\n', '<br>'))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.box(train_subset, x='dv', y=construct, points=\"all\", hover_data = ['conversation_id', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'hover_text',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# 'message_with_interaction_clean'\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t])\n",
    "fig.update_layout(yaxis_range=[-0.1,1])\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J-TI2LYsROiR",
   "metadata": {
    "id": "J-TI2LYsROiR"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iItuLZfUJCKz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "iItuLZfUJCKz",
    "outputId": "7c9da691-0bdd-44c4-ec08-4675a55e1efd"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "construct = 'Active suicidal ideation & suicidal planning'\n",
    "\n",
    "df = test_subset.copy()\n",
    "\n",
    "y_similarity = df[construct].values\n",
    "\n",
    "y = df['dv'].replace({'active_rescue':1, 'suicidal_desire':0}).values\n",
    "roc_auc = metrics.roc_auc_score(y, y_similarity)\n",
    "print(f'roc auc {roc_auc:.2}')\n",
    "f1 = metrics.f1_score(y, y_similarity>0.5)\n",
    "print(f'f1 score {f1:.2}') # you can't use 0.5 as the threshold, the cosine similarity isn't calibrated, it depends on each model_name type\n",
    "\n",
    "# Get optimal threshold\n",
    "false_positive_rate_list, true_positive_rate_list, thresholds = metrics.roc_curve(y,y_similarity) # Important: other metrics take binary predictions y_pred. Here we test different thresholds, so we need probabilities (this will change the outputs)\n",
    "i_opt = np.argmax(np.array(true_positive_rate_list)-np.array(false_positive_rate_list))          # at which cutoff index are TPR and FPR maximal?\n",
    "print(i_opt)\n",
    "cut_opt = thresholds[i_opt]\n",
    "print(f'Optimal cutoff value: {cut_opt:.3f}')\n",
    "f1_opt = metrics.f1_score(y, y_similarity>cut_opt)\n",
    "print(f'f1 score with optimal cutoff {f1:.2}') # altough this is if we have access to a test set.\n",
    "\n",
    "ax = sns.kdeplot(data=df, x=construct, hue=y)\n",
    "# plt.title(f'ROC AUC = {roc_auc:.2f}\\nF1 = {f1:.2f}')\n",
    "plt.title(f'ROC AUC = {roc_auc:.2f}')\n",
    "\n",
    "# Add a vertical line for the optimal cut-off\n",
    "line = plt.axvline(x=cut_opt, color='r', linestyle='dashed', linewidth=1)\n",
    "label_line = f'Opt thresh {cut_opt:.2f}: F1 = {f1_opt:.2f}'\n",
    "labels = []\n",
    "# Add the custom legend entry for the axvline\n",
    "\n",
    "labels.append(f'active_rescue'.replace('_', ' ').capitalize())\n",
    "labels.append(f'suicidal_desire'.replace('_', ' ').capitalize())\n",
    "labels.append(label_line)\n",
    "\n",
    "# Redraw the legend with the combined handles and labels\n",
    "ax.legend(labels=labels)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8DQVD-CRKPs",
   "metadata": {
    "id": "r8DQVD-CRKPs"
   },
   "source": [
    "# Explainability: construct tokens vs. doc tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TNp2r9vHOWbc",
   "metadata": {
    "id": "TNp2r9vHOWbc"
   },
   "outputs": [],
   "source": [
    "def return_cosine_df(doc_id, construct, docs_clauses, construct_tokens_d, cosine_scores_per_doc):\n",
    "  doc_clauses_i = docs_clauses[doc_id]\n",
    "  construct_tokens_i = construct_tokens_d[construct]\n",
    "  df = pd.DataFrame(cosine_scores_per_doc[f'{doc_id}_{construct}'], index = construct_tokens_i, columns = doc_clauses_i)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DIPn83clB-Ky",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "DIPn83clB-Ky",
    "outputId": "2858b4fb-ed3d-425f-a67d-ebab10ca9082"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "construct = 'gratitude'\n",
    "doc_id = 65\n",
    "\n",
    "construct = 'anger'\n",
    "doc_id = 22\n",
    "\n",
    "construct = 'annoyance'\n",
    "doc_id = 64\n",
    "'''\n",
    "\n",
    "construct = 'Direct self-injury'\n",
    "doc_id = 0\n",
    "\n",
    "\n",
    "\n",
    "df = return_cosine_df(doc_id, construct, , construct_tokens_d,X_test_cosine_scores_per_doc)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TdLvzMvD1NOM",
   "metadata": {
    "id": "TdLvzMvD1NOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc716b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
