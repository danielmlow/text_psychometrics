{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.38.1\n",
    "# !pip install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "import sys\n",
    "\n",
    "# Set default values\n",
    "toy = False\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "with_interaction = True\n",
    "\n",
    "# Check for arguments and assign them if they exist\n",
    "if len(sys.argv) > 1:\n",
    "    toy = sys.argv[1]\n",
    "if len(sys.argv) > 2:\n",
    "    model_name = sys.argv[2]\n",
    "if len(sys.argv) > 3:\n",
    "    with_interaction = sys.argv[3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "location = 'openmind'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "\tConfusionMatrixDisplay,\n",
    "\tauc,\n",
    "\tconfusion_matrix,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    "\tprecision_score,\n",
    "\trecall_score,\n",
    "\troc_auc_score,\n",
    ")\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "import api_keys # local\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "if location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/datasets/'\n",
    "  output_dir = './data/output/ml_performance/'\n",
    "elif location =='local':\n",
    "  input_dir = '/Users/danielmlow/data/ctl/input/datasets/'\n",
    "  output_dir = '/home/dlow/datum/lexicon/data/output/ml_performance/'\n",
    "\n",
    "\n",
    "set_name = 'train10_test'\t\n",
    "filename = f'{set_name}_metadata_messages_clean.gzip'\n",
    "test = pd.read_parquet(input_dir + filename, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Function to add a file handler to the root logger\n",
    "def add_file_handler(log_filename):\n",
    "    # Create a file handler that logs even debug messages\n",
    "    fh = logging.FileHandler(log_filename, mode='a')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    # Create formatter and add it to the handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    # Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    # Add the handler to the root logger\n",
    "    logging.root.addHandler(fh)\n",
    "\n",
    "# Define your custom print function\n",
    "def custom_print(*args, **kwargs):\n",
    "    # Convert all arguments into a string. You might want to customize the separator.\n",
    "    message = ' '.join(str(arg) for arg in args)\n",
    "    # Log the message using logging\n",
    "    logging.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name_clean = model_name.replace('/', '-')\n",
    "ts = datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "output_dir_i = output_dir+f'{model_name_clean}_{ts}/' \n",
    "os.makedirs(output_dir_i , exist_ok=True)\n",
    "# this will get replaced at inference time creating one for each DV\n",
    "logging.basicConfig(filename=output_dir_i+f'log_print_statements_gpu_info.txt', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "print = custom_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Toy: {toy}, Model Name: {model_name}, With Interaction: {with_interaction}\")\n",
    "print('running:', input_dir+filename)\n",
    "print('location:', location)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_tags13 = ['self_harm',\n",
    " 'suicide',\n",
    " 'bully',\n",
    " 'abuse_physical',\n",
    " 'abuse_sexual',\n",
    " 'relationship',\n",
    " 'bereavement',\n",
    " 'isolated',\n",
    " 'anxiety',\n",
    " 'depressed',\n",
    " 'gender',\n",
    " 'eating',\n",
    " 'substance']\n",
    "\n",
    "\n",
    "# prompt_names = dict(zip(ctl_tags13, ['']*len(ctl_tags13)))\n",
    "prompt_names = {'self_harm': 'self harm or self injury',\n",
    " 'suicide': 'suicidal thoughts or suicidal behaviors',\n",
    " 'bully': 'bullying',\n",
    " 'abuse_physical': 'physical abuse',\n",
    " 'abuse_sexual': 'sexual abuse',\n",
    " 'relationship': 'relationship issues',\n",
    " 'bereavement': 'bereavement or grief',\n",
    " 'isolated': 'loneliness or social isolation',\n",
    " 'anxiety': 'anxiety',\n",
    " 'depressed': 'depression',\n",
    " 'gender': 'gender identity',\n",
    " 'eating': 'an eating disorder or body image issues',\n",
    " 'substance': 'substance use'}\n",
    "\n",
    "print('/n')\n",
    "print('prompt_names:', prompt_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=api_keys.huggingface)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device    \n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print number of GPUs available\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"\\tName: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"\\tCuda version: {print(torch.version.cuda)}\")\n",
    "        print(f\"\\tCompute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"\\tTotal Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        # Additional details can be accessed via `torch.cuda.get_device_properties(i)`\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation and if your hardware supports CUDA.\")\n",
    "    print('/n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_nvitop():\n",
    "\n",
    "    # Command to run 'nvitop' in one-shot mode\n",
    "    command = [\"python3\", \"-m\", \"nvitop\", \"-1\"]\n",
    "\n",
    "    # Run the command and capture its output\n",
    "\n",
    "    try: \n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(result)\n",
    "        print(result.stdout)\n",
    "    except: pass\n",
    "    return\n",
    "\n",
    "\n",
    "def create_binary_dataset(df_metadata, dv = 'suicide', n_per_dv = 3000):\n",
    "    df_metadata_tag_1 = df_metadata[df_metadata[dv]==1].sample(n=n_per_dv,random_state=123)\n",
    "    df_metadata_tag_0 = df_metadata[df_metadata[dv]==0].sample(n=n_per_dv,random_state=123)\n",
    "    assert df_metadata_tag_1.shape[0] == n_per_dv\n",
    "    assert df_metadata_tag_0.shape[0] == n_per_dv\n",
    "\n",
    "    df_metadata_tag = pd.concat([df_metadata_tag_1, df_metadata_tag_0]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df_metadata_tag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_json_in_string(string: str) -> str:\n",
    "    \"\"\"Finds the JSON object in a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to search for a JSON object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    json_string : str\n",
    "    \"\"\"\n",
    "    start = string.find(\"{\")\n",
    "    end = string.rfind(\"}\")\n",
    "    if start != -1 and end != -1:\n",
    "        json_string = string[start : end + 1]\n",
    "    else:\n",
    "        json_string = \"{}\"\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cm(y_true, y_pred, output_dir, model_name, ts, classes = [\"SITB-\", \"SITB+\"], save=True):\n",
    "\tcm = confusion_matrix(y_true, y_pred, normalize=None)\n",
    "\tcm_df = pd.DataFrame(cm, index=classes , columns=classes )\n",
    "\tcm_df_meaning = pd.DataFrame([[\"TN\", \"FP\"], [\"FN\", \"TP\"]], index=classes , columns=classes )\n",
    "\n",
    "\tcm_norm = confusion_matrix(y_true, y_pred, normalize=\"all\")\n",
    "\tcm_norm = (cm_norm * 100).round(2)\n",
    "\tcm_df_norm = pd.DataFrame(cm_norm, index=classes , columns=classes )\n",
    "\n",
    "\t\n",
    "\tplt.rcParams[\"figure.figsize\"] = [4, 4]\n",
    "\tConfusionMatrixDisplay(cm_norm, display_labels=classes ).plot()\n",
    "\tplt.tight_layout()\n",
    "\t\n",
    "\tif save:\n",
    "\t\tplt.savefig(output_dir + f\"cm_{model_name}_{ts}.png\", dpi = 300)\n",
    "\t\tcm_df_meaning.to_csv(output_dir + f\"cm_meaning_{model_name}_{ts}.csv\")\n",
    "\t\tcm_df.to_csv(output_dir + f\"cm_{model_name}_{ts}.csv\")\n",
    "\t\tcm_df_norm.to_csv(output_dir + f\"cm_norm_{model_name}_{ts}.csv\")\n",
    "\n",
    "\treturn cm_df_meaning, cm_df, cm_df_norm\n",
    "\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, y_pred_proba_1, output_dir,gridsearch=None,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=None,feature_vector=None,model_name=None,round_to = 2, ts = None, save_results=False):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    np.set_printoptions(suppress=True)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # calculate precision and recall for each threshold\n",
    "    lr_precision, lr_recall, thresholds = precision_recall_curve(y_true, y_pred_proba_1)\n",
    "\n",
    "    # TODO: add best threshold\n",
    "    fscore = (2 * lr_precision * lr_recall) / (lr_precision + lr_recall)\n",
    "    fscore[np.isnan(fscore)] = 0\n",
    "    ix = np.argmax(fscore)\n",
    "    best_threshold = thresholds[ix].item()\n",
    "\n",
    "    pr_auc = auc(lr_recall, lr_precision)\n",
    "    # AU P-R curve is also approximated by avg. precision\n",
    "    # avg_pr = metrics.average_precision_score(y_true,y_pred_proba_1)\n",
    "\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)  # OR: recall_score(y_true,y_pred, pos_label=0)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        [feature_vector,model_name, sensitivity, specificity, precision, f1, roc_auc, pr_auc, best_threshold, gridsearch, best_params],\n",
    "        index=[\"Feature vector\",\"Model\", \"Sensitivity\", \"Specificity\", \"Precision\", \"F1\", \"ROC AUC\", \"PR AUC\", \"Best th PR AUC\", \"Gridsearch\", \"Best parameters\"],\n",
    "    ).T.round(2)\n",
    "    if save_results:\n",
    "        results.to_csv(output_dir + f\"results_{model_name}_{ts}.csv\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def obtain_json(responses):\n",
    "\n",
    "    jsons = []\n",
    "\n",
    "    added = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        try:\n",
    "            response_eval = eval(response)\n",
    "            if type(response_eval) == dict:\n",
    "                jsons.append(response_eval)\n",
    "                added.append(i)\n",
    "            elif type(response_eval) == set:\n",
    "                jsons.append(list(response_eval))\n",
    "                added.append(i)\n",
    "\n",
    "        except:\n",
    "            matches = re.findall(r'\\{.*?\\}', response)\n",
    "\n",
    "            # Assuming there's at least one match and it's safe to evaluate\n",
    "            if matches != []:\n",
    "                # Convert the first match to dictionary\n",
    "                try: \n",
    "                    dictionary = eval(matches[0])\n",
    "                    jsons.append(dictionary)\n",
    "                    added.append(i)\n",
    "                except:\n",
    "                    jsons.append(response)\n",
    "                    added.append(i)\n",
    "\n",
    "            else:\n",
    "                jsons.append(response)\n",
    "                added.append(i)\n",
    "    \n",
    "        if i not in added:\n",
    "            jsons.append(response)\n",
    "        \n",
    "    not_added = list(set(range(len(responses)))- set(added))\n",
    "    if len(not_added)>1:\n",
    "        print('WARNING: indexes not added, fix:', not_added)\n",
    "    print('/n')\n",
    "    return jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model (download if not in cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase model from session\n",
    "# try: del tokenizer\n",
    "# except: pass\n",
    "# torch.cuda.empty_cache()\n",
    "print('/n')\n",
    "run_nvitop()\n",
    "print('loading model...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See how they use it for text classification: (from probs or output layer directly?)\n",
    "# https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "if 'gemma' in model_name:\n",
    "\t# Gemma\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "elif 'llama' in model_name:\n",
    "\t# Have to restart session after updating transformers\n",
    "\tfrom transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\tmodel = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "elif \"paulml/OGNO-7B\" in model_name:\n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "elif \"microsoft/phi-2\" in model_name:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,  trust_remote_code=True,torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "print('/n')\n",
    "print('model loaded')    \n",
    "run_nvitop()\n",
    "# !nvidia-sim\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# # alternative\n",
    "# # Llama\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# sequences = pipeline(\n",
    "#     prompt,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=200,\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sense of token length vs word length\n",
    "# ========================================================\n",
    "run_this =False\n",
    "\n",
    "if run_this:\n",
    "\n",
    "\t# for dv in ctl_tags13:\n",
    "\tdv = 'eating'\n",
    "\tdf_i = create_binary_dataset(test, dv = dv, n_per_dv = 300)\n",
    "\tdf_i[ctl_tags13].sum()\n",
    "\n",
    "\n",
    "\tdocuments = df_i['message_with_interaction_clean'].values\n",
    "\ty_test = df_i[dv].values\n",
    "\tconstruct = prompt_names.get(dv)\n",
    "\tprint(construct)\n",
    "\tprint(len(documents))\n",
    "\tprint(len(y_test))\n",
    "\n",
    "\tlen_tokens = []\n",
    "\tword_counts = []\n",
    "\n",
    "\tfor text in documents:\n",
    "\t\t# Tokenize text\n",
    "\t\ttokens = tokenizer.tokenize(text)\n",
    "\n",
    "\t\t# Number of tokens\n",
    "\t\tlen_tokens.append(len(tokens))\n",
    "\t\tword_counts.append(len(text.split(' ')))\n",
    "\t\t\n",
    "\tplt.hist(len_tokens, bins = 100, alpha =0.3)\n",
    "\tplt.hist(word_counts, bins = 100, alpha =0.3, label = 'word counts')\n",
    "\tplt.legend()\n",
    "\tprint(np.mean(len_tokens), np.mean(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# batch tokenization was messing up my memory usage even though it doesnt use much memory.\n",
    "# ================================================================================================\n",
    "# max_length = 1012\n",
    "# prompts = [prompt.format(context = 'Crisis Text Line service', document = document, construct = construct) for document, construct in zip(documents, constructs)] \n",
    "# input_ids_all = tokenizer(prompts, padding=True, truncation=True,max_length = max_length, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# outputs = model.generate(input_ids_all, max_length=max_length)\n",
    "# # Decode generated sequences\n",
    "# prompt_length = input_ids_all.shape[1]\n",
    "# # # Decode only the generated part, skipping the prompt\n",
    "# generated_texts = [tokenizer.decode(output_sequence[prompt_length:], skip_special_tokens=True) for output_sequence in outputs]\n",
    "# end = time.time()\n",
    "# time_elapsed = end - start\n",
    "# print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_position = 0\n",
    "\n",
    "prompts ={ \n",
    "\t'google/gemma-2b-it': \n",
    "\"\"\"\n",
    "You are a conversation classification assistant. Classify the following {context} conversation:\n",
    "\n",
    "Here is the {context} conversation (ends with ```):\n",
    "```\n",
    "{document}\n",
    "```\n",
    "\n",
    "Assign probabilities for following labels and return using this JSON format (do not provide additional notes, explanations, or warnings). Provide your best guess, only return JSON (both probabilities should sum to 1):\n",
    "\n",
    "JSON:\n",
    "{{'texter mentions something related to {construct}': <your_probability>, 'texter does not mention anything related to {construct}': <your_probability>}}\n",
    "\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "prompt = prompts.get(model_name)\n",
    "print('/n')\n",
    "print('prompt:\\n', prompt, '\\n')\n",
    "print('/n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_tags13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if toy:\n",
    "    n_per_dv = 30\n",
    "else:\n",
    "    n_per_dv = 300\n",
    "\n",
    "\n",
    "if with_interaction:\n",
    "    max_length = int(1750*1.4)+75 #word count * 1.4 +75 for the prompt ~ tokens, 98%have less than this\n",
    "else:\n",
    "    # just texter     \n",
    "    max_length = int(1000*1.4)+75\n",
    "    \n",
    "# documents = ['No one cares about me']\n",
    "\n",
    "\n",
    "# Accessing tokenized ids\n",
    "for dv in ctl_tags13:\n",
    "    output_dir_i_dv = output_dir_i+f'{dv}/'\n",
    "    os.makedirs(output_dir_i_dv, exist_ok = True)\n",
    "        \n",
    "\n",
    "    responses = []\n",
    "    time_elapsed_all = []\n",
    "    add_file_handler(output_dir_i_dv+f'log_print_statements_{dv}.txt')\n",
    "#     print = custom_print\n",
    "    \n",
    "    construct = prompt_names.get(dv)\n",
    "    # Configure logging\n",
    "\n",
    "    df_i = create_binary_dataset(test, dv = dv, n_per_dv = n_per_dv)\n",
    "    \n",
    "    if with_interaction:\n",
    "        documents = df_i['message_with_interaction_clean'].values\n",
    "    else:\n",
    "        documents = df_i['message_clean'].values\n",
    "    y_test = df_i[dv].values\n",
    "    print('\\n', dv, '============================================')\n",
    "    print(df_i[ctl_tags13].sum())\n",
    "    print('construct:', construct)\n",
    "    print('len of documents:',len(documents))\n",
    "    print('len of y_test:',len(y_test))\n",
    "\n",
    "    for document, y_test_i in tqdm(zip(documents, y_test)):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        prompt_i = prompt.format(context = 'Crisis Text Line service', document = document, construct = construct)\n",
    "        # print(prompt_i)\n",
    "\n",
    "\n",
    "        if 'gemma' in model_name:\n",
    "            # Gemma\n",
    "            input_ids = tokenizer(prompt_i,truncation=True,max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(**input_ids, max_new_tokens = 1000)\n",
    "            tokenizer.decode(outputs[0])\n",
    "            # Find the length of the input_ids to know where the original prompt ends\n",
    "            prompt_length = input_ids[\"input_ids\"].shape[1]\n",
    "            # Decode only the generated part, skipping the prompt\n",
    "            response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        elif 'llama' in model_name:\n",
    "            inputs = tokenizer(prompt_i, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate\n",
    "            generate_ids = model.generate(inputs.input_ids, max_length=max_length)\n",
    "            response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    #     elif \"paulml/OGNO-7B\" in model_name:\n",
    "    #         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #         prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    #         outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    #         response = outputs[0][\"generated_text\"] \n",
    "    #     elif 'microsoft/phi-2' in model_name:     \n",
    "    #         inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "    #         outputs = model.generate(**inputs, max_length=200)\n",
    "    #         response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        responses.append(response)\n",
    "\n",
    "        print('y_test_i', y_test_i, '=======')\n",
    "        print(response)\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        time_elapsed = end - start\n",
    "        # print(time_elapsed)\n",
    "        time_elapsed_all.append(time_elapsed)\n",
    "\n",
    "    # clean\n",
    "    jsons = obtain_json(responses)\n",
    "\n",
    "\n",
    "    # TODO: you need to randomly assign 0.2 or 0.8 if you can't parse a response\n",
    "    could_not_parse = []\n",
    "    json_responses_clean = []\n",
    "    for i, response in enumerate(jsons):\n",
    "        if type(response) == dict:\n",
    "            response_values = response.values()\n",
    "            json_responses_clean.append(list(response_values))\n",
    "            could_not_parse.append(0)\n",
    "        elif type(response) == list:\n",
    "            json_responses_clean.append(response)\n",
    "            could_not_parse.append(0)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            random_float_0 = random.uniform(0.51, 0.99)\n",
    "            random_float_1 = 1 - random_float_0\n",
    "            selected_list = random.choice([\n",
    "                [random_float_0,random_float_1],\n",
    "                [random_float_1,random_float_0],\n",
    "                ])\n",
    "            print('\\n\\n=======', i, 'could not parse, randomly assiging a value:', response)\n",
    "            json_responses_clean.append(selected_list)\n",
    "            could_not_parse.append(1)\n",
    "\n",
    "\n",
    "    if construct_position == 0: \n",
    "        labels_order = [f\"{dv}\", f\"Other\"]\n",
    "    else:\n",
    "        labels_order = [f\"Other\",f\"{dv}\"]\n",
    "    y_pred_df = pd.DataFrame(json_responses_clean, columns = labels_order)\n",
    "    y_pred_df['could_not_parse'] = could_not_parse\n",
    "    y_pred_df['jsons'] = jsons\n",
    "    y_pred_df['time_elapsed'] = time_elapsed_all\n",
    "\n",
    "    y_pred_proba_1 = [n[construct_position] for n in json_responses_clean] # 1 value is construct\n",
    "    y_pred_proba_1\n",
    "    y_pred = np.array([n>=0.5 for n in y_pred_proba_1])*1  # 1 if construct >=0.5 independent of order in json_responses_clean\n",
    "\n",
    "    y_pred_df['y_pred'] = y_pred\n",
    "    y_pred_df['y_test'] = y_test\n",
    "    \n",
    "    y_pred_df.to_csv(output_dir_i_dv+f'y_proba_{dv}.csv')\n",
    "\n",
    "    # here we don't change label orders because y_pred and y_test are well defined (1 if construct >=0.5)     \n",
    "    dv_clean = dv.replace('_', ' ').capitalize()\n",
    "    cm_df_meaning, cm_df, cm_df_norm = cm(y_test, y_pred, output_dir_i_dv, model_name_clean, ts, classes = [f\"Other\",f\"{dv_clean}\"], save=True)\n",
    "    cr = classification_report(y_test, y_pred,output_dict=True)\n",
    "    cr = pd.DataFrame(cr)\n",
    "\n",
    "    cr.to_csv(output_dir_i_dv+f'cr_{dv}_{ts}.csv')\n",
    "\n",
    "    results = custom_classification_report(y_test, y_pred, y_pred_proba_1, output_dir_i_dv,gridsearch=None,\n",
    "        best_params=None,feature_vector=None,model_name=model_name_clean+f'_{dv}',round_to = 2, ts =ts, save_results=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
