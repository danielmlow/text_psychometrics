{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad9613-67ce-40a3-a207-595bd98e1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b161c23-59e0-46d3-8d08-02a0d1d64cbd",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. Preprocess document (and construct)\n",
    "- Tokenize into clauses\n",
    "- Tokenize into words, remove stop words, lemmatize : useful for lexicon count as well\n",
    "\n",
    "2. Encode construct:\n",
    "- construct prototype: I want to die\n",
    "- each token: I wish I didn't wake up tomorrow\n",
    "- weighted centroid\n",
    "\n",
    "3. Encode doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c966a-d2ae-4ff1-a31f-10e38bfe000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "# catpro\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, './../catpro')\n",
    "\n",
    "from catpro.text.utils import stop_words\n",
    "from catpro.text.utils import clean\n",
    "from catpro.text.utils.tokenizer import spacy_tokenizer\n",
    "from catpro.text.utils.lemmatizer import spacy_lemmatizer\n",
    "\n",
    "from catpro.text.embeddings import vectorize, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d205d-495e-4fae-84f5-73f02b71b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ts = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa30977-c7a5-42ab-8277-15c70c72adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "output_dir = './data/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897a002-9237-4f2a-99d3-7cdd19066d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a00402-e048-43de-acfc-ebfc92c9a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee638d5-29ce-4f34-a023-c1f8634494e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encode docs and lexicon (skip if done with this specific lexicon and dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25147204-e754-4a9b-9cfc-b9bb491ec556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir+'BurstingStudy_DailyData_02_15_2021_clean_preprocessed.csv', index_col = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc66f2-4e1a-46de-b538-a2774aff1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8444f01-8a61-494d-bd2f-e784c82bb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(input_dir+'train10_train_concurrent_metadata_100perconstruct_with_messages.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cdf4e-a3af-4f3c-b4ab-93aba9b20608",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['SI_DescribeText_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50298f4c-75a3-4ab3-bcfe-434064a7d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_embedding_names = [  # lexicon centroid weighted by distance to construct label\n",
    "   # 'w_w_glove', \n",
    "    # 'w_c_psychbert',\n",
    "    # 'w_w_minilm', \n",
    "    'w_c_minilm',           \n",
    "    # 'wl_w_minilm',\n",
    "    # 'wl_c_minilm'\n",
    "]\n",
    "    \n",
    "\n",
    "embedding_name_type = {\n",
    "#    model_name:embedding_type \n",
    "    # 'glove': 'word',\n",
    "    'all-MiniLM-L6-v2': 'sentence',\n",
    "    # 'all-MiniLM-L6-v2': 'document',\n",
    "    # 'mnaylor/psychbert-cased': 'document',# need to fix  \n",
    "}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e07ae-92d1-47da-bce8-517b66fe6a5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e8238-0ca1-4ea9-9b5e-5925c8b91ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_similarity_token_category(embeddings_tokens_doc, constructs_d, df, docs_clean, summary_stats = None):\n",
    "    '''\n",
    "    embeddings_tokens_doc\n",
    "    '''\n",
    "    # for each doc, it creates a value (e.g., mean across tokens --either words or clauses) for each construct    \n",
    "    \n",
    "    feature_vectors_mean = []\n",
    "    feature_vectors_median = []\n",
    "    feature_vectors_max = []\n",
    "    \n",
    "    constructs = list(constructs_d.keys())\n",
    "\n",
    "    for i, doc in enumerate(docs_clean):\n",
    "        embeddings_tokens_doc_i = embeddings_tokens_doc[i]\n",
    "        df_scores_category_all = pd.DataFrame(docs_clean[i], columns = ['token'])\n",
    "        for category in constructs:\n",
    "            embedding_category = constructs_d.get(category)\n",
    "            embedding_category = np.array(embedding_category, dtype=float)\n",
    "            embeddings_tokens_doc_i = np.array(embeddings_tokens_doc_i, dtype=float)\n",
    "            if embeddings_tokens_doc_i.shape[0] == 0: #happens when there is an empty str\n",
    "                embeddings_tokens_doc_i = [np.zeros(embedding_category.shape[0])]\n",
    "            cosine_scores = cosine_similarity(embedding_category, embeddings_tokens_doc_i)\n",
    "            # each token is a row, and each col is a construct being measured for that token.             \n",
    "            df_scores_category_all[category] = np.array(cosine_scores, dtype = float)[0]#pd.DataFrame(cosine_scores, columns = ['category'])\n",
    "            \n",
    "            \n",
    "            # df_scores_category = pd.DataFrame([docs_clean[i], np.array(cosine_scores[0])]).T\n",
    "            # df_scores_category.columns = ['token', category]\n",
    "            # df_scores_category = df_scores_category.sort_values(by='token')\n",
    "            # # df_scores_category_all= df_scores_category_all.merge(df_scores_category, on='token', how = 'outer')\n",
    "            # df_scores_category_all.append(df_scores_category)\n",
    "        # df_scores_category_all = pd.concat(df_scores_category_all, axis=1)\n",
    "        df_scores_category_all = df_scores_category_all[constructs].astype(float)\n",
    "\n",
    "        # display(df_scores_category_all)\n",
    "        feature_vectors_mean.append(df_scores_category_all.mean())\n",
    "        feature_vectors_median.append(df_scores_category_all.median())\n",
    "        feature_vectors_max.append(df_scores_category_all.max())\n",
    "\n",
    "    feature_vectors_mean = pd.concat(feature_vectors_mean,axis=1).T\n",
    "    feature_vectors_mean.columns = [n+'_mean' for n in feature_vectors_mean.columns]\n",
    "\n",
    "    feature_vectors_median = pd.concat(feature_vectors_median,axis=1).T\n",
    "    feature_vectors_median.columns = [n+'_median' for n in feature_vectors_median.columns]\n",
    "    \n",
    "    feature_vectors_max = pd.concat(feature_vectors_max,axis=1).T\n",
    "    feature_vectors_max.columns = [n+'_max' for n in feature_vectors_max.columns]\n",
    "    \n",
    "    feature_vectors = pd.concat([feature_vectors_mean, feature_vectors_median, feature_vectors_max],axis=1)\n",
    "    df[feature_vectors.columns.tolist()] = feature_vectors.values\n",
    "    # feature_cols = list(set(feature_vectors.columns)-set(['subreddit','author','date','docs','docs_clean']))\n",
    "    # feature_cols.sort()\n",
    "    # feature_vectors= feature_vectors[['subreddit','author','date','docs','docs_clean']+feature_cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22c57d-6a74-4cdb-bc71-8b7315aa9bb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4def0-cdef-454d-a2c1-d569c7c6e12b",
   "metadata": {},
   "source": [
    "### TODO: remove words with negation 1-3 words prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa8b73-62cc-4577-95dd-031449a17bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_this = True\n",
    "\n",
    "if run_this:\n",
    "    # docs = [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", n) for n in docs] #replace text within parentheses/brackets and parentheses/brackets\n",
    "    # docs = [n.replace('//', '').replace(' .', '.').replace(' ,', ',') for n in docs] \n",
    "    # docs = [n.replace('ampx200b', '').replace('\\n','').replace('\\xa0', '') for n in docs]\n",
    "    docs_clean = [str(n) if str(n)!='nan' else '' for n in docs]\n",
    "    docs_clean = [n.replace('!.', '!').replace('?.', '?').replace('....', '...') for n in docs_clean]\n",
    "    docs_clean = [clean.remove_multiple_spaces(doc) for doc in docs_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028551c2-8853-4fda-b8d8-f358ca98a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_stop_words = ['ca', 'nt','like', \"'\", \"´\", \"n’t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4687139-d7fd-4ffa-9de7-baf87aa2d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_this = False \n",
    "\n",
    "if run_this:\n",
    "    # words: tokenize by words, remove stop words and lemmatize for word-word similarity\n",
    "    docs_clean_w_w = stop_words.remove(list(docs_clean), language = 'en', sws = 'nltk', remove_punct=True, extend_stopwords=more_stop_words)\n",
    "    docs_clean_w_w = [clean.remove_multiple_spaces(doc) for doc in docs_clean_w_w]\n",
    "    docs_clean_w_w = spacy_lemmatizer(docs_clean_w_w, language ='en') #this takes 22s for 5200 docs\n",
    "    df['docs_clean_w_w'] = docs_clean_w_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfe266-1d8a-4c52-afb5-6d80b97a1ff8",
   "metadata": {},
   "source": [
    "### Tokenize into clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37543778-0d30-47c7-b781-a194ba0cbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867604e3-9d18-4026-a5f6-6a4929f30808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#1 min for 1300 counesling sessions. 3s for 2300 sentences.  \n",
    "\n",
    "\n",
    "\n",
    "run_this = True\n",
    "\n",
    "import re\n",
    "\n",
    "if run_this:\n",
    "    # docs_clean_clauses = [clean.remove_multiple_spaces(doc) for doc in docs_clean]\n",
    "    docs_clean_clauses = spacy_tokenizer(docs_clean, \n",
    "                                     language = 'en', model='en_core_web_sm', \n",
    "                                     token = 'clause', # clause tokenization\n",
    "                                     lowercase=False, \n",
    "                                     display_tree = False, \n",
    "                                     remove_punct=True, \n",
    "                                     clause_remove_conj = True)\n",
    "    df['docs_clean_clauses'] = docs_clean_clauses\n",
    "    df.to_csv(input_dir+f'BurstingStudy_DailyData_02_15_2021_clean_preprocessed_tokenized_{ts}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a93035-90c5-4ec9-a593-bee98967af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c39db-389d-4dde-a99a-92253a437b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f97f9-e6e5-48bc-a7cc-6644e762c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['docs_clean_clauses','docs_clean_w_w']].iloc[::10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212529bd-ce7f-483a-9b13-a1db02e686eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c9517-50fe-4bba-8d8c-f472ef408d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359620d-96cc-488e-8a25-b6d7b18583ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[n,i] for n,i in zip(docs, docs_clean_w_c)][::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2895e3e-dcfe-4fd6-a2ec-a5f46dbba453",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encode embeddings and compute similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ddbc2-32c0-421b-b0f9-401af18b2ca9",
   "metadata": {},
   "source": [
    "### Construct (Lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d1cb1-6db9-4d18-8e37-2864c20d99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catpro_text_dir = './../catpro/catpro/text/data/'\n",
    "lexicon_df = pd.read_csv(catpro_text_dir+'lexicons/suicidal_thoughts_and_behaviors/OsirisRankinFirstPassForDanLowMarch_3_ 2023_daniel_added_prototypes.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ead9f-c169-4073-a705-2372390d3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4a9cf-9901-470c-8722-725f48f2d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17122279-6915-4688-b8ad-0400cff058d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructs = [n for n in lexicon_df.columns if ('_add' not in n and '_remove' not in n and '_examples' not in n)]\n",
    "lexicon = {}\n",
    "\n",
    "\n",
    "for construct in constructs:\n",
    "    construct_cols = [n for n in lexicon_df.columns if construct in n]\n",
    "    lexicon_df_i = lexicon_df[construct_cols]\n",
    "    seed_tokens = lexicon_df_i[lexicon_df_i[construct+'_remove'].isin(['seed_token', 'construct label', 'prototype'])][construct].values\n",
    "    for token in ['active_si', 'passive_si', 'thwarted belongingness']:\n",
    "        try: seed_tokens.remove(token)\n",
    "        except: pass\n",
    "\n",
    "    if len(seed_tokens)==0:\n",
    "        print(construct, 'has 0 seed_tokens')\n",
    "        \n",
    "    other_tokens = lexicon_df_i[lexicon_df_i[construct+'_remove'].astype(str).isin(['nan', '0'])][construct].values\n",
    "    other_tokens = [n for n in other_tokens if str(n)!='nan']                                                      \n",
    "    if len(other_tokens)==0:\n",
    "        print(construct, 'has 0 other_tokens')\n",
    "        \n",
    "    lexicon[construct] = {\n",
    "        'seed_tokens':seed_tokens,\n",
    "        'other_tokens':other_tokens,\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     if '_add' not in n or '_remove' not in n or '_examples' not in n   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078990fa-51c6-42ad-9034-f5a0d06cf9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load risk factor and encode\n",
    "\n",
    "# input_dir_catpro = './../data/'\n",
    "# # lexicon = pd.read_csv(input_dir_catpro+'lexicons/suicidal_thoughts_and_behaviors/suicide_risk_lexicon_thesauri_questionnaires_23-03-16T19-35-06.csv', index_col = 0)\n",
    "# import json\n",
    "\n",
    "# with open(input_dir_catpro+'lexicons/suicidal_thoughts_and_behaviors/suicide_risk_lexicon_thesauri_questionnaires_23-03-16T19-35-06.json') as f:\n",
    "#     lexicon = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bce74-d0fc-4a02-8205-51eafeffbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dd4c6-deca-48f6-ad2a-691cb3587f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./../data/lexicons/suicidal_thoughts_and_behaviors/suicide_risk_lexicon_thesauri_questionnaires_23-03-16T19-35-06.csv', index_col = 0)\n",
    "\n",
    "# print(len(df.columns.tolist()))\n",
    "# print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92986703-025c-4af2-b753-d7cf0462808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructs_to_measure = ['abuse_physical',\n",
    " 'abuse_sexual',\n",
    " 'active_si',\n",
    " 'aggression_irritability',\n",
    " 'agitation',\n",
    " # 'alcohol_use',\n",
    " # 'anhedonia_uninterested',\n",
    " 'anxiety',\n",
    " # 'barriers_to_treatment',\n",
    " # 'bully',\n",
    " 'burdensomeness',\n",
    " 'defeat_failure',\n",
    " 'depressed_mood',\n",
    " # 'desire_to_escape',\n",
    " # 'discrimination',\n",
    " # 'eating_disorder',\n",
    " 'emotional_pain',\n",
    " 'emptiness',\n",
    " 'entrapment',\n",
    " 'fatigue_tired',\n",
    " 'finances_work',\n",
    " 'gender_sexual_identity',\n",
    " 'grief_bereavement',\n",
    " 'guilt',\n",
    " 'hopelessness',\n",
    " 'impulsivity',\n",
    " 'loneliness_isolated',\n",
    " # 'panic',\n",
    " 'passive_si',\n",
    " # 'perfectionism',\n",
    " 'relationships',\n",
    " 'rumination',\n",
    " 'self-injury',\n",
    " 'shame_self-disgust',\n",
    " 'sleep_issues',\n",
    " # 'social_withdrawl',\n",
    " 'substance_use',\n",
    " # 'thwarted_belongingness'\n",
    "                        ]\n",
    "\n",
    "lexicon_final = {}\n",
    "for c in constructs_to_measure:\n",
    "    lexicon_final[c] = lexicon.get(c)\n",
    "    \n",
    "lexicon = lexicon_final.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a03ef-ba93-4e46-b033-ce520a6730b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in lexicon.keys():\n",
    "    print(c,lexicon.get(c).get('seed_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98588ca-b881-45db-80a6-7acbe93a3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: merge active_si and passive_si, change name of self-injury\n",
    "\n",
    "\n",
    "# constructs_to_measure = ['self-injury',\n",
    "#  'active_si',\n",
    "#  'passive_si',\n",
    "#  'bully',\n",
    "#  'abuse_physical',\n",
    "#  'abuse_sexual',\n",
    "#  'relationships',\n",
    "#  'grief_bereavement',\n",
    "#  'loneliness_isolated',\n",
    "#  'anxiety',\n",
    "#  'depressed_mood',\n",
    "#  'gender_sexual_identity',\n",
    "#  'eating_disorder',\n",
    "#  'substance_use']\n",
    "\n",
    "\n",
    "\n",
    "# # values = ['']*len(constructs_to_measure)\n",
    "# # ctl_tags = dict(zip(constructs_to_measure, values))\n",
    "# ctl_tags_d = {'self-injury': 'self_harm',\n",
    "#  # 'active_si': 'suicide',\n",
    "#  # 'passive_si': 'suicide',\n",
    "#     'suicide':'suicide',\n",
    "#  'bully': 'bully',\n",
    "#  'abuse_physical': 'abuse_physical',\n",
    "#  'abuse_sexual': 'abuse_sexual',\n",
    "#  'relationships': 'relationship',\n",
    "#  'grief_bereavement': 'bereavement',\n",
    "#  'loneliness_isolated': 'isolated',\n",
    "#  'anxiety': 'anxiety_stress',\n",
    "#  'depressed_mood': 'depressed',\n",
    "#  'gender_sexual_identity': 'gender_sexual_identity',\n",
    "#  'eating_disorder': 'eating_body_image',\n",
    "#  'substance_use': 'substance'}\n",
    "\n",
    "# ctl_tags = np.unique(list(ctl_tags_d.values())).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0376b2-5f9b-4b86-8640-d53369e0942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge active and passive\n",
    "# suicide_tokens = lexicon['active_si']+lexicon['passive_si']\n",
    "# suicide_tokens = [n for n in suicide_tokens if str(n)!='nan']\n",
    "# lexicon['suicide'] = suicide_tokens\n",
    "\n",
    "# # rename lexicon constructs\n",
    "# for k_old, k_new in ctl_tags_d.items():\n",
    "#     lexicon[k_new] = lexicon.pop(k_old)\n",
    "\n",
    "# # keep only ctl constructs\n",
    "# lexicon_constructs_not_in_ctl = set(lexicon.keys()) - set(ctl_tags_d.values())\n",
    "# lexicon_constructs_not_in_ctl\n",
    "# for construct in lexicon_constructs_not_in_ctl:\n",
    "#     del lexicon[construct]\n",
    "\n",
    "# # for construct-word to doc analyses, have a single construct \n",
    "# # lexicon_prototypes = dict(zip(lexicon.keys(), ['']*len(lexicon.keys())))\n",
    "# lexicon_prototypes = {'suicide': 'suicide',\n",
    "#  'self_harm': 'I cut myself',\n",
    "#  'bully': \"bullied\",\n",
    "#  'abuse_physical': 'physical abuse',\n",
    "#  'abuse_sexual': 'sexual abuse and rape',\n",
    "#  'relationship': 'relationship',\n",
    "#  'bereavement': \"grieving and mourning\",\n",
    "#  'isolated': 'lonely',\n",
    "#  'anxiety_stress': 'anxious',\n",
    "#  'depressed': 'depressed',\n",
    "#  'gender_sexual_identity': 'gender and sexual orientation',\n",
    "#  'eating_body_image': 'eating disorder',\n",
    "#  'substance': 'drugs'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f4b0a-6fb3-4124-8b8d-887cd8555d5a",
   "metadata": {},
   "source": [
    "### Encode\n",
    "\n",
    "you want to encode each token once, because can appear in multiple lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9c6f9-2441-4d77-84f0-f3801d6b2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_lexicon_tokens_d = {}\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedding_type = 'sentence'\n",
    "list_of_lists = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf73e7-b72a-4699-8c06-47bd50163f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1843f01-eba2-4240-9604-efcffb7c06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745a889-95bd-464a-bdc3-3df63935471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_encoded_embeddings = pd.read_csv(input_dir_catpro+'lexicons/suicidal_thoughts_and_behaviors/tokens_embeddings_22-12-02T17-43-57.csv', index_col = 0)\n",
    "# prior_encoded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ce50e-1d4d-4fff-910c-a9120deca223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all lexicon tokens\n",
    "\n",
    "tokens_all = []\n",
    "\n",
    "for construct in lexicon.keys():\n",
    "    tokens = list(lexicon.get(construct).get('seed_tokens'))+list(lexicon.get(construct).get('other_tokens'))\n",
    "    tokens_all.append(tokens)\n",
    "tokens_all = [n for i in tokens_all for n in i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee5220-17a1-4644-8daf-1dfb089bdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#20 sec\n",
    "run_this = True\n",
    "\n",
    "if run_this:\n",
    "    embeddings_lexicon_tokens = vectorize(tokens_all, list_of_lists=list_of_lists, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs\n",
    "    embeddings_lexicon_tokens_d = dict(zip(tokens_all, embeddings_lexicon_tokens))\n",
    "\n",
    "\n",
    "    with open(embeddings_dir+f'embeddings_lexicon-tokens_{model_name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(embeddings_lexicon_tokens_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:        \n",
    "    with open(embeddings_dir+f'embeddings_lexicon-tokens_{model_name}.pickle', 'rb') as handle:\n",
    "        embeddings_lexicon_tokens_d = pickle.load(handle)\n",
    "    # with open(embeddings_dir+f'embeddings_{model_name}.pickle') as f:\n",
    "    #     embeddings_lexicon_tokens_d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004309a-3e42-4f9e-9cb1-d926e7ee3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon_prototypes.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10b249-35a9-4bae-a7db-121eed01bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# run_this = False\n",
    "\n",
    "# if run_this:\n",
    "\n",
    "#     embeddings_lexicon_prototypes = vectorize(list(lexicon_prototypes.values()), list_of_lists=False, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs\n",
    "\n",
    "#     for k,v in zip(list(lexicon_prototypes.values()), embeddings_lexicon_prototypes):\n",
    "#         embeddings_lexicon_tokens_d[k]=v\n",
    "#     with open(embeddings_dir+f'embeddings_{model_name}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(embeddings_lexicon_tokens_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0fffc-6eeb-4784-a3e6-e6c373609b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create weighted lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f40837-fd5b-497d-ba6b-153838c53676",
   "metadata": {},
   "source": [
    "### Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4c311-3708-4c38-b038-2bbc09e293e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(input_dir+f'train10_train_concurrent_metadata_100perconstruct_with_messages_preprocessed_23-03-20T17-50-34.csv')\n",
    "# docs_clean_clauses = df['docs_clean_clauses'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca4387-f244-4fc0-9e22-cc45ab33a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['message','docs_clean_clauses','docs_clean_w_w']].iloc[::10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3188d0-2135-48c5-8da5-eb708175c34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa11a3-3532-4e6b-8655-98faa5838884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28033890-3318-4121-93aa-8f53c098a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon_remove = pd.read_csv('./../data/lexicons/suicidal_thoughts_and_behaviors/suicide_risk_lexicon_thesauri_questionnaires_23-03-16T19-35-06_osiris_v01.csv', index_col = 0,encoding='cp1252')\n",
    "# # suicide_list = lexicon_remove['active_si'].tolist()+lexicon_remove['passive_si'].tolist()\n",
    "# nans= ['nan']*(lexicon_remove.shape[1]+6)\n",
    "# lexicon_remove['suicide'] = suicide_list_temp+nans\n",
    "# lexicon_remove['suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef7d33-3e51-472e-8a88-136076738ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lexicon_remove_d = {}                            \n",
    "# for l in lexicon.keys():\n",
    "#     l=     ctl_tags_d_inv.get(l)\n",
    "#     if l=='suicide':\n",
    "#         lexicon_remove_d[l] = lexicon_remove_i\n",
    "#         continue\n",
    "\n",
    "#     lexicon_remove_i = lexicon_remove[[l,l+'_remove']].replace('seed_token', 0).replace('construct label', 1)\n",
    "#     lexicon_remove_i['self-injury_remove'] = lexicon_remove_i['self-injury_remove'].astype(float)\n",
    "    \n",
    "    \n",
    "#     lexicon_remove_i = lexicon_remove_i[lexicon_remove_i[l+'_remove']>=1][l].values\n",
    "#     lexicon_remove_i = [n for n in lexicon_remove_i if str(n)!= 'nan' ]\n",
    "#     print(lexicon_remove_i)    \n",
    "#     # to_remove_i = []\n",
    "#     lexicon_remove_d[l] = lexicon_remove_i\n",
    "    \n",
    "                             \n",
    "                             \n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5fc3e-1f01-4ba3-8e52-3846caa7f59d",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a90a0-4d23-4221-b6a0-a0499d049c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'word_clause'\n",
    "method.startswith('word_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e1982-5233-4b5c-9b78-1c61e840f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def construct_text_similarity(\n",
    "    constructs = None,\n",
    "    lexicon = None, \n",
    "    construct_prototype_d = None,\n",
    "    embeddings_construct_d = None,\n",
    "    docs = None,\n",
    "    embeddings_docs_d = None,\n",
    "    method = 'word_clause'\n",
    "):\n",
    "    '''\n",
    "    A doc is composed of tokens. We compute the similarity between the construct and each token\n",
    "    and taken some summary statistics\n",
    "    \n",
    "    Args:\n",
    "        construct_embeddings: \n",
    "        docs_embeddings: \n",
    "        method: {'word_word', 'word_clause', 'wlexicon_clause'}\n",
    "\n",
    "    Returns:\n",
    "    '''\n",
    "    # encode all list of lists \n",
    "\n",
    "    \n",
    "\n",
    "    feature_vectors_all = []\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "        embeddings_tokens_doc_i = embeddings_docs_d.get(i)\n",
    "        feature_vectors_doc = [str(doc)]\n",
    "        feature_vectors_doc_col_names = ['doc']\n",
    "        for construct in constructs:\n",
    "            if method.startswith('word_'):\n",
    "                construct_prototype = construct_prototype_d.get(construct)\n",
    "                embedding_construct = embeddings_construct_d.get(construct_prototype)\n",
    "                \n",
    "            elif method.startswith('lexicon_'):\n",
    "                lexicon_tokens = lexicon.get(construct)\n",
    "                \n",
    "                embedding_construct = []\n",
    "                for token in lexicon_tokens:\n",
    "                    token_embedding = embeddings_construct_d.get(token)\n",
    "                    embedding_construct.append(token_embedding)\n",
    "            elif method.startswith('wlexicon_'):\n",
    "                print('need to implement. break.')\n",
    "                break\n",
    "            # formatting\n",
    "            embedding_construct = np.array(embedding_construct, dtype=float)\n",
    "            embeddings_tokens_doc_i = np.array(embeddings_tokens_doc_i, dtype=float)\n",
    "            \n",
    "\n",
    "            if method.startswith('word_'):\n",
    "                assert len(embedding_construct.shape) == 1\n",
    "                if embeddings_tokens_doc_i.shape[0] == 0: #happens when there is an empty str\n",
    "                    embeddings_tokens_doc_i = [np.zeros(embedding_construct.shape[0])]\n",
    "                cosine_scores_docs_i = cosine_similarity([embedding_construct], embeddings_tokens_doc_i)\n",
    "            else: #construct is a list of lists\n",
    "                if doc == []:\n",
    "                    cosine_scores_docs_i = [0]\n",
    "                else:                \n",
    "                    try: cosine_scores_docs_i = cosine_similarity(embedding_construct, embeddings_tokens_doc_i)\n",
    "                    except: \n",
    "                        print('broke, returning cosine_similarity = 0')\n",
    "                        cosine_scores_docs_i  = cosine_scores_docs_i = [0]\n",
    "                        # return doc, embedding_construct, embeddings_tokens_doc_i\n",
    "            doc_sim_mean = np.mean(cosine_scores_docs_i)\n",
    "            doc_sim_median = np.median(cosine_scores_docs_i)\n",
    "            doc_sim_max = np.max(cosine_scores_docs_i)\n",
    "            feature_vectors_doc.extend([doc_sim_mean, doc_sim_median,doc_sim_max])\n",
    "            feature_vectors_doc_col_names.extend([construct+\"_mean\", construct+\"_median\", construct+\"_max\"])\n",
    "\n",
    "        feature_vectors_doc_df = pd.DataFrame(feature_vectors_doc, index = feature_vectors_doc_col_names).T\n",
    "        feature_vectors_all.append(feature_vectors_doc_df)\n",
    "\n",
    "    feature_vectors_all = pd.concat(feature_vectors_all).reset_index(drop=True)            \n",
    "    return feature_vectors_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59589d0-58a7-4649-9331-cf2fa7a8a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs_clean_clauses'].astype(str).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188a9e4-da67-4bc7-b50f-71565dc77918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs_clean_clauses'] = df['docs_clean_clauses'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c5f21-c77e-4da2-b67f-5a469e1df405",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean_clauses = df['docs_clean_clauses'].tolist()\n",
    "docs_clean_clauses = [eval(n) for n in docs_clean_clauses]\n",
    "# docs_clean_clauses = [eval(n) for n in docs_clean_clauses]\n",
    "\n",
    "docs_clean_clauses_unique = np.unique(docs_clean_clauses).tolist()\n",
    "docs_clean_clauses_unique.remove([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679bac1-4c9a-4cf9-ab18-e927d3dd7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_clean_clauses_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba88d4c-b6ef-4931-aee1-cbfefa22e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "%%time \n",
    "\n",
    "# 12 m for 1300 docs, each one tokenized\n",
    "\n",
    "\n",
    "run_this = True\n",
    "\n",
    "if run_this:\n",
    "    model_name = 'all-MiniLM-L6-v2'\n",
    "    embedding_type = 'sentence'\n",
    "    list_of_lists = True\n",
    "    verbose = True\n",
    "\n",
    "    embeddings_tokens_docs_unique = vectorize(docs_clean_clauses_unique, list_of_lists=list_of_lists, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs    \n",
    "    embeddings_tokens_docs_unique_d = dict(zip(list(range(len(docs_clean_clauses))), embeddings_tokens_docs_unique))\n",
    "    with open(embeddings_dir+f'embeddings_docs_tokenized_unique_{model_name}_{ts}.pickle', 'wb') as handle:\n",
    "        pickle.dump(embeddings_tokens_docs_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(embeddings_dir+f'embeddings_docs_tokenized_unique_{model_name}_{ts}.pickle', 'rb') as handle:\n",
    "        embeddings_tokens_docs_unique_d = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59b6f4-fdeb-4bc7-b160-96a5f2734be8",
   "metadata": {},
   "source": [
    "# Link back to documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321feb2-5da6-4710-8d09-a03aa5d3d5e1",
   "metadata": {},
   "source": [
    "### Encode docs (list of lists is faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357e6cb-f5ea-4e49-877e-59778c1c6df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# # 12 m for 1300 docs, each one tokenized\n",
    "\n",
    "\n",
    "# run_this = False\n",
    "\n",
    "# if run_this:\n",
    "#     model_name = 'all-MiniLM-L6-v2'\n",
    "#     embedding_type = 'sentence'\n",
    "#     list_of_lists = True\n",
    "#     verbose = True\n",
    "\n",
    "#     embeddings_tokens_docs = vectorize(docs_clean_clauses, list_of_lists=list_of_lists, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs    \n",
    "#     embeddings_tokens_docs_d = dict(zip(list(range(len(docs_clean_clauses))), embeddings_tokens_docs))\n",
    "#     with open(embeddings_dir+f'embeddings_docs_{model_name}_train10_train_concurrent_metadata_100perconstruct_with_messages_{ts}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(embeddings_tokens_docs_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# else:\n",
    "#     with open(embeddings_dir+'embeddings_docs_all-MiniLM-L6-v2_train10_train_concurrent_metadata_100perconstruct_with_messages.pickle', 'rb') as handle:\n",
    "#         embeddings_tokens_docs_d = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9300eac-0d6f-4497-a0a2-eb3c26b00300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2fb1a-51bc-4588-9c23-a177c26b88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs = lexicon.keys()\n",
    "# construct_prototype_d = lexicon_prototypes\n",
    "# embeddings_construct_d = embeddings_lexicon_tokens_d\n",
    "# docs = docs_clean_clauses\n",
    "# embeddings_docs_d = embeddings_tokens_docs_d\n",
    "# method = 'word_clause'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53409cde-547e-4a74-9ad1-39aff097f2d5",
   "metadata": {},
   "source": [
    "### TODO Encode tokes from words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2172a9-c221-4641-9cb0-c04c6c86087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "\n",
    "\n",
    "# feature_vectors_d = {}\n",
    "\n",
    "# for method in ['word_word']:\n",
    "    \n",
    "#     if method == 'word_word':\n",
    "#         docs = df['docs_clean_w_w'].values\n",
    "#     print('=========================')\n",
    "#     print(method)\n",
    "#     feature_vectors_all = construct_text_similarity(\n",
    "#         constructs = lexicon.keys(),\n",
    "#         construct_prototype_d = lexicon_prototypes,\n",
    "#         embeddings_construct_d = embeddings_lexicon_tokens_d,\n",
    "#         docs = docs,\n",
    "#         embeddings_docs_d = ######TODO,\n",
    "#         method = method,\n",
    "#                                                    )\n",
    "#     feature_vectors_d[method]=feature_vectors_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73102e6-ba90-4552-8145-c68d7ee6c883",
   "metadata": {},
   "source": [
    "### todo: wlexicon for weightedLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fce10-ddae-4bd8-9f2d-c3d79dac3485",
   "metadata": {},
   "source": [
    "### word_ methods for prototype and lexicon_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4b61c-cffd-4209-92e7-1aa32f13096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82faacd-2f5a-4e7f-8839-20502096e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs = lexicon.keys()\n",
    "# lexicon\n",
    "# construct_prototype_d = lexicon_prototypes\n",
    "# embeddings_construct_d = embeddings_lexicon_tokens_d\n",
    "# docs = docs_clean_clauses\n",
    "# embeddings_docs_d = embeddings_tokens_docs_unique_d\n",
    "# method = 'lexicon_clause'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c621165-4382-4392-9e91-561f30dabac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_seed = {}\n",
    "for c in lexicon.keys():\n",
    "    lexicon_seed[c] = lexicon.get(c).get('seed_tokens')\n",
    "\n",
    "lexicon_all = {}\n",
    "for c in lexicon.keys():\n",
    "    lexicon_all[c] = list(lexicon.get(c).get('seed_tokens'))+list(lexicon.get(c).get('other_tokens'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fae2a-855d-4d88-9777-07e9eefc1945",
   "metadata": {},
   "source": [
    "# Prototypes - clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12bc09-7e66-49c6-ae41-4db8f4ef6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "\n",
    "feature_vectors_d = {}\n",
    "\n",
    "for method in ['lexicon_clause']:#['word_clause', 'lexicon_clause']:\n",
    "    print('=========================')\n",
    "    print(method)\n",
    "    feature_vectors_all = construct_text_similarity(\n",
    "        constructs = lexicon.keys(),\n",
    "        lexicon = lexicon_seed, #SEED tokens only\n",
    "        construct_prototype_d = None,\n",
    "        embeddings_construct_d = embeddings_lexicon_tokens_d,\n",
    "        docs = docs_clean_clauses_unique,\n",
    "        embeddings_docs_d = embeddings_tokens_docs_unique_d,\n",
    "        method = method,\n",
    "                                                   )\n",
    "    feature_vectors_d[method]=feature_vectors_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe760f9-85c3-48fb-ba3c-9be3986321b1",
   "metadata": {},
   "source": [
    "### map back onto DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a0bb6-8424-40b1-80f2-b5a297873fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = [n+'_max' for n in lexicon.keys()]\n",
    "method_i_max = feature_vectors_d.get(method)[max_cols+['doc']]\n",
    "method_i_max.columns = max_cols+['docs_clean_clauses']\n",
    "# df_cts = df.copy()\n",
    "# df_cts[method_i_max.columns] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6215e-0ee3-4116-a67c-88c66305af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_i_max.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510665f-b909-4ac7-9a0d-75898bad1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts = df.merge(method_i_max, on='docs_clean_clauses', how='outer')\n",
    "print(df_cts.shape)\n",
    "df_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d421c-79d1-4b13-9c1c-cbbeb96f8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts.to_csv(input_dir+f'dataset_cts_protoypes_{ts}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5add3c-1afd-4e2b-81b4-0e9cf9bcac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44411038-e625-41d7-b9bf-ce6dd8f2ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27ffae-3d30-468e-9989-43fca4b4f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cts_threshold = df_cts[df_cts[max_cols]>0.45]\n",
    "cts_rank = df_cts_threshold[max_cols].sum().sort_values()[::-1]\n",
    "cts_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d801513-75f4-45b5-9719-733db66b1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d7939-d223-406d-bdf8-d7669edc1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_rank_df = pd.DataFrame(cts_rank).reset_index()\n",
    "cts_rank_df.columns = ['Construct', 'Sum > 0.45']\n",
    "cts_rank_df['Sum > 0.45'] = [np.round(n,1) for n in cts_rank_df['Sum > 0.45'].values]\n",
    "cts_rank_df['Construct'] = [n.replace('_' ,' ').replace(' max', '').replace('si', 'SI').capitalize() for n in cts_rank_df['Construct'].values]\n",
    "cts_rank_df.to_csv(output_dir+'rank_cts_045_prototypes.csv')\n",
    "cts_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0649f6-cab8-4602-b5c9-205456060060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98791693-d01b-47d0-bee4-5c70a4745c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c01c3c-14f3-4fbe-aa62-23d4804d1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15f0b1-2a75-4fba-bc71-30c0d815a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(df_cts[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]], hover_data=[\"SI_DescribeText_clean\"],points=\"all\",title='Prototype-Clause Similarity')\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=dict(text=\"Prototype-Clause Similarity\", font=dict(size=16)),\n",
    "    xaxis_title=\"Constructs (prototypes)\", yaxis_title=\"Max. cosine similarity<br>with document clauses\", \n",
    "    template='simple_white',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "savefig=True\n",
    "# for template in [\"simple_white\",]:  #\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]:\n",
    "    # fig.update_layout(template=template,)# title=\"'%s' theme\" % template)\n",
    "if savefig:\n",
    "    time.sleep(5)\n",
    "    print('done')\n",
    "    # save all files in the same html \n",
    "    with open(output_dir+f'cts_{method}_prototypes_{ts}.html', 'a') as f:\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "      # save each file separately \n",
    "      # pio.write_image(fig,output_dir+\"submissions_desire.png\", scale=3, width=600, height=1200, engine='auto')\n",
    "      # fig.write_html(output_dir+f\"submissions_{activity}.html\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3224e-bcc9-4890-b904-35312f13cdea",
   "metadata": {},
   "source": [
    "## Remove documents below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f95b56-8589-4ce0-a6ef-42b65c3fff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts_nan = df_cts.copy()[max_cols]\n",
    "df_cts_nan[(df_cts_nan<=threshold)] = np.nan\n",
    "df_cts_nan['SI_DescribeText_clean'] = df_cts['SI_DescribeText_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe34c1f-f9a3-4f82-936c-60764af760ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1b05f-fa4c-437e-bcbc-3189c7f2b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(df_cts_nan[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]], hover_data=[\"SI_DescribeText_clean\"],points='all',title='Prototype-Clause Similarity')\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=dict(text=\"Prototype-Clause Similarity\", font=dict(size=16)),\n",
    "    xaxis_title=\"Constructs (prototypes)\", yaxis_title=\"Max. cosine similarity<br>with document clauses\", \n",
    "    template='simple_white',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "savefig=True\n",
    "# for template in [\"simple_white\",]:  #\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]:\n",
    "    # fig.update_layout(template=template,)# title=\"'%s' theme\" % template)\n",
    "if savefig:\n",
    "    # time.sleep(5)\n",
    "    # print('done')\n",
    "    # save all files in the same html \n",
    "    with open(output_dir+f'cts_{method}_prototypes_nan_{ts}.html', 'a') as f:\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "      # save each file separately \n",
    "      # pio.write_image(fig,output_dir+\"submissions_desire.png\", scale=3, width=600, height=1200, engine='auto')\n",
    "      # fig.write_html(output_dir+f\"submissions_{activity}.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f67407-268a-47a2-a2d2-9d1f701d59e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9000e6-95f4-46e0-bb55-7b800c77e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a76664-67b6-4f21-8497-2f133b3aeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cols = ['ID', 'time_sent', 'time_complete', 'date_sent', 'date_complete',\n",
    "       'survey_numb', 'day_study', 'Daily_SI_DesireKill', 'Daily_SI_Urge',\n",
    "       'Daily_SI_Intent', 'Daily_SI_ResistUrge', 'Daily_SI_DesireDie',\n",
    "       'Day_SI_Percent', 'Day_SI_Time', 'Day_SI_Describe',\n",
    "       'Daily_SI_Interfere', 'Daily_SI_Cope', 'Daily_SI_Images', 'Day_SI_Plan',\n",
    "       'SI_DescribeText', 'Day_NSSI', 'Daily_SA', 'Daily_Functioning',\n",
    "       'Daily_Affect_Feel', 'Daily_Affect_Aware', 'Daily_Affect_Pos',\n",
    "       'Daily_Affect_Neg', 'Daily_Affect_Stress', 'Daily_Affect_Temper',\n",
    "       'Daily_Affect_Anger', 'Daily_Impul_SaidWithoutThink',\n",
    "       'Daily_Impul_Money', 'Daily_Impul_Impatient', 'Daily_Impul_Decision',\n",
    "       'Daily_Impul_Upset', 'Daily_Impul_ActEmotions', 'Daily_Impul_Impulse',\n",
    "       'Daily_Impul_Irrespon', 'Daily_Impul_Dangerous', 'Daily_Impul_Ate',\n",
    "       'Daily_SocialSupport_Family', 'Daily_SocialSupport_Friends',\n",
    "       'Daily_SI_Desire_Tomorrow', 'Daily_SI_Urge_Tomorrow',\n",
    "       'Daily_SI_ResistUrge_Tomorrow', 'SI_DescribeText_clean', 'word_count',\n",
    "       'skipped', 'Day_SI_Describe_transformed', 'docs_clean_clauses',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e819501-e0b5-4ea0-b1f0-b665c7d021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts_zero = df_cts.copy()[max_cols]\n",
    "df_cts_zero[(df_cts_zero<=threshold)] = 0\n",
    "\n",
    "\n",
    "df_cts_zero[other_cols] = df_cts[other_cols]\n",
    "\n",
    "df_cts_zero.to_csv(input_dir+f'dataset_cts_protoypes_zero_{ts}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5b45c-b2ac-408d-9a74-6780a0361570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts_nan = df_cts[df_cts<threshold]==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fd406-ae9e-4a54-8a45-5641136cb542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b69c0df4-097c-4f5c-a86a-fd3ca5e3f12a",
   "metadata": {},
   "source": [
    "# Lexicon - clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce9d1f-023e-44c5-82d8-3d5dd24b07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "\n",
    "feature_vectors_d = {}\n",
    "\n",
    "for method in ['lexicon_clause']:#['word_clause', 'lexicon_clause']:\n",
    "    print('=========================')\n",
    "    print(method)\n",
    "    feature_vectors_all = construct_text_similarity(\n",
    "        constructs = lexicon.keys(),\n",
    "        lexicon = lexicon_all, #just changed this\n",
    "        construct_prototype_d = None,\n",
    "        embeddings_construct_d = embeddings_lexicon_tokens_d,\n",
    "        docs = docs_clean_clauses_unique,\n",
    "        embeddings_docs_d = embeddings_tokens_docs_unique_d,\n",
    "        method = method,\n",
    "                                                   )\n",
    "    feature_vectors_d[method]=feature_vectors_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66dd61-1839-46dc-8ca0-0e8ff7a94931",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = [n+'_max' for n in lexicon.keys()]\n",
    "method_i_max = feature_vectors_d.get(method)[max_cols+['doc']]\n",
    "method_i_max.columns = max_cols+['docs_clean_clauses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35d637-cbb6-41db-9712-56aa04092f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs_clean_clauses'] = df['docs_clean_clauses'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419c43b-3967-4308-a0ba-86944a64c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts = df.merge(method_i_max, on='docs_clean_clauses', how='outer')\n",
    "df_cts_threshold = df_cts[df_cts[max_cols]>0.45]\n",
    "cts_rank = df_cts_threshold[max_cols].sum().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ab8e4-0f07-43a5-9a37-4f00420e1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts.to_csv(input_dir+f'dataset_cts_alltokens_{ts}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa5d1a-ff47-4a78-9b55-db36b3076516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(df_cts[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]], hover_data=[\"SI_DescribeText_clean\"],points=\"all\")\n",
    "\n",
    "savefig=True\n",
    "for template in [\"simple_white\",]:  #\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]:\n",
    "    fig.update_layout(template=template,)# title=\"'%s' theme\" % template)\n",
    "    if savefig:\n",
    "      # save all files in the same html \n",
    "      with open(output_dir+f'cts_{method}_alltokens_{ts}.html', 'a') as f:\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "      # save each file separately \n",
    "      # pio.write_image(fig,output_dir+\"submissions_desire.png\", scale=3, width=600, height=1200, engine='auto')\n",
    "      # fig.write_html(output_dir+f\"submissions_{activity}.html\")\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee41e1e-c825-4068-b4c4-b4770dcc552e",
   "metadata": {},
   "source": [
    "### Without documents below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba91112-d4a7-4bf9-a8b5-79caeb071046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cts_nan = df_cts.copy()[max_cols]\n",
    "df_cts_nan[(df_cts_nan<=threshold)] = np.nan\n",
    "df_cts_nan['SI_DescribeText_clean'] = df_cts['SI_DescribeText_clean']\n",
    "\n",
    "\n",
    "fig = px.box(df_cts_nan[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]], hover_data=[\"SI_DescribeText_clean\"],points='all',title='Lexicon-Clause Similarity')\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=dict(text=\"Prototype-Clause Similarity\", font=dict(size=16)),\n",
    "    xaxis_title=\"Constructs (lexicon)\", yaxis_title=\"Max. cosine similarity<br>with document clauses\", \n",
    "    template='simple_white',\n",
    ")\n",
    "\n",
    "\n",
    "savefig=True\n",
    "# for template in [\"simple_white\",]:  #\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]:\n",
    "    # fig.update_layout(template=template,)# title=\"'%s' theme\" % template)\n",
    "if savefig:\n",
    "    # time.sleep(5)\n",
    "    # print('done')\n",
    "    # save all files in the same html \n",
    "    with open(output_dir+f'cts_{method}_alltokens_nan_{ts}.html', 'a') as f:\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "      # save each file separately \n",
    "      # pio.write_image(fig,output_dir+\"submissions_desire.png\", scale=3, width=600, height=1200, engine='auto')\n",
    "      # fig.write_html(output_dir+f\"submissions_{activity}.html\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f34ac-f207-4b9c-860d-5823d0486976",
   "metadata": {},
   "source": [
    "### Colorcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9e2b-43f0-45c7-87e4-09641935094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://plotly.com/python/line-and-scatter/\n",
    "\n",
    "# # Need to make more longform\n",
    "\n",
    "# data = df_cts[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]+['Day_SI_Describe_transformed']]\n",
    "# fig = px.scatter(data, y=\"count\", x=\"nation\", color=\"Day_SI_Describe_transformed\")\n",
    "# fig.update_traces(marker_size=10)\n",
    "# fig.update_layout(scattermode=\"group\")\n",
    "# fig = px.box(data, hover_data=[\"SI_DescribeText_clean\"],points=\"all\",color_discrete_sequence = 'Day_SI_Describe_transformed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6a0d1-e3c4-4adf-a61b-59f7f5ef8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Day_SI_Describe_transformed']=data['Day_SI_Describe_transformed'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed9037-50e2-4f4a-ae1a-0dcda22d8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.box(data, hover_data=[\"SI_DescribeText_clean\"],points=\"all\",color = 'Day_SI_Describe_transformed')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8c0cb-3b51-4975-9ec9-b62989e5d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(data, hover_data=[\"SI_DescribeText_clean\"],points=\"all\",color = 'Day_SI_Describe_transformed')\n",
    "\n",
    "\n",
    "\n",
    "savefig=True\n",
    "for template in [\"simple_white\",]:  #\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]:\n",
    "    fig.update_layout(template=template,)# title=\"'%s' theme\" % template)\n",
    "    if savefig:\n",
    "      # save all files in the same html \n",
    "      with open(output_dir+f'cts_{method}_alltokens_colorcode-intent_{ts}.html', 'a') as f:\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "      # save each file separately \n",
    "      # pio.write_image(fig,output_dir+\"submissions_desire.png\", scale=3, width=600, height=1200, engine='auto')\n",
    "      # fig.write_html(output_dir+f\"submissions_{activity}.html\")\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8541a-b10f-4af8-a88a-adf8f797f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.parallel_coordinates(df_cts[cts_rank.index.tolist()+[\"SI_DescribeText_clean\"]])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ec7ba-abd3-4314-b943-38ab9ec8ce22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de462b-433a-4bad-b928-ffca6f210425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db1c8e-ac7d-4665-8e42-f4a35a8d097f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cace897-0b32-4551-b045-ebe87ec663a4",
   "metadata": {},
   "source": [
    "# Concurrent validity (once we have human judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38ebd5-e690-4659-b42a-9a9a243349ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dcor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import spearmanr, pointbiserialr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def cohend(d1, d2):\n",
    " # calculate the size of samples\n",
    " n1, n2 = len(d1), len(d2)\n",
    " # calculate the variance of the samples\n",
    " s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    " # calculate the pooled standard deviation\n",
    " s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    " # calculate the means of the samples\n",
    " u1, u2 = np.mean(d1), np.mean(d2)\n",
    " # calculate the effect size\n",
    " return (u1 - u2) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1807659-a4dd-4342-a469-610068133431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f05b5-ba2b-4200-9127-4b154795374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcor_all = []\n",
    "    \n",
    "r_all = []\n",
    "\n",
    "for method in ['word_clause', 'lexicon_clause']:\n",
    "    print(method)\n",
    "    feature_vectors_all = feature_vectors_d.get(method)\n",
    "    tags = df[constructs]\n",
    "\n",
    "    max_cols = [n+'_max' for n in constructs]\n",
    "    method_i_max = feature_vectors_all[max_cols]\n",
    "    assert [n+'_max' for n in constructs] == max_cols \n",
    "    method_i_max.columns = constructs        \n",
    "    # dcor\n",
    "    # dcor_i = dcor.distance_correlation(tags,method_i_max) \n",
    "    # dcor_all.append(dcor_i)\n",
    "    # print('dcor', dcor_i)\n",
    "    \n",
    "\n",
    "    r_method_i = []\n",
    "    for construct in constructs:\n",
    "        y_true_1 = tags[tags[construct]==1]\n",
    "        y_true_0 = tags[tags[construct]==0].sample(n=y_true_1.shape[0])\n",
    "        y_true_1_indexes = y_true_1.index.tolist()\n",
    "        y_true_0_indexes = y_true_0.index.tolist()\n",
    "        y_true = pd.concat([y_true_1,y_true_0],axis=0)[construct].values\n",
    "        # y_true = y_true.sample(n=y_true.shape[0])\n",
    "        y_pred = method_i_max[construct][y_true_1_indexes+y_true_0_indexes].values\n",
    "\n",
    "        # r, p = spearmanr(y_true,y_pred)\n",
    "        r, p = pointbiserialr(y_true,y_pred)\n",
    "\n",
    "\n",
    "\n",
    "        # dcor_i = dcor.distance_correlation(y_true,y_pred) #0.45\n",
    "        # print(construct, f'dcor={np.round(dcor_i,2)}', np.round(r,2), np.round(p, 4))\n",
    "\n",
    "        df_i = pd.DataFrame(method_i_max[construct][y_true_1_indexes+y_true_0_indexes])\n",
    "        df_i['Truth'] = [1]*len(y_true_1_indexes) + [0]*len(y_true_0_indexes) \n",
    "        y_pred_0 = df_i[df_i['Truth']==0][construct].values\n",
    "        y_pred_1 = df_i[df_i['Truth']==1][construct].values\n",
    "        # cohens_d = (np.mean(y_pred_1) - np.mean(y_pred_0)) / (np.sqrt((np.std(y_pred_1) ** 2 + np.std(y_pred_0) ** 2) / 2))\n",
    "        cohens_d = cohend(y_pred_1,y_pred_0)\n",
    "        # rocauc = roc_auc_score(y_true,y_pred)\n",
    "        # rocauc = str(np.round(roc_auc_score,2))\n",
    "        title_i = f\"{construct}: rho={np.round(r,2)} (p={np.round(p, 4)}) Cohen's {np.round(cohens_d,2)}\"\n",
    "\n",
    "        # display(df_i)\n",
    "        sns.kdeplot(data=df_i,x = construct, hue='Truth')\n",
    "        plt.title(title_i)\n",
    "        plt.show()\n",
    "        r_method_i.append(r)\n",
    "        # r_all_method_i_stats.append(\n",
    "\n",
    "    r_all.append(r_method_i)\n",
    "    stats = np.round([np.mean(r_all), np.std(r_all),np.min(r_all),np.max(r_all)],2)\n",
    "    print(f'stats: {stats[0]} ± {stats[1]} ({stats[2]}-{stats[3]})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d42ad-eba9-4b84-9394-47a4be2e0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882cb09-5fa2-45fa-b841-a38afff630cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e621d8-b8eb-4720-bfc3-46fe1f6f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.round([np.mean(r_all), np.std(r_all),np.min(r_all),np.max(r_all)],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96573eef-067b-42ff-8427-c96f84dc8a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df_i,x = construct, hue='Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df146d7c-3db4-4617-bf9a-4b18adac7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e84cc2-f90f-4a70-b9d6-987540feed24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86aff82-3f8a-4410-84ab-98a385e519b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cols = [n+'_max' for n in constructs]\n",
    "\n",
    "# for c in constructs:\n",
    "#     print(c)\n",
    "#     feature_vectors_all_max_docs = feature_vectors_all[['doc']+max_cols].sort_values([construct+'_max'])[::-1]['doc'].values[:5]\n",
    "#     [print('. '.join(eval(n)), '\\n') for n in feature_vectors_all_max_docs]\n",
    "#     # print(feature_vectors_all_max_docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df3bbb-2cfb-4ed3-bca2-122f8b9b6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in feature_vectors_all.index:\n",
    "#     truth = dict(zip(constructs, df[constructs].iloc[idx,:].values))\n",
    "#     print(truth)\n",
    "    \n",
    "#     features = feature_vectors_all.iloc[idx,:].values\n",
    "#     print('. '.join(eval(features[0])))\n",
    "#     print(features[1:])\n",
    "          \n",
    "#     max_cols = [n+'_max' for n in constructs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13339b5d-adc8-4e2a-97f0-8b8b3a478505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e0c37-d295-460a-9d07-e964b553858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33334890-bc7a-42c8-ac96-a95a6e1202b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# tags = df[constructs]\n",
    "\n",
    "# max_cols = [n+'_max' for n in constructs]\n",
    "# word_clause_max = feature_vectors_all[max_cols]\n",
    "# assert [n+'_max' for n in constructs] == max_cols \n",
    "# word_clause_max.columns = constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12d9c9-edb3-40e0-b251-b719da3cb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clause_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3404f-92b3-453b-991f-92bb0b14c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcor_lexicon_clause_max = dcor.distance_correlation(tags,lexicon_clause_max) #0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f488dc-ebd0-4f0e-b018-f67bc6bfd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcor_lexicon_clause_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69186f85-8dc9-4163-b0ab-946cea2135ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f20933-2625-44da-a199-025e55b2d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clause_max.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9b40b-6e95-4645-926b-7226aff61fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tags.columns.tolist() == word_clause_max.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886df18a-76e5-46f1-aea5-30ecb8c8b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for construct in constructs:\n",
    "    y_true_1 = tags[tags[construct]==1]\n",
    "    y_true_0 = tags[tags[construct]==0].sample(n=y_true_1.shape[0])\n",
    "    y_true_1_indexes = y_true_1.index.tolist()\n",
    "    y_true_0_indexes = y_true_0.index.tolist()\n",
    "    y_true = pd.concat([y_true_1,y_true_0],axis=0)[construct].values\n",
    "    # y_true = y_true.sample(n=y_true.shape[0])\n",
    "    y_pred = word_clause_max[construct][y_true_1_indexes+y_true_0_indexes].values\n",
    "    r, p = spearmanr(y_true,y_pred)\n",
    "    print(construct, np.round(r,2), np.round(p, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7b7e6-14b0-45d5-85d2-b23dbd8e4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9e6c0-cebf-40b8-a62e-7b82f3a84888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919501c-635b-4890-885b-29ae72551eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96166ee-98e5-4ff3-afe5-cebc0702417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f591a1-722f-4260-a1a2-888f10873cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d5f86-06b5-4819-b7d6-ef626e726963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2aa211-1935-439a-b7d6-ebdb50a2170d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa671b5-06c0-4815-9ff5-f5e759bc29dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b3c4f-fa0f-4421-82b7-72be3052b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplotword_clause_max[construct][y_true_1_indexes].values\n",
    "word_clause_max[construct][y_true_0_indexes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cccef-725e-48d8-97f2-b42963ae42cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067dd41e-2356-465c-b98b-69c988e4bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct in constructs:\n",
    "    y_true = tags[construct].values\n",
    "    y_pred = word_clause_max[construct]\n",
    "    r, p = spearmanr(y_true,y_pred)\n",
    "    print(construct, np.round(r,2), np.round(p, 4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6780a8-2f3c-456b-85cf-c84fa4776644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct in constructs:\n",
    "    y_true = tags[construct].values\n",
    "    y_pred = word_clause_max[construct]\n",
    "    r, p = spearmanr(y_true,y_pred)\n",
    "    print(construct, np.round(r,2), np.round(p, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34fc24-2311-4fd4-8359-4f827c47bb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b312c9e-e7f6-4c92-b3f8-514e55b5e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32365d99-8319-43ae-bd49-a4d4ad937aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # embeddings_tokens_docs ={}\n",
    "\n",
    "# feature_vectors_all = []\n",
    "\n",
    "# for i, doc in enumerate(docs_clean_clauses[:10]):\n",
    "#     if i%100==0:\n",
    "#         print(i)\n",
    "#     embeddings_tokens_doc_i = vectorize(doc, list_of_lists=list_of_lists, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ecac7-ec10-4a63-aac1-b121e1f87588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# embeddings_tokens_docs ={}\n",
    "\n",
    "feature_vectors_all = []\n",
    "\n",
    "\n",
    "embeddings_tokens_doc_i = vectorize(docs_clean_clauses[:10], list_of_lists=list_of_lists, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab438dbf-57f7-4f83-9583-b2691610a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_construct_prototype.shape) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fda09-1396-45f3-b593-260daf35167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_vectors_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d883104-44dc-4e8f-9b90-da33ffac61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#     for i in docs_embeddings:\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # def df_similarity_token_category(embeddings_tokens_doc, constructs_d, df, docs_clean, summary_stats = None):\n",
    "# #     '''\n",
    "# #     embeddings_tokens_doc\n",
    "# #     '''\n",
    "# # for each doc, it creates a value (e.g., mean across tokens --either words or clauses) for each construct    \n",
    "\n",
    "# feature_vectors_mean = []\n",
    "# feature_vectors_median = []\n",
    "# feature_vectors_max = []\n",
    "\n",
    "# constructs = list(constructs_d.keys())\n",
    "\n",
    "# for i, doc in enumerate(docs_clean):\n",
    "#     embeddings_tokens_doc_i = embeddings_tokens_doc[i]\n",
    "#     df_scores_category_all = pd.DataFrame(docs_clean[i], columns = ['token'])\n",
    "#     for category in constructs:\n",
    "#         embedding_category = constructs_d.get(category)\n",
    "#         embedding_category = np.array(embedding_category, dtype=float)\n",
    "#         embeddings_tokens_doc_i = np.array(embeddings_tokens_doc_i, dtype=float)\n",
    "#         if embeddings_tokens_doc_i.shape[0] == 0: #happens when there is an empty str\n",
    "#             embeddings_tokens_doc_i = [np.zeros(embedding_category.shape[0])]\n",
    "#         cosine_scores = cosine_similarity(embedding_category, embeddings_tokens_doc_i)\n",
    "#         # each token is a row, and each col is a construct being measured for that token.             \n",
    "#         df_scores_category_all[category] = np.array(cosine_scores, dtype = float)[0]#pd.DataFrame(cosine_scores, columns = ['category'])\n",
    "\n",
    "\n",
    "#         # df_scores_category = pd.DataFrame([docs_clean[i], np.array(cosine_scores[0])]).T\n",
    "#         # df_scores_category.columns = ['token', category]\n",
    "#         # df_scores_category = df_scores_category.sort_values(by='token')\n",
    "#         # # df_scores_category_all= df_scores_category_all.merge(df_scores_category, on='token', how = 'outer')\n",
    "#         # df_scores_category_all.append(df_scores_category)\n",
    "#     # df_scores_category_all = pd.concat(df_scores_category_all, axis=1)\n",
    "#     df_scores_category_all = df_scores_category_all[constructs].astype(float)\n",
    "\n",
    "#     # display(df_scores_category_all)\n",
    "#     feature_vectors_mean.append(df_scores_category_all.mean())\n",
    "#     feature_vectors_median.append(df_scores_category_all.median())\n",
    "#     feature_vectors_max.append(df_scores_category_all.max())\n",
    "\n",
    "# feature_vectors_mean = pd.concat(feature_vectors_mean,axis=1).T\n",
    "# feature_vectors_mean.columns = [n+'_mean' for n in feature_vectors_mean.columns]\n",
    "\n",
    "# feature_vectors_median = pd.concat(feature_vectors_median,axis=1).T\n",
    "# feature_vectors_median.columns = [n+'_median' for n in feature_vectors_median.columns]\n",
    "\n",
    "# feature_vectors_max = pd.concat(feature_vectors_max,axis=1).T\n",
    "# feature_vectors_max.columns = [n+'_max' for n in feature_vectors_max.columns]\n",
    "\n",
    "# feature_vectors = pd.concat([feature_vectors_mean, feature_vectors_median, feature_vectors_max],axis=1)\n",
    "# df[feature_vectors.columns.tolist()] = feature_vectors.values\n",
    "# # feature_cols = list(set(feature_vectors.columns)-set(['subreddit','author','date','docs','docs_clean']))\n",
    "# # feature_cols.sort()\n",
    "# # feature_vectors= feature_vectors[['subreddit','author','date','docs','docs_clean']+feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77561dc2-df5d-4e01-913f-0b509d331190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir+f'train10_train_concurrent_metadata_100perconstruct_with_messages_preprocessed_23-03-20T17-50-34.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0b522-c235-4ca4-bbbf-96f6c41471af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_embeddings = {}\n",
    "print('encoding...')\n",
    "embeddings_tokens_doc = vectorize(docs_final, list_of_lists=True, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs\n",
    "# encode tokens of each doc\n",
    "np.save(npy_filepath, embeddings_tokens_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0d197-df96-4645-a46c-2e8fb89f1c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d7a99-af7e-4aa8-9991-57a35e56c6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cfabe-94d8-4118-bab2-7b303255ce75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c77a4c-cedc-4083-8fef-5b968fce8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05685017-be64-45d8-81bf-833c56efeff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd3ca9-bb3e-45e4-b3ce-81750a69912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructs_suicide_risk_lexicon_weighted_centroid = pd.read_csv('./../data/lexicons/suicidal_thoughts_and_behaviors/weighted_centroids_22-12-04T01-06-02.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7447f-cded-44e2-ac92-6a5049f9b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_this = True\n",
    "\n",
    "lexicons_dir = './../data/lexicons/'\n",
    "embeddings_path = lexicons_dir+'embeddings_tokens_all-MiniLM-L6-v2_22-12-02T17-32-05.json'\n",
    "tokens_path = lexicons_dir+'suicidal_thoughts_and_behaviors/concurrent_validity_tokens_cosine_similarity_22-12-02T17-43-57.csv'\n",
    "\n",
    "\n",
    "\n",
    "if run_this:\n",
    "    for approach_embedding_name in approach_embedding_names:\n",
    "        if 'glove' in approach_embedding_name:\n",
    "            model_name = 'glove'\n",
    "\n",
    "        elif 'minilm' in approach_embedding_name:\n",
    "            model_name = 'all-MiniLM-L6-v2'\n",
    "        elif 'psychbert' in approach_embedding_name:\n",
    "            model_name = 'mnaylor/psychbert-cased'\n",
    "        \n",
    "        embedding_type = embedding_name_type.get(model_name)\n",
    "        # if model_name in [\n",
    "        #     # 'mnaylor/psychbert-cased',# cannot run on Mac M1, will run on colab: vectorize(docs_clean_joined, package = 'transformers', model_name = 'mnaylor/psychbert-cased', embedding_type = 'document')\n",
    "        #     # 'all-MiniLM-L6-v2',\n",
    "        #     # 'glove',\n",
    "        #                 ]:\n",
    "        #     continue # skip      \n",
    "        print('approach_embedding_name: ', approach_embedding_name, 'model_name:', model_name, 'embedding_type: ', embedding_type)\n",
    "\n",
    "        len_docs = len(docs_clean)\n",
    "        # print(len_constructs , docs_per_construct )\n",
    "        \n",
    "        if '_w' in approach_embedding_name:\n",
    "            docs_final = df['docs_clean_w_w'].values\n",
    "            # df = pd.read_csv(output_dir+'feature_vectors_16constructs_7subreddits_156docs_w_w_minilm_22-10-09T02-19-07.csv', index_col = 0)\n",
    "            # df = df.iloc[:, :5]\n",
    "        elif '_c' in approach_embedding_name:\n",
    "            docs_final = df['docs_clean_w_c'].values\n",
    "            # df = pd.read_csv(output_dir+'feature_vectors_16constructs_7subreddits_156docs_w_c_minilm_22-10-09T02-19-07.csv', index_col = 0)\n",
    "            # df = df.iloc[:, :5]\n",
    "        df['docs_final']=docs_final\n",
    "\n",
    "\n",
    "        type_of_document_tokenization = '_'.join(approach_embedding_name.split('_')[-2:]) #w, c\n",
    "\n",
    "        \n",
    "        npy_filepath = output_dir+f'army_starrs_{len(constructs)}constructs_{len_docs}docs_{type_of_document_tokenization}_embeddings.npy'\n",
    "        try:\n",
    "            embeddings_tokens_doc = np.load(npy_filepath,\n",
    "                                            allow_pickle=True)\n",
    "            print('loaded from prior run')\n",
    "\n",
    "        except:\n",
    "            print('did not find: ',npy_filepath)\n",
    "            print('encoding...')\n",
    "            embeddings_tokens_doc = vectorize(docs_final, list_of_lists=True, embedding_type = embedding_type, model_name = model_name) # 10 s for list of tokens for 5200 docs\n",
    "            # encode tokens of each doc\n",
    "            np.save(npy_filepath, embeddings_tokens_doc)\n",
    "\n",
    "        filename = model_name.split('/')[-1]\n",
    "\n",
    "        if approach_embedding_name.startswith('wl_'):\n",
    "            # centroid weighted by cosine sim to construct label             \n",
    "\n",
    "            constructs_d = {}\n",
    "            # Load embedings for construct\n",
    "            with open(embeddings_path, 'r') as json_file:\n",
    "                lexicons_embeddings = json.load(json_file)\n",
    "\n",
    "            lexicons_tokens = pd.read_csv(tokens_path, index_col = 0)\n",
    "            for construct in lexicons_tokens['construct'].unique():\n",
    "                weighted_centroid = constructs_suicide_risk_lexicon_weighted_centroid[construct].values\n",
    "                constructs_d[construct]=weighted_centroid\n",
    "                # lexicons_tokens_i = lexicons_tokens[lexicons_tokens['construct']==construct]\n",
    "                # tokens_i = lexicons_tokens_i['token'].values\n",
    "                # scores_i = lexicons_tokens_i['score'].values\n",
    "                # embeddings_i = np.array([lexicons_embeddings.get(token) for token in tokens_i])\n",
    "                # weighted_centroid = np.average(embeddings_i, axis=0, weights=scores_i)\n",
    "                \n",
    "                \n",
    "            print('loaded dict of construct embeddings')\n",
    "\n",
    "        else:\n",
    "            try: \n",
    "#               # TODO:\n",
    "                with open(output_dir+f'constructs{len(constructs)}_{approach_embedding_name}.pkl', 'rb') as f:\n",
    "                    constructs_d = pickle.load(f)\n",
    "                print('loaded dict of construct embeddings')\n",
    "            except:\n",
    "                print('encoding construct embeddings...')    \n",
    "                # encode constructs     \n",
    "                constructs_d = {}\n",
    "                embeddings_constructs = vectorize(constructs, embedding_type = embedding_type, model_name = model_name)\n",
    "                for category, embedding in zip(constructs, embeddings_constructs):\n",
    "                    constructs_d[category] = embedding\n",
    "                with open(output_dir+f'constructs{len(constructs)}_{approach_embedding_name}.pkl', 'wb') as f:\n",
    "                    pickle.dump(constructs_d, f)\n",
    "\n",
    "        # compute similarity (extract features)    \n",
    "        # embeddings_tokens_doc = np.load(output_dir+f'army_starrs_5_{model_name.split('/')[-1]}_embeddings.npy')\n",
    "        # constructs_d['hallucinating'] = constructs_d['hallucinating_hallucination']\n",
    "        # del  constructs_d['hallucinating_hallucination']\n",
    "\n",
    "        feature_vectors = df_similarity_token_category(embeddings_tokens_doc, constructs_d, df, docs_final, summary_stats = None)\n",
    "        feature_vectors.to_csv(output_dir+f'feature_vectors_{len(constructs)}constructs_{len_docs}docs_{approach_embedding_name}_{ts}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
