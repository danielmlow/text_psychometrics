{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.38.1\n",
    "# !pip install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other: \n",
    "# !pip install torch==2.0.1 accelerate==0.15.0 transformers==4.28.1\n",
    "# !conda install pytorch torchvision torchaudio cudatoolkit=11.7 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Local\n",
    "import api_keys\n",
    "from huggingface_hub import login\n",
    "login(token=api_keys.huggingface)\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device    \n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print number of GPUs available\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"\\tName: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"\\tCuda version: {print(torch.version.cuda)}\")\n",
    "        print(f\"\\tCompute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"\\tTotal Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        # Additional details can be accessed via `torch.cuda.get_device_properties(i)`\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation and if your hardware supports CUDA.\")\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "def obtain_json(responses):\n",
    "    jsons = []\n",
    "    for response in responses:\n",
    "        matches = re.findall(r'\\{.*?\\}', response)\n",
    "\n",
    "        # Assuming there's at least one match and it's safe to evaluate\n",
    "        if matches:\n",
    "            # Convert the first match to dictionary\n",
    "            dictionary = eval(matches[0])\n",
    "            jsons.append(dictionary)\n",
    "        else:\n",
    "            jsons.append(response)\n",
    "    return jsons\n",
    "\n",
    "\n",
    "\n",
    "def find_json_in_string(string: str) -> str:\n",
    "    \"\"\"Finds the JSON object in a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to search for a JSON object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    json_string : str\n",
    "    \"\"\"\n",
    "    start = string.find(\"{\")\n",
    "    end = string.rfind(\"}\")\n",
    "    if start != -1 and end != -1:\n",
    "        json_string = string[start : end + 1]\n",
    "    else:\n",
    "        json_string = \"{}\"\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m nvitop -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del tokenizer\n",
    "except: pass\n",
    "torch.cuda.empty_cache()\n",
    "!python3 -m nvitop -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See how they use it for text classification: (from probs or output layer directly?)\n",
    "# https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# model_name = \"google/gemma-2b-it\"\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "# model_name = \"paulml/OGNO-7B\"\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = 0 \n",
    "\n",
    "if 'gemma' in model_name:\n",
    "\t# Gemma\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "\tmodel = model.to(device)\n",
    "elif 'llama' in model_name:\n",
    "\t# Have to restart session after updating transformers\n",
    "\tfrom transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\tmodel = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "elif \"paulml/OGNO-7B\" in model_name:\n",
    "    pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "elif \"microsoft/phi-2\" in model_name:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,  trust_remote_code=True,torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# # alternative\n",
    "# # Llama\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# sequences = pipeline(\n",
    "#     prompt,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=200,\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m nvitop -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "location = 'openmind'\n",
    "\n",
    "if location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/datasets/train10_subset_30/'\n",
    "  output_dir = 'home/dlow/'\n",
    "elif location =='local':\n",
    "  input_dir = './data/ctl/'\n",
    "  output_dir = '/home/dlow/datum/lexicon/data/output/'\n",
    "\n",
    "\n",
    "train = pd.read_csv(input_dir+'train10_train_30perc_messages_texter_metadata.csv')\n",
    "test = pd.read_csv(input_dir+'train10_test_15perc_messages_texter_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_tags13 = ['self_harm',\n",
    " 'suicide',\n",
    " 'bully',\n",
    " 'abuse_physical',\n",
    " 'abuse_sexual',\n",
    " 'relationship',\n",
    " 'bereavement',\n",
    " 'isolated',\n",
    " 'anxiety',\n",
    " 'depressed',\n",
    " 'gender',\n",
    " 'eating',\n",
    " 'substance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ctl_tags13+['message']\n",
    "# test_tags = test[cols]\n",
    "\n",
    "# for tag in ctl_tags13:\n",
    "#     test_tag_i_1 = test_tags[test_tags[tag]==1]\n",
    "#     test_tag_i_0 = test_tags[test_tags[tag]==0]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_binary_dataset(df_metadata, dv = 'suicide', n_per_dv = 3000):\n",
    "    df_metadata_tag_1 = df_metadata[df_metadata[dv]==1].sample(n=n_per_dv,random_state=123)\n",
    "    df_metadata_tag_0 = df_metadata[df_metadata[dv]==0].sample(n=n_per_dv,random_state=123)\n",
    "    assert df_metadata_tag_1.shape[0] == n_per_dv\n",
    "    assert df_metadata_tag_0.shape[0] == n_per_dv\n",
    "\n",
    "    df_metadata_tag = pd.concat([df_metadata_tag_1, df_metadata_tag_0]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df_metadata_tag\n",
    "\n",
    "\n",
    "# for dv in ctl_tags13:\n",
    "dv = 'suicide'\n",
    "df_i = create_binary_dataset(test, dv = dv, n_per_dv = 100)\n",
    "df_i[ctl_tags13].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*150/60, 'minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I need help. I've been feeling depressed for a while because I work nights and it's taking a toll on my mental health. I'm scheduled to work tonight and the next 2 nights, but I don't think I can go in tonight. I'm having suicidal thoughts and won't be able to focus on my job. I'm worried about calling off since I'm already at the [scrubbed] amount of absences, and I don't want to get fired. I've attempted suicide a couple times in the past, and I've taken pills each time. I do have access to them, but I don't plan on taking anything. I want to avoid hurting myself because I know it doesn't help and it only makes my family worry. I just have such a hard time going to work when I feel this bad. I just don't know what to do about work. I know my safety is more important but I  don't want to lose my job, I already am struggling financially.\",\n",
    "        \"I need help. I've been feeling depressed for a while because I work nights and it's taking a toll on my mental health. I'm scheduled to work tonight and the next 2 nights, but I don't think I can go in tonight. I'm having suicidal thoughts and won't be able to focus on my job. I'm worried about calling off since I'm already at the [scrubbed] amount of absences, and I don't want to get fired. I've attempted suicide a couple times in the past, and I've taken pills each time. I do have access to them, but I don't plan on taking anything. I want to avoid hurting myself because I know it doesn't help and it only makes my family worry. I just have such a hard time going to work when I feel this bad. I just don't know what to do about work. I know my safety is more important but I  don't want to lose my job, I already am struggling financially.\",\n",
    "    \n",
    "#     \"talk. done. okay thank you so much. there's a lot so i'm sorry if this is to much , but me and my grandma have been argue a lot and it's been going on sense i was 6-7 i believe , this morning i woke up and my ex boyfriend told me he likes my bestfriend . . also my dad committed suicide back in 2017 , he meant the world to me and i have just been thinking about giving up alot lately , there's always being suicide thoughts in my mind sense my dad passed , i go to counseling and therapy and i take medication , i have smoked weird and vapes and that was a horrible Decision to make .. i have smoked weed *. i hate myself more and more every day from smoking weed and vapes .. yea , i have f's in every single one of my classes except gym and i feel like a failure of a student , i'm really trying to get my [scrubbed]s up but at some points i just gi be up cus i feel like i'll never make it to the 8th [scrubbed]. give up *. yes i do , many times i think about it but i'm scared to do it . i don't wanna hurt my family and friends . i don't really have a plan , i've tried at one time tho i would cut my self . my dad hung himself ... i haven't in about 3 weeks. of course my name is [scrubbed]. yes it does. yea , i just really miss my old relationship with my grandma . and i miss my boyfriend . ex now but he always made me happy literally every little thing he did always made me happy . just the little things mattered to me . every time i was with him i was so happy and i just don't know what to do anymore . i never thought i would lose him .. well here's one more thing . i live with my grandma and grandpa and my uncle sexual assaulted me when he lived here .. my grandma only knows and it's my grandpas son , that kills me every day i was only 3 and he was 13 . im sorry if this is tmi but that's what happened and he tried to kiss me in walmart . i'm sorry , i was eating. watch tik [scrubbed] and play games. yea. okay. a little better. thank you.\",\n",
    "#     \"talk. done. okay thank you so much. there's a lot so i'm sorry if this is to much , but me and my grandma have been argue a lot and it's been going on sense i was 6-7 i believe , this morning i woke up and my ex boyfriend told me he likes my bestfriend . . also my dad committed suicide back in 2017 , he meant the world to me and i have just been thinking about giving up alot lately , there's always being suicide thoughts in my mind sense my dad passed , i go to counseling and therapy and i take medication , i have smoked weird and vapes and that was a horrible Decision to make .. i have smoked weed *. i hate myself more and more every day from smoking weed and vapes .. yea , i have f's in every single one of my classes except gym and i feel like a failure of a student , i'm really trying to get my [scrubbed]s up but at some points i just gi be up cus i feel like i'll never make it to the 8th [scrubbed]. give up *. yes i do , many times i think about it but i'm scared to do it . i don't wanna hurt my family and friends . i don't really have a plan , i've tried at one time tho i would cut my self . my dad hung himself ... i haven't in about 3 weeks. of course my name is [scrubbed]. yes it does. yea , i just really miss my old relationship with my grandma . and i miss my boyfriend . ex now but he always made me happy literally every little thing he did always made me happy . just the little things mattered to me . every time i was with him i was so happy and i just don't know what to do anymore . i never thought i would lose him .. well here's one more thing . i live with my grandma and grandpa and my uncle sexual assaulted me when he lived here .. my grandma only knows and it's my grandpas son , that kills me every day i was only 3 and he was 13 . im sorry if this is tmi but that's what happened and he tried to kiss me in walmart . i'm sorry , i was eating. watch tik [scrubbed] and play games. yea. okay. a little better. thank you.\",\n",
    "]\n",
    "constructs = [\n",
    "    'Sexual abuse',\n",
    "    'suicidal ideation',\n",
    "    'substance use'\n",
    "#     'relationship issues',\n",
    "#     'suicidal ideation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_names = dict(zip(ctl_tags13, ['']*len(ctl_tags13)))\n",
    "prompt_names = {'self_harm': '',\n",
    " 'suicide': 'suicidal ideation',\n",
    " 'bully': '',\n",
    " 'abuse_physical': '',\n",
    " 'abuse_sexual': '',\n",
    " 'relationship': '',\n",
    " 'bereavement': '',\n",
    " 'isolated': '',\n",
    " 'anxiety': '',\n",
    " 'depressed': '',\n",
    " 'gender': '',\n",
    " 'eating': '',\n",
    " 'substance': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = df_i['message'].values\n",
    "y_test = df_i[dv].values\n",
    "construct = prompt_names.get(dv)\n",
    "print(construct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"related to\" vs \"might be expressing\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 2500 \n",
    "\n",
    "# documents = ['No one cares about me']\n",
    "import time\n",
    "responses = []\n",
    "time_elapsed_all = []\n",
    "\n",
    "for document, y_test_i in tqdm(zip(documents, y_test)):\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are a text classification assistant for a {context}.\n",
    "\n",
    "    The text to classify is:\n",
    "    ```\n",
    "    {document}\n",
    "    ```\n",
    "    \n",
    "    \n",
    "\n",
    "    Assign a probability for following labels and return using this JSON format (do not provide additional notes, explanations or warnings, just that JSON output):\n",
    "\n",
    "    {{'At least one of the sentences in text is related to {construct}': <your_probability>, 'None of the sentences in text are related to {construct}': <your_probability>}}\n",
    "    \n",
    "\n",
    "    json:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    prompt = prompt.format(context = 'Crisis Text Line service', document = document, construct = construct)\n",
    "    print(prompt)\n",
    "\n",
    "\n",
    "    if 'gemma' in model_name:\n",
    "        # Gemma\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**input_ids, max_length = max_length)\n",
    "        tokenizer.decode(outputs[0])\n",
    "        # Find the length of the input_ids to know where the original prompt ends\n",
    "        prompt_length = input_ids[\"input_ids\"].shape[1]\n",
    "        # Decode only the generated part, skipping the prompt\n",
    "        response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "    elif 'llama' in model_name:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=max_length)\n",
    "        response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    elif \"paulml/OGNO-7B\" in model_name:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "        response = outputs[0][\"generated_text\"] \n",
    "    elif 'microsoft/phi-2' in model_name:     \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    responses.append(response)\n",
    "        \n",
    "    print('y_test_i', y_test_i, '=======')\n",
    "    print(response)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(time_elapsed)\n",
    "    print()\n",
    "    time_elapsed_all.append(time_elapsed)\n",
    "\n",
    "print(responses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(time_elapsed_all), np.std(time_elapsed_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses = obtain_json(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_1 = [list(n.values())[0] for n in json_responses]\n",
    "y_pred_proba_1\n",
    "y_pred = np.array([n>=0.5 for n in y_pred_proba_1])*1\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "\tConfusionMatrixDisplay,\n",
    "\tauc,\n",
    "\tconfusion_matrix,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    "\tprecision_score,\n",
    "\trecall_score,\n",
    "\troc_auc_score,\n",
    ")\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def cm(y_true, y_pred, output_dir, model_name, ts, classes = [\"SITB-\", \"SITB+\"], save=True):\n",
    "\tcm = confusion_matrix(y_true, y_pred, normalize=None)\n",
    "\tcm_df = pd.DataFrame(cm, index=classes , columns=classes )\n",
    "\tcm_df_meaning = pd.DataFrame([[\"TN\", \"FP\"], [\"FN\", \"TP\"]], index=classes , columns=classes )\n",
    "\n",
    "\tcm_norm = confusion_matrix(y_true, y_pred, normalize=\"all\")\n",
    "\tcm_norm = (cm_norm * 100).round(2)\n",
    "\tcm_df_norm = pd.DataFrame(cm_norm, index=classes , columns=classes )\n",
    "\n",
    "\t\n",
    "\tplt.rcParams[\"figure.figsize\"] = [4, 4]\n",
    "\tConfusionMatrixDisplay(cm_norm, display_labels=classes ).plot()\n",
    "\tplt.tight_layout()\n",
    "\t\n",
    "\tif save:\n",
    "\t\tplt.savefig(output_dir + f\"cm_{model_name}_{ts}.png\", dpi = 300)\n",
    "\t\tcm_df_meaning.to_csv(output_dir + f\"cm_meaning_{model_name}_{ts}.csv\")\n",
    "\t\tcm_df.to_csv(output_dir + f\"cm_{model_name}_{ts}.csv\")\n",
    "\t\tcm_df_norm.to_csv(output_dir + f\"cm_norm_{model_name}_{ts}.csv\")\n",
    "\n",
    "\treturn cm_df_meaning, cm_df, cm_df_norm\n",
    "\n",
    "\n",
    "def classification_report(y_true, y_pred, y_pred_proba_1, output_dir,gridsearch=None,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=None,feature_vector=None,model_name=None,round_to = 2, ts = None, save_results=False):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    np.set_printoptions(suppress=True)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # calculate precision and recall for each threshold\n",
    "    lr_precision, lr_recall, thresholds = precision_recall_curve(y_true, y_pred_proba_1)\n",
    "\n",
    "    # TODO: add best threshold\n",
    "    fscore = (2 * lr_precision * lr_recall) / (lr_precision + lr_recall)\n",
    "    fscore[np.isnan(fscore)] = 0\n",
    "    ix = np.argmax(fscore)\n",
    "    best_threshold = thresholds[ix].item()\n",
    "\n",
    "    pr_auc = auc(lr_recall, lr_precision)\n",
    "    # AU P-R curve is also approximated by avg. precision\n",
    "    # avg_pr = metrics.average_precision_score(y_true,y_pred_proba_1)\n",
    "\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)  # OR: recall_score(y_true,y_pred, pos_label=0)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        [feature_vector,model_name, sensitivity, specificity, precision, f1, roc_auc, pr_auc, best_threshold, gridsearch, best_params],\n",
    "        index=[\"Feature vector\",\"Model\", \"Sensitivity\", \"Specificity\", \"Precision\", \"F1\", \"ROC AUC\", \"PR AUC\", \"Best th PR AUC\", \"Gridsearch\", \"Best parameters\"],\n",
    "    ).T.round(2)\n",
    "    if save_results:\n",
    "        results.to_csv(output_dir + f\"results_{model_name}_{ts}.csv\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_preds = len(y_pred)\n",
    "cm_df_meaning, cm_df, cm_df_norm = cm(y_test[:amount_of_preds], y_pred, output_dir, model_name, None, classes = [f\"Other\", f\"{dv}\"], save=False)\n",
    "\n",
    "\n",
    "results = classification_report(y_test[:amount_of_preds], y_pred, y_pred_proba_1, None,gridsearch=None,\n",
    "    best_params=None,feature_vector=None,model_name=model_name,round_to = 2, ts = None, save_results=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = [find_json_in_string(n) for n in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version #3.11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiE9zz8vTfvo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# Local\n",
    "import api_keys\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get api key\n",
    "from huggingface_hub import login\n",
    "login(token=api_keys.huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print number of GPUs available\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"\\tName: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"\\tCompute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"\\tTotal Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        # Additional details can be accessed via `torch.cuda.get_device_properties(i)`\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation and if your hardware supports CUDA.\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    # Print CUDA version\n",
    "    print(torch.version.cuda)\n",
    "    # Additionally, to get the name of the CUDA device PyTorch is using:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load openmind8/cuda/11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==x.x.x+cu11.7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lncb9JMYOhMn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_json_in_string(string: str) -> str:\n",
    "    \"\"\"Finds the JSON object in a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to search for a JSON object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    json_string : str\n",
    "    \"\"\"\n",
    "    start = string.find(\"{\")\n",
    "    end = string.rfind(\"}\")\n",
    "    if start != -1 and end != -1:\n",
    "        json_string = string[start : end + 1]\n",
    "    else:\n",
    "        json_string = \"{}\"\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!seff 35521836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "location = 'openmind'\n",
    "\n",
    "if location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/datasets/train10_subset_30/'\n",
    "  output_dir = 'home/dlow/'\n",
    "elif location =='local':\n",
    "  input_dir = './data/ctl/'\n",
    "  output_dir = '/home/dlow/datum/lexicon/data/output/'\n",
    "\n",
    "\n",
    "train = pd.read_csv(input_dir+'train10_train_30perc_messages_texter_metadata.csv')\n",
    "test = pd.read_csv(input_dir+'train10_test_15perc_messages_texter_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['abuse_physical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "e0748e91d14148df891aa3e39c13c6d3",
      "e434f05a4e5149f4a0967fab3e83e0f9",
      "8db1196ae73f4c2392b1b21c60fc816e",
      "fa8d7931a14b423995cb69b57085d0ab",
      "559836edb68e46a7b18f9c3118c7f0cf",
      "b7566811b71f4272b1781c0d810976e0",
      "c11410ecd92a4344943bcc6ea6286fbb",
      "f31e3293e0bb40bab04790e38e3a3861",
      "72215691a18f40559b3c1244278bd795",
      "cd0658244f19439b8b1a0533ef2c9682",
      "c9d4035164784d1d96f92feb5ea8b730",
      "fb0bffa125ee40dfa5327dd7c5ecd28e",
      "8339a414d7c4495db1b9438d4ad7c8b4",
      "7817890190394c1aa52f40c53016344b",
      "8a99b9655eaa4fa2bf731ea603df8247",
      "770e15783e064e2a9697bfc7f5300eab",
      "d5ac040347b54c97a6286ee116227b80",
      "8716ee1afdb44614a1d7312a19dd238a",
      "1f1b3a8f6c51470b9327ca17437d84c2",
      "b74b530fa3c04e478cbe61ed04bc0a07",
      "e7b3b4e599e147abaea80de1e8ee6d23",
      "c4aacd9afc234b559b46da4c1d17db89",
      "1e82a1de6c644841af349f913ae3a388",
      "355cb34629f749d1b9d1679cec2fe999",
      "8c8ec4f8f90a4498a0da5be32a68f3f2",
      "0523eaf6f66d481380ba7103af722c46",
      "2c2f7ec74323465eb8039e33d71b5750",
      "efa2eebaa5a44b228ca6a91f867c65c5",
      "d10132934a0d440794dd6542dba726d7",
      "dbb1a77dba964143a509b4405e679f75",
      "0fa5ffa15f644bdaa3c58dcd9e2e9639",
      "3d73fded7cea4d96930edb43beb03ed1"
     ]
    },
    "id": "0qg4lsyLLCrJ",
    "outputId": "328d82fe-6552-4f4a-feab-e1acf9da190e"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# api_keys.huggingface\n",
    "\n",
    "\n",
    "\n",
    "#TODO: move to api_keys  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiZHKS9FLjDa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "\"google/gemma-2b-it\" 5GB\n",
    "\n",
    "\"google/gemma-7b-it\" 10GB\n",
    "\n",
    "\"meta-llama/Llama-2-7b-chat-hf\" 13.5GB\n",
    "\n",
    "\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "\"meta-llama/Llama-2-70b-chat-hf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto_gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "# reload(auto_gptq)\n",
    "\n",
    "import auto_gptq\n",
    "print(auto_gptq.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AutoGPTQ: 0.2.1\n",
    "# PyTorch: 2.1.0.dev20230520\n",
    "# Transformers: 4.30.0.dev0\n",
    "# Accelerate: 0.20.0.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto_gptq==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "MODEL = \"TheBloke/WizardLM-7B-uncensored-GPTQ\" # 4GB\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(MODEL,\n",
    "#         model_basename=model_basename,\n",
    "        device=device,\n",
    "        use_safetensors=True,\n",
    "        use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=256, min_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(auto_gptq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install auto-gptq\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvitop -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n",
    "# !pip install optimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y auto-gptq\n",
    "!git clone https://github.com/PanQiWei/AutoGPTQ\n",
    "!cd AutoGPTQ\n",
    "!git checkout v0.5.1\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "MODEL = \"TheBloke/open-llama-7b-open-instruct-GPTQ\"\n",
    "model_basename = \"open-llama-7B-open-instruct-GPTQ-4bit-128g.no-act.order\"\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(MODEL,\n",
    "#         model_basename=model_basename,\n",
    "        device=device,\n",
    "        use_safetensors=True,\n",
    "        use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=256, min_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "#         model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error: metadata-generation-failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "dNjrThtgLq1M",
    "outputId": "8cd5ef61-1328-46fa-f8dd-54250f9da815"
   },
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer # llama\n",
    "# import transformers # llama\n",
    "# import torch\n",
    "\n",
    "# TODO: See how they use it for text classification: (from probs or output layer directly?)\n",
    "# https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb\n",
    "\n",
    "\n",
    "# model_name = \"google/gemma-2b-it\"\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "responses = []\n",
    "if 'gemma' in model_name:\n",
    "  # Gemma\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "  model = model.to(device)\n",
    "elif 'llama' in model_name:\n",
    "  # Have to restart session after updating transformers\n",
    "  from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "  model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "  # # alternative\n",
    "  # # Llama\n",
    "  # pipeline = transformers.pipeline(\n",
    "  #     \"text-generation\",\n",
    "  #     model=model,\n",
    "  #     torch_dtype=torch.float16,\n",
    "  #     device_map=\"auto\",\n",
    "  # )\n",
    "\n",
    "  # sequences = pipeline(\n",
    "  #     prompt,\n",
    "  #     do_sample=True,\n",
    "  #     top_k=10,\n",
    "  #     num_return_sequences=1,\n",
    "  #     eos_token_id=tokenizer.eos_token_id,\n",
    "  #     max_length=200,\n",
    "  # )\n",
    "  # for seq in sequences:\n",
    "  #     print(f\"Result: {seq['generated_text']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['suicide']==1].sample(n=3)['message'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "\n",
    "document = \"Help me please. Everything hurts. Why is it worse at night? God I hate everything. I can't really take it anymore. I'm so over it. I hate feeling like this, I don't WANT to feel like this. Thank you. My feelings.. Depression. I don't have clinical but lately I've been feeling like shit, especially at night. It really really is. I just want to be that happy optimistic person full of [scrubbed] that spreads confetti all over my friends and families lives. And I usually can be but I want to be like that all the time and I'm just so fucking tired of everything.. Yeah, that makes sense. It sounds about right. Yeah. Loneliness is something I feel a lot. I have amazing friends to vent to, and that I love and they love me, but I'm still really lonely. I guess confusion is a big feeling I feel as well. I'm lonely and confused. Yes. Drowning myself. Or stabbing myself. Yes I do. The Saturday coming up. No one will be home. Then I can call the cops and tell them I'm going to kill my self so my family doesn't have to see my body. Yes i am. About suicide? No. Your welcome.. Yeah. I don't need them to worry about me. Yeah I do. My family and friends are my world. My dad would. And my best friends [scrubbed] and [scrubbed]. That's something I wonder. Am I really their world. I'm kinda a bitch. Or I can be. Like I said, I try to be really upbeat, but I have some unflattering moments. If I'm their world? No. It seems a bit possessive. Well they're always here for me and willing to talk and let me vent, so that means a lot to me. Especially since we all know we can come to each other. I vent to them, they vent to me. Yeah, I think they do too. I'm not sure. Yes I think so. Maybe distractions. Like YouTube or something. Yeah I think so. I also write in my [scrubbed] a lot. That helps. The online chat thing sounds like it would be a good support system. Thank you so much. And yeah, I've reached out here multiple times. I'm feeling stable now.. That sounds good [scrubbed]. Thank you. Thank you so much.\"\n",
    "construct = 'suicide'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'related to {construct}': <your_probability>, 'not related to {construct}': <your_probability>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = \"\"\"\n",
    "You are a text classification assistant.\n",
    "\n",
    "The text to classify:\n",
    "```\n",
    "{document}\n",
    "```\n",
    "\n",
    "Assign a probability for following labels and return in a JSON format:\n",
    "\n",
    "'related to {construct} at any point': <your_probability>, 'not related to {construct}': <your_probability>\n",
    "\n",
    "Do not provide additional text or explanations, just that JSON output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = prompt.format(document = document, construct = construct)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "responses = []\n",
    "if 'gemma' in model_name:\n",
    "  # Gemma\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(**input_ids, max_length = max_length)\n",
    "  tokenizer.decode(outputs[0])\n",
    "  # Find the length of the input_ids to know where the original prompt ends\n",
    "  prompt_length = input_ids[\"input_ids\"].shape[1]\n",
    "  # Decode only the generated part, skipping the prompt\n",
    "  response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "  responses.append(response)\n",
    "elif 'llama' in model_name:\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "  # Generate\n",
    "  generate_ids = model.generate(inputs.input_ids, max_length=max_length)\n",
    "  response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  responses.append(response)\n",
    "\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "2bad1e9b549f48919831f8bd10d55bba",
      "cd3d157166db4db8862933d45f39eec3",
      "148e20ab02c241cab909a25ab413c237",
      "07c441e779e340a99bc4f3d099e2d048",
      "641e08868556421d82bd1daac10d4798",
      "52f81fb4b6854e0392e8781da09f1253",
      "5c40d498f4084397aa22895fd34c2753",
      "95c38478c49745b488cde5afa9dfbb41",
      "fd11a3fcbd004bccb7a8e39c679adba2",
      "e3de03cd497b44d589531c8b7856d2aa",
      "405b8870faa24d1a983c1082e54cd6fa",
      "4f271d430f524c5583561ee414ac3fa3",
      "8d0b83150b0d4ed88b7b64a5be19cf04",
      "134c18232b794ec1a92fb318e0f33226",
      "daa6e367e12b44729217986cde5f38eb",
      "02cee567dc9747198c9c9c1ce154d5b3",
      "d15fe17ecc064ca1b8497c915332f9bd",
      "77cfdb522be6450db8ebc41308ad9dce",
      "0951fd01ff37423eb33f8d6398603f38",
      "c6ff37a1b9e14e639b77390673dca46c",
      "cc22cc28a8d0488ebc64bdb76e5cd00d",
      "43647adf45a4461a9135a8424d019260",
      "5db1fe88d4404248b80d8373f1b4d8c7",
      "b9751debe5c04074a393a2a3f0317c29",
      "d4fe61f3baa140258299f51d8e3f5cf6",
      "0c7045e69a0b4c75a7cff0b28d5e6387",
      "04d6e021659943e6a25239429eddcb22",
      "6b4761312db34b35a7a29c09dd950cd6",
      "f0ab431d81714163864101e0929585ed",
      "2b4aaa51bcb14dacb4161b0b31815e7d",
      "6b60b9d4f7bb45fca92a3d15d43b18b6",
      "34750a03f7df4af89eae62fe7ed3c72d",
      "1643c26fea124c759fcf91ffd16ee6d3",
      "f7ff8e6eb33b4489934035746f6d9fbf",
      "23f2128c114f4972a856cfad57f42629",
      "de709c1f4d86444ab0fb763adaaa18cf",
      "0048ebd616b74645a83264c6268fb530",
      "516003c263bb4839a698358ff6658b63",
      "a6448c930a7f4e64b84e1251395921b8",
      "a30bb4ea45304327964814df78c3b4d9",
      "3cfb225bd0fd4247ad7c61192d6cc262",
      "b96caf304f8a42fdb8d06689d210aa0b",
      "7907582e1029450cbdefba18de412796",
      "27709187cb2941b0a3d7236b1a374cbd",
      "eb3abeb3a6114013ac0a832f3770ad23",
      "f60d18f953794e8cb8b3335b5f908aba",
      "1879f39a61a34166aaaa545531612729",
      "0810d3191faa422f84efb08b42e09c47",
      "46e7f3d2b092449b84aebb2c7b8c8d51",
      "99f003167afb49e68f6c6ea27d06ddce",
      "17710304a35342dd89d3fde3a1274983",
      "97bf24ac3dee49f598439e081e2bde94",
      "3457b1b941b942029a477d8ba5f13451",
      "2a8e52929f6b4333b599fc4266c534c7",
      "b700bb21b1d8422dbbd77bdc7fb6aa05"
     ]
    },
    "id": "YmkllPiGW-g3",
    "outputId": "74023254-059f-4c7e-ae1a-f9187e8c51ec"
   },
   "outputs": [],
   "source": [
    "# Have to restart session after updating transformers\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MJbIloQvMLhc",
    "outputId": "7ce8c987-742f-4ba2-b8b7-17d078f5e264"
   },
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLFWjS1DNppa",
    "outputId": "799257ee-3794-4051-b9a2-33167e1867e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "eval(find_json_in_string(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0S_-6taOA8m"
   },
   "outputs": [],
   "source": [
    "\n",
    "a = '1'\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWlkvWvaN3TG"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(generated_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xypJcy7ANKzG",
    "outputId": "16457c50-6134-41a0-86c1-70ca025a6445"
   },
   "outputs": [],
   "source": [
    "print(generated_text_only)==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "46NANDu_Me0p",
    "outputId": "6eb3bec5-1f63-4ea4-d262-80e87c6fb48a"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612,
     "referenced_widgets": [
      "70cc254c3f2648c89d0f3d0095c7a54a",
      "66f00dc3dfb149e6b249a93a24e13662",
      "5d030b269dab4f7d8e16dedde3182cec",
      "765936da08c84d5fb37bbe983c6cd912",
      "7a35ab075996415baa242ec31fe6b7e3",
      "b9f7e2817d4c4a8689ebe763bbd5f561",
      "6d9efbe96d3641279e3510478c15877c",
      "1cb6b6de4a874c9e946b560d34fac923",
      "208aa4395779418798a8f631208e4ec3",
      "3293a1c6f8b24b659a5962b776d98a8e",
      "9315edd45b444602b49368d640781661",
      "1599275c36ea4799b46b6e23ff3946bc",
      "8535343beb3f431ebc8315c7b9acf784",
      "e74e868dd5964025a062b4e827546901",
      "a096b685ef304ed893d960a50dd9a8f1",
      "96efd59c5216469aa46644e6c245671c",
      "009dcc256add4ff8b7963e7d4816684d",
      "7004a8c507da4b99a17ac0d3304e3cd5",
      "60ec065a680c4c3b9cf69749f220ec0e",
      "5246d20280ef48e0ad7e60f58c0455ad",
      "738a2a48db8d4afc9950877c3f52bfb8",
      "7e8fbb6324f0482da72be51b49a1d511",
      "a704e945179645c98d3233415f424610",
      "bdab6cd290b14770b456965430587912",
      "a2d76a8c322049f19895f5d2bab9f8ea",
      "5093196ff9e54d9b95f86c2f0b70f7cc",
      "fdf73987070a42269d4417fda2448443",
      "d88c82ac255341af98263c11994be016",
      "c4f16e29b310431cbc8b28bdceafb42b",
      "449e022ad9ba43c19586274a30b2df32",
      "9399e3c0de7c4768b20506165166d9aa",
      "a78e7bec82274544ba4020daa6ce5b70",
      "fd5219ff07974d7ca47c1fa10a17d659",
      "337e6d39959c429fb4475401d5d57365",
      "0e507551542a482ab4ea1bf625303312",
      "c8847fae77624907a352d1f30deac22a",
      "9d0aa21444b842d192e14551ea7cc408",
      "3b19a9815e03490486d27f9c117d0eaa",
      "bfc1346b77fb4342911b4af1db258338",
      "4a1d232d32794e6da57bdf8304a75780",
      "1a127315d763423086ece900f4afcd26",
      "edc74b5f6f7446bdaa1742ea3005597f",
      "535979735ea1446b9be86d0c6385fdf2",
      "df11c66895fa43f183f5ae127acd9a36",
      "a3e5a6375fc74f689fafd82f2ad8625a",
      "e827c88b98f34b89ae4f2c631cc123a7",
      "d4b772ef0b584864bfb673c7e1dc39e9",
      "cce96ee25e7744e48f95814e645b1b9f",
      "0cba296f416b47bb9483d9266e875933",
      "7c75313737854757bcf82551c862c9eb",
      "2618af77498642efb855a291f7dc69ab",
      "2d7888e18c8f4a5d8a96ff5be9057396",
      "9315f2f1df8646048c9ae12fccc92945",
      "38ce541e0f3b40d09384fdc20caec3e0",
      "20071fd31d9b4a96b4a36ae7a40ac62e",
      "68bae2ea7ad24d2e9e97602529307ae9",
      "b69ed408de064b748a7eb40337bde57a",
      "b9b8ab85fc3d44aaac0287b60be2274a",
      "17371b0b4416403ba312c43fd1a3b02b",
      "30fb88e8d7264963bb6b9723531a463f",
      "44b2700be333417096d42760a2b60a04",
      "8ce0a0bb9d8b416c89b991b7b527f05c",
      "291c528295884749b232c48700e596ca",
      "eea1ecc1503b42058f723721386bde2d",
      "ca92470d7ee743b6a3e723319b03f4f7",
      "4c9bae038cb24beb8cf965bc94ba3698",
      "90641505e4f94ce48513d36ba2c7fc88",
      "a73c9cb86cef41608b50033804e406d9",
      "3089e54cc8fd4a5cbc3ac110f677ae24",
      "773581fbd4694d1cb3aab4200eff1a94",
      "23e253eca8a24aa29eeff62c17641ff2",
      "2bbf13a2353f41b08ce078e92e953e6a",
      "4d3b0752515d43c2942b35d11111e42a",
      "5f36fc3b493e4dbb9880bb9801d47dd2",
      "f75cc8af14af45e58cdb933c31ce231f",
      "4b56d832e53a4f9da0ddf6c374edc8f2",
      "feebf706a0074be8a031b063513c8d2d",
      "cc186a1fc6574b6fba894bc2813d2d69",
      "cc00b22211954df4bdf4befecdf2b38b",
      "bc296a928b3646ce90ba1b270d7a4354",
      "562c76739f7f4719ab6eaa751d92bfdd",
      "e78e6af707864ef099e17ec636b3a5d9",
      "e3fe23bb3f474732a3b70103c5d88ead",
      "71cac8dab15c4977b6906c1b1c975fed",
      "2e671dd914094d08a5a9c73b6e364390",
      "d1d1fe7245a84aaaa66f5eba0cbeff75",
      "2951cbe3686c435dafc1db866ba6305c",
      "37453a4d7cc543d996a5887869a9e0c8"
     ]
    },
    "id": "Z8GE41UiJ6o2",
    "outputId": "fe0d3b1d-528a-4136-aa9f-cc2d4e408c87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YCqY-cIH0aU",
    "outputId": "ed717532-0f21-4e8e-c4ad-77fc216c4aec"
   },
   "outputs": [],
   "source": [
    "pip install transformers\n",
    "huggingface-cli login\n",
    "!pip -i install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615,
     "referenced_widgets": [
      "99a8d26f2f6c45ca8201b156663e9d2d",
      "2f747477532942608cf28a69137b98ef",
      "5144f9ed557b46df96c278cb842fde8d",
      "93a164061f5b4e4cb94b1a330eb4968c",
      "ce825e7eb842498c911e07c5c552eca6",
      "1be53c27a7904d5294acbdecf26bf91a",
      "e40c01015bf148b89bea684400fb4a56",
      "abd5ccc76b564acaa4f902e4d269701e",
      "2bd8802fbe6c4fb586bdede2741ebda0",
      "15338ed4760c4730bdc34904277cdbf8",
      "672aa41fbf764764ae4529f243a88f30",
      "07bbfc9c6c274428bedd6f813030b3e9",
      "ac8e3fc33b644bb6ab8343145d70ed55",
      "963dc170a4ae41e1ac99ee6c90559f99",
      "f9a4a0dd50e4463e9e83a3e404a34b66",
      "e0549151393743cb8ef3b253e578f424",
      "ac0bf912206e43bc85fcb038941e1ee4",
      "ae61c2fa08644082b1be19ae95c47f88",
      "6b181746df6f4f5aae6274917ad5ec4c",
      "2f90ffb511694b15bc2d3a51a7e518fc",
      "dbaba0df47c7447597fb1ee99218aefe",
      "89dbfa28f6e44b88a6282576c5ebd473",
      "34b6ecfaa8d949c4b7d0ebe6950d5877",
      "5a196b8e835748aaa4255ff385fe6d00",
      "dacf248cab7542ca9945e4bc444bbf07",
      "bb882078e7c54b29a11da5f3524326c7",
      "3c0c60e448324c3895b1aab089b54ae9",
      "7643264113804fc4b2241943224ff1b3",
      "f372b54961f949008e78e2c93ee11516",
      "5459e3a17f484bbaa3fc6788baee28a3",
      "3029e872f6ae46669468e16b58bc0ec8",
      "46c5a5d269704422bb7521fa912cb79d",
      "164acd49059f4bf2978d6bd151f0b026",
      "5685329f89a84fceaa1cb2b555e2638f",
      "84f96150da9b48218ac650182a51de77",
      "0f5c672ec407412096f550f1eedfd3e0",
      "b9bbc9cca0de4ab58dba3d7171a9e49b",
      "724795cb0ad243498ecab669cdb4712c",
      "671956d6bbba47d49dd7d440d5ccac5a",
      "4c0d2b1ce74d4a30bdf43a39d15f8020",
      "d0c41e63956d42fc8fbb3111acfc2615",
      "71acb3777b6e4205ab094aae3feb3756",
      "cd45f4b82bf4446fadc9b6f3b8bfd7e4",
      "e1c9f255bb524dbfa947071c555173c6",
      "0be4e11dc2604d93b20179953ec8f885",
      "2a3d3a38616b430590774832298a236a",
      "f1346b727fe140cab234649a5c5ffe38",
      "c9858e982a91416ca84f0d6545a61a62",
      "3f3bc7ff7c764ee3a1250107cc421d13",
      "be4cdf45e6184e7bbc6e21bd89503f9b",
      "3c4768ada879427cbbd04d69654f8738",
      "629809fd565848d081b3149b376a671d",
      "515c77ef011e40028b3468eacee51550",
      "e400e2ea4bf84f4ead4c53e69a82688b",
      "91a5458790a8403bbd759f17c9c5663d",
      "c0e71e66ae7e412689b9a2fd7392b8fd",
      "fbab6e73fc5842f6ae47faaee23cd5f7",
      "f387219e1ba34dae80c3d6a0e33700b0",
      "6f942d162071460ea83fbdd5cee8644c",
      "6e25e8150e7e454a9e85665949e83d7a",
      "cdbd4d49e9a643c581670146df696a1c",
      "33cd939f5aff4f9a97d7074fb398fb4e",
      "841f5879c9fc448f8c68d27b186f421f",
      "1c53ab99748749a094cf5bd917b0e10a",
      "f724930bcb8b472aaab87f055a208d62",
      "4d7dd6e60504432bbc1bdacc7ae48b4c",
      "27d1440a8f1645dbbecc4e2948e24ed1",
      "f6ea175ebb7943e88af6fe70450e096b",
      "8af25a6b648e450cbe670104c4c5bdab",
      "a6e3a729ce1847e8899d2f9bbb3c854f",
      "32b8b72c0a2b49858f13d13a996c4d4a",
      "18917ae3a20e45b8a5fbbe9cfda755dd",
      "5dc6ae3dd3dc478bb1c0301d4ec87ebf",
      "d4fe992373494e4fb056d70ff28f961f",
      "725810af45124482a0ac623fc60d61e6",
      "c2625b0e08124e44a4aeded9e7a8748a",
      "c0ed77c1c0ec4728a457da3cd9b27e86",
      "a5e0d6b9aea745508679c5a044585a89",
      "c2597734ea3c44f1a64eb4ac1679e604",
      "6a34ab380f9d47529a76f66cb0c80d97",
      "40018928c7e24deba339a10189768528",
      "a45aaca1a6d741ef947ea6707e5f78cb",
      "d209c57e561045feabd9dd2b5f15017f",
      "7315bc2657f242c6ae90a7a026afda1f",
      "28b18835c35047d5bed933dcf3301818",
      "a951d40befa44f19a34609280c6242c4",
      "9202e3b162ae48c0860a5ded168de902",
      "5c8b24b36d3640108373854edfa15937",
      "37f8dbac29904e93ba1965fe1d462663",
      "3738df86b8fd4b91b5e22e5c92f9b5c5",
      "c1fdaef37ce94c55803561fb51991db4",
      "907ce5cb9cc642f385a354e097bc956d",
      "6f5d0ee60d2741a997f61b4985050447",
      "ac16af7a4f3c4ee08af54097df78b905",
      "72f9411e93294223939fb8d9b0177187",
      "bdc27a81d33945998387b7191be77785",
      "e43001c4a02847b29de6ae92c639f381",
      "6d8b21e0c95b41cb9ada60fa5ed752dd",
      "1ea883b3a91e4cf38e6b3f32c9d381c3",
      "846f4828d566486dbacb66504c5c4ff7",
      "8789a4044ffb464ca647d83c7a2751b1",
      "3a7dbf998c9b4b708a506d1394927381",
      "720be2d981f3497582d89c11ba97a2fd",
      "d20376cd01694a5da3a39c1c4105a3a9",
      "3132e2ec18ec4c3ba9a74ab938caf980",
      "3933fb8776254653821db9f697b34fad",
      "849d4dae2717401e9c7134dc36276c47",
      "037148ee4b974321b81958b9e2999ed3",
      "5d37b0dd46d5461a8bbcf01365c8919a",
      "b3e32590e4ab4a2faf13d02539d85019",
      "d5a6fee29de94484b459aa567cc052d5",
      "92d4207d71574d68ad8d8b7bc5034780",
      "22a0fa3cd2b74d5c93a316f999e63cc8",
      "98ea43e9cddb48c08e90ddc3b504d134",
      "cc946ba27c17496486c86bab78aa4cc8",
      "0556f4b22aca45ffae38f452fed750f5",
      "f8d7afa0f4824369ba438605d18c0266",
      "57df8fd9e04f4271be323c71d3f0d9d6",
      "72bfb0328895469583152712a2c53639",
      "4f78430ba417430ead5a801438eb3c59",
      "d4ef3515280c44c2b6c9cebb2b7da74e"
     ]
    },
    "id": "pIcrbw9xHekm",
    "outputId": "44401bd4-9534-4769-838c-3c032ac3147f"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "import torch\n",
    "\n",
    "base_model_name = \"bardsai/jaskier-7b-dpo-v5.6\"\n",
    "chatbot = pipeline(\"conversational\", model=base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd3BISgxI9ZA"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VURJ7EyqI-Fv",
    "outputId": "0259fa39-994a-4175-f3ab-4abcb4ede75d"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(conversation.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cflu752ZIy53",
    "outputId": "ea9e7d37-56ba-4182-e3c5-f71e0db03c45"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = \"\"\"\n",
    "You are a text classification assistant.\n",
    "The text to classify:\n",
    "```\n",
    "The table is round\n",
    "```\n",
    "Assign a probability for each possible label: 'loneliness' or 'not loneliness' and return in a JSON format\n",
    "For instance, return this in JSON format:\n",
    "'loneliness': <your_score>,\n",
    "'not loneliness': <your_score>\n",
    "Do not provide additional text or explanations, just the JSON output.\n",
    "\"\"\"\n",
    "\n",
    "conversation = Conversation(prompt)\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJUVso05Ck_4"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8pHkL9uCmCy",
    "outputId": "95cdc7b5-e7f1-414d-c6dd-144a897c2acb"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8JUYPYHCgI4"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6C1ewVDEEh4",
    "outputId": "70d90123-cd00-4d74-bb74-44169a04b981"
   },
   "outputs": [],
   "source": [
    "# Setup the environment\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install git+https://github.com/huggingface/transformers -U\n",
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZU-ZJrGpFbVc",
    "outputId": "439da787-73a9-473d-e415-14c1ee7987c4"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install kaggle_secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "mGpkvcZmEXgw",
    "outputId": "1bcc052d-a3e2-4386-aa3f-b9762c28e58e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "access_token_read = UserSecretsClient().get_secret(api_keys.huggingface)\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "TIC2Q4ReCwhl",
    "outputId": "f524c143-c88a-401c-bc67-03cc4f337b71"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b/2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "qpsjaZsrCzEV",
    "outputId": "6ec24d01-df52-40b8-8a20-42453b74d962"
   },
   "outputs": [],
   "source": [
    "# Use the model\n",
    "input_text = \"What is the best thing about Kaggle?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
